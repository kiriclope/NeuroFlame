{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Settings\n",
    "=================\n",
    "\n",
    "``` ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "%run ../../../notebooks/setup.py\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "REPO_ROOT = \"/home/leon/models/NeuroFlame\"\n",
    "pal = sns.color_palette(\"tab10\")\n",
    "```\n",
    "\n",
    "Imports\n",
    "=======\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from time import perf_counter\n",
    "from scipy.stats import circmean\n",
    "\n",
    "from src.network import Network\n",
    "from src.plot_utils import plot_con\n",
    "from src.decode import decode_bump, circcvl, decode_bump_torch\n",
    "from src.lr_utils import masked_normalize, clamp_tensor, normalize_tensor\n",
    "```\n",
    "\n",
    "Helpers\n",
    "=======\n",
    "\n",
    "plots\n",
    "-----\n",
    "\n",
    "``` ipython\n",
    "def add_vlines(model, ax=None):\n",
    "\n",
    "    if ax is None:\n",
    "        for i in range(len(model.T_STIM_ON)):\n",
    "            plt.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)\n",
    "    else:\n",
    "        for i in range(len(model.T_STIM_ON)):\n",
    "            ax.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)\n",
    "\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def plot_rates_selec(rates, idx=0, thresh=0.5, figname='fig.svg'):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=[2*width, height])\n",
    "        r_max = thresh * np.max(rates[idx])\n",
    "\n",
    "        idx = np.random.randint(0, 96)\n",
    "        vmin, vmax = np.percentile(rates[idx].reshape(-1), [5, 95])\n",
    "\n",
    "        ax[0].imshow(rates[idx].T, aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax[0].set_ylabel('Neuron #')\n",
    "        ax[0].set_xlabel('Step')\n",
    "\n",
    "        idx = np.random.randint(0, 96)\n",
    "        vmin, vmax = np.percentile(rates[idx].reshape(-1), [5, 95])\n",
    "        ax[1].imshow(rates[idx].T, aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax[1].set_ylabel('Neuron #')\n",
    "        ax[1].set_xlabel('Step')\n",
    "        # ax[1].set_ylim([745, 755])\n",
    "        # plt.savefig(figname, dpi=300)\n",
    "        plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def plot_m0_m1_phi(model, rates, idx, figname='fig.svg'):\n",
    "\n",
    "    m0, m1, phi = decode_bump_torch(rates, axis=-1, RET_TENSOR=0)\n",
    "    print(m0.shape, m1.shape, phi.shape)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=[2*width, height])\n",
    "\n",
    "    xtime = np.linspace(0, model.DURATION, m0.shape[-1])\n",
    "    idx = np.random.randint(0, 96, 16)\n",
    "\n",
    "    ax[0].plot(xtime, m0[idx].T)\n",
    "    #ax[0].set_ylim([0, 360])\n",
    "    #ax[0].set_yticks([0, 90, 180, 270, 360])\n",
    "    ax[0].set_ylabel('$\\mathcal{F}_0$ (Hz)')\n",
    "    ax[0].set_xlabel('Time (s)')\n",
    "    add_vlines(model, ax[0])\n",
    "\n",
    "    ax[1].plot(xtime, m1[idx].T)\n",
    "    # ax[1].set_ylim([0, 360])\n",
    "    # ax[1].set_yticks([0, 90, 180, 270, 360])\n",
    "    ax[1].set_ylabel('$\\mathcal{F}_1$ (Hz)')\n",
    "    ax[1].set_xlabel('Time (s)')\n",
    "    add_vlines(model, ax[1])\n",
    "\n",
    "    ax[2].plot(xtime, phi[idx].T * 180 / np.pi, alpha=.5)\n",
    "    ax[2].set_ylim([0, 360])\n",
    "    ax[2].set_yticks([0, 90, 180, 270, 360])\n",
    "    ax[2].set_ylabel('Phase (°)')\n",
    "    ax[2].set_xlabel('Time (s)')\n",
    "    add_vlines(model, ax[2])\n",
    "\n",
    "    plt.savefig(figname, dpi=300)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "Data Split\n",
    "----------\n",
    "\n",
    "``` ipython\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "def split_data(X, Y, train_perc=0.8, batch_size=32, shuffle=True):\n",
    "\n",
    "    # if shuffle:\n",
    "    #     X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "    #                                                         train_size=train_perc,\n",
    "    #                                                         stratify=Y[:, 0].cpu().numpy(),\n",
    "    #                                                         shuffle=True)\n",
    "    # else:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        train_size=train_perc,\n",
    "                                                        stratify=None,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "    plt.hist(Y_train[Y_train!=-999].cpu() * 180 / np.pi, bins=15, label='train')\n",
    "    plt.hist(Y_test[Y_test!=-999].cpu() * 180 / np.pi, bins=15, label='test')\n",
    "    plt.xlabel('Target Loc. (°)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    print(Y_train.shape, Y_test.shape)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "```\n",
    "\n",
    "Optimization\n",
    "------------\n",
    "\n",
    "``` ipython\n",
    "def prune_and_grow_vectorized(W, mask, K):\n",
    "    new_mask = mask.clone()\n",
    "\n",
    "    # --- PRUNE ---\n",
    "    big_num = 1e9\n",
    "    # Set inactive weights to inf (so they're not selected for pruning)\n",
    "    to_prune = W.abs().clone()\n",
    "    to_prune[new_mask == 0] = big_num\n",
    "\n",
    "    # Find indices of K smallest per-column\n",
    "    prune_vals, prune_idx = torch.topk(-to_prune, K, dim=0)  # negative for smallest values\n",
    "\n",
    "    # Vectorized column/row indices for scatter\n",
    "    cols = torch.arange(N).view(1, -1).expand(K, -1)   # shape (K, N)\n",
    "    new_mask[prune_idx, cols] = 0.0\n",
    "\n",
    "    # --- GROW ---\n",
    "    # Regrow: randomly select K locations per column where mask == 0\n",
    "    # Make a mask for regrow-eligible entries\n",
    "    grow_candidates = (new_mask == 0).float()\n",
    "    # Random scores for each eligible position, -inf for non-candidates\n",
    "    grow_scores = torch.rand_like(W) * grow_candidates + (1 - grow_candidates) * (-big_num)\n",
    "    grow_vals, grow_idx = torch.topk(grow_scores, K, dim=0)\n",
    "    new_mask[grow_idx, cols] = 1.0\n",
    "\n",
    "    return new_mask\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def training_step(dataloader, model, loss_fn, optimizer, SET=0):\n",
    "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rates = model(X)\n",
    "        loss = loss_fn(rates, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if SET:\n",
    "            mask = prune_and_grow_vectorized(W, mask, model.Ka[0])\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "    return avg_loss\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def validation_step(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            rates = model(X)\n",
    "            batch_loss = loss_fn(rates, y)\n",
    "            val_loss += batch_loss.item() * X.size(0)\n",
    "\n",
    "    val_loss /= size\n",
    "    return val_loss\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def optimization(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100, thresh=0.005, gamma=0.9):\n",
    "\n",
    "    # Choose one scheduler\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    device = torch.device(DEVICE if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = training_step(train_loader, model, loss_fn, optimizer)\n",
    "        val_loss = validation_step(val_loader, model, loss_fn)\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < thresh and loss < thresh:\n",
    "            print(f'Stopping training as loss has fallen below the threshold: {loss}, {val_loss}')\n",
    "            break\n",
    "\n",
    "        if val_loss > 300:\n",
    "            print(f'Stopping training as loss is too high: {val_loss}')\n",
    "            break\n",
    "\n",
    "        if torch.isnan(torch.tensor(loss)):\n",
    "            print(f'Stopping training as loss is NaN.')\n",
    "            break\n",
    "\n",
    "    return loss_list, val_loss_list\n",
    "```\n",
    "\n",
    "Loss\n",
    "----\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CircularAngleLoss(nn.Module):\n",
    "    def __init__(self, mode='angular', reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.reduction = reduction\n",
    "        self.mse = nn.MSELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, pred_angle, target_angle):\n",
    "        if self.mode == 'polar':\n",
    "            pred_sin, pred_cos = torch.sin(pred_angle), torch.cos(pred_angle)\n",
    "            target_sin, target_cos = torch.sin(target_angle), torch.cos(target_angle)\n",
    "            loss_sin = self.mse(pred_sin, target_sin)\n",
    "            loss_cos = self.mse(pred_cos, target_cos)\n",
    "            return (loss_sin + loss_cos) / 2\n",
    "\n",
    "        elif self.mode == 'angular':\n",
    "            error = 1 - torch.cos(pred_angle - target_angle)\n",
    "            if self.reduction == 'mean':\n",
    "                return error.mean()\n",
    "            elif self.reduction == 'sum':\n",
    "                return error.sum()\n",
    "            else:\n",
    "                return error\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss mode: {self.mode}\")\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions\n",
    "\n",
    "class VonMisesNLLLoss(nn.Module):\n",
    "    def __init__(self, kappa=4.0, reduction='none'):\n",
    "        super().__init__()\n",
    "        self.kappa = kappa\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, pred_angle, target_angle):\n",
    "        # pred_angle and target_angle in radians, same shape\n",
    "        vm = torch.distributions.VonMises(pred_angle, self.kappa)\n",
    "        nll = -vm.log_prob(target_angle)\n",
    "        if self.reduction == 'mean':\n",
    "            return nll.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return nll.sum()\n",
    "        else:\n",
    "            return nll  # (no reduction)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AngularErrorLoss(nn.Module):\n",
    "    def __init__(self, thresh=1.0, reg_tuning=0.1):\n",
    "        super(AngularErrorLoss, self).__init__()\n",
    "\n",
    "        self.loss = nn.MSELoss(reduction='none')\n",
    "        # self.loss = nn.SmoothL1Loss(reduction='none')\n",
    "\n",
    "        # self.polar_loss = VonMisesNLLLoss(reduction='none')\n",
    "        self.polar_loss = CircularAngleLoss(reduction='none')\n",
    "\n",
    "        self.thresh = thresh\n",
    "        self.reg_tuning = reg_tuning\n",
    "\n",
    "    def forward(self, readout, theta_batch):\n",
    "        m0, m1, y_pred = decode_bump_torch(readout, axis=-1, device=readout.device)\n",
    "\n",
    "        valid_mask = theta_batch != -999\n",
    "        invalid_mask = ~valid_mask\n",
    "        total_loss = 0\n",
    "\n",
    "        # angular loss (Dcos, Dsin)\n",
    "        loss_polar = self.polar_loss(theta_batch, y_pred) * valid_mask\n",
    "        loss_angular = loss_polar.sum()\n",
    "        total_loss += loss_angular\n",
    "\n",
    "        # imposing tuning strength\n",
    "        regularization = F.relu((self.thresh * m0 - m1)) * valid_mask\n",
    "        # regularization = F.relu((1.0 - m1 / (self.thresh * m0 + 1e-6))) * valid_mask\n",
    "        total_loss += self.reg_tuning * regularization.sum()\n",
    "\n",
    "        regularization += F.relu((0.5 - m0)) * valid_mask\n",
    "        total_loss += regularization.sum()\n",
    "\n",
    "        regularization += F.relu((0.5 - m1)) * valid_mask\n",
    "        total_loss += regularization.sum()\n",
    "\n",
    "        # normalize over batch and time points\n",
    "        total_loss /= valid_mask.sum()\n",
    "\n",
    "        # imposing zero tuning in invalid mask\n",
    "        loss_zero = self.loss(m1, 0.0 * m1) * invalid_mask\n",
    "        total_loss += loss_zero.sum() / invalid_mask.sum()\n",
    "\n",
    "        return total_loss\n",
    "```\n",
    "\n",
    "Other\n",
    "-----\n",
    "\n",
    "``` ipython\n",
    "import pickle as pkl\n",
    "\n",
    "def pkl_save(obj, name, path=\".\"):\n",
    "      pkl.dump(obj, open(path + \"/\" + name + \".pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "def pkl_load(name, path=\".\"):\n",
    "     return pkl.load(open(path + \"/\" + name + '.pkl', \"rb\"))\n",
    "\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def convert_seconds(seconds):\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    return h, m, s\n",
    "```\n",
    "\n",
    "Model\n",
    "=====\n",
    "\n",
    "``` ipython\n",
    "kwargs = {\n",
    "    'TRAINING': 1,\n",
    "    'LR_INI': 0.001,\n",
    "\n",
    "    'GAIN': 1.0,\n",
    "    'DURATION': 13.0,\n",
    "    'T_STEADY': 10,\n",
    "\n",
    "    'T_STIM_ON': [4.0, 8.0],\n",
    "    'T_STIM_OFF': [5.0, 9.0],\n",
    "\n",
    "    'Jab': [1.0, -1.5, 1.0, -1],\n",
    "    'Ja0': [2.0, 1.0],\n",
    "\n",
    "    'STIM_EI': 0,\n",
    "    'I0': [1.0, -10.0],\n",
    "    'PHI0': [180.0, 180],\n",
    "    'SIGMA0': [1.0, 0.0],\n",
    "    'M0': 1.0,\n",
    "\n",
    "    'RANDOM_DELAY': 1,\n",
    "    'MIN_DELAY': 1,\n",
    "    'MAX_DELAY': 6,\n",
    "\n",
    "    'IF_STP': 1,\n",
    "    'IS_STP': [1, 0, 0, 0],\n",
    "    'J_STP': 1.0,\n",
    "    'USE': [0.03, 0.03, 0.03, 0.1],\n",
    "    'TAU_FAC': [2.0, 2.0, 2.0, 0.0],\n",
    "    'TAU_REC': [0.2, 0.2, 0.2, 0.1],\n",
    "    'W_STP': [1.0, 3.0, 4.0, 1.0],\n",
    "\n",
    "    'DT': 0.02,\n",
    "    'RATE_DYN': 1,\n",
    "    'TAU': [0.2, 0.1],\n",
    "\n",
    "    'SYN_DYN': 0,\n",
    "    'TAU_SYN': [0.2, 0.1],\n",
    "\n",
    "    'IF_NMDA': 1,\n",
    "    'R_NMDA': 1.0,\n",
    "    'TAU_NMDA': [0.5, 0.5],\n",
    "\n",
    "    'IF_ADAPT': 0,\n",
    "    'A_ADAPT': 1.0,\n",
    "    'TAU_ADAPT': 100.0,\n",
    "}\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "REPO_ROOT = \"/home/leon/models/NeuroFlame\"\n",
    "conf_name = \"train_odr_EI.yml\"\n",
    "# conf_name = \"train_vanilla.yml\"\n",
    "DEVICE = 'cuda:1'\n",
    "\n",
    "total_batches = 128 * 5\n",
    "batch_size = 128\n",
    "\n",
    "ratio = total_batches // batch_size\n",
    "\n",
    "N_BATCH = int(batch_size * ratio)\n",
    "print('N_BATCH', N_BATCH, 'batch_size', batch_size)\n",
    "\n",
    "seed = np.random.randint(0, 1e6)\n",
    "seed = 0\n",
    "print('seed', seed)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE=DEVICE, SEED=seed, N_BATCH=N_BATCH, **kwargs)\n",
    "```\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "``` ipython\n",
    "model.J_STP.requires_grad = True\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "``` ipython\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "model.N_BATCH = N_BATCH\n",
    "rwd_mask = torch.zeros((model.N_BATCH, int((model.N_STEPS-model.N_STEADY) / model.N_WINDOW)), device=DEVICE, dtype=torch.bool)\n",
    "print('rwd_mask', rwd_mask.shape)\n",
    "\n",
    "for i in range(model.N_BATCH):\n",
    "    # from first stim onset to second stim onset\n",
    "    mask = torch.arange((model.start_indices[0, i] - model.N_STEADY)/ model.N_WINDOW,\n",
    "                        (model.start_indices[1, i] - model.N_STEADY) / model.N_WINDOW).to(torch.int)\n",
    "    # print(mask)\n",
    "    rwd_mask[i, mask] = True\n",
    "\n",
    "idx = np.random.randint(N_BATCH)\n",
    "print(torch.where(rwd_mask[idx]==1)[0])\n",
    "```\n",
    "\n",
    "### Inputs and Labels\n",
    "\n",
    "``` ipython\n",
    "total_batches = N_BATCH // batch_size\n",
    "\n",
    "print('total_batches', N_BATCH // batch_size)\n",
    "\n",
    "labels = []\n",
    "for _ in range(total_batches):\n",
    "    batch_labels = torch.randint(0, 360, (batch_size, 1)).to(DEVICE)\n",
    "    labels.append(batch_labels)\n",
    "\n",
    "labels = torch.cat(labels, dim=0)\n",
    "print(labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "model.PHI0 = torch.ones((N_BATCH, 2, 1), device=DEVICE, dtype=torch.float)\n",
    "model.PHI0[:, 0] = labels * np.pi / 180.0\n",
    "\n",
    "window_size = int((model.N_STEPS-model.N_STEADY) / model.N_WINDOW)\n",
    "labels = labels.repeat(1, window_size) * np.pi / 180.0\n",
    "labels[~rwd_mask] = -999\n",
    "\n",
    "ff_input = model.init_ff_input()\n",
    "print(model.PHI0.shape, ff_input.shape, labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plt.hist(labels[labels!=-999].cpu() * 180 / np.pi, bins=15)\n",
    "plt.xlabel('Target Loc. (°)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Run\n",
    "\n",
    "``` ipython\n",
    "train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size, shuffle=False)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "criterion = AngularErrorLoss(thresh=1.5)\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "num_epochs = 20\n",
    "start = perf_counter()\n",
    "loss = optimization(model, train_loader, val_loader, criterion, optimizer, num_epochs, thresh=.005)\n",
    "end = perf_counter()\n",
    "print(\"Elapsed (with compilation) = %dh %dm %ds\" % convert_seconds(end - start))\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "torch.save(model.state_dict(), '../models/odr/odr_%d.pth' % seed)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "from src.utils import clear_cache\n",
    "clear_cache()\n",
    "```\n",
    "\n",
    "Testing\n",
    "=======\n",
    "\n",
    "``` ipython\n",
    "model_state_dict = torch.load('../models/odr/odr_%d.pth' % seed);\n",
    "print(model_state_dict.keys())\n",
    "print(torch.allclose(model.Wab_train, model_state_dict['Wab_train']))\n",
    "model.load_state_dict(model_state_dict);\n",
    "model.eval();\n",
    "print(model.J_STP)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "with torch.no_grad():\n",
    "    model.N_BATCH = N_BATCH\n",
    "\n",
    "    labels = torch.randint(0, 360, (N_BATCH, 1)).to(DEVICE) * torch.pi / 180.0\n",
    "    model.PHI0 = torch.ones((N_BATCH, 2, 1), device=DEVICE, dtype=torch.float)\n",
    "    model.PHI0[:, 0] = labels\n",
    "\n",
    "\n",
    "    ff_input = model.init_ff_input()\n",
    "    print(model.PHI0.shape, ff_input.shape, labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "target_loc = labels[:, 0].cpu() * 180 / np.pi\n",
    "\n",
    "plt.hist(target_loc, bins='auto', density=True)\n",
    "plt.xlabel('Target Loc. (°)')\n",
    "plt.ylabel('Density')\n",
    "plt.xticks(np.linspace(0, 360, 5))\n",
    "# plt.savefig('./figs/memhist/targets.svg', dpi=300)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "with torch.no_grad():\n",
    "    rates = model.forward(ff_input=ff_input).cpu().detach().numpy()\n",
    "print('rates', rates.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plot_rates_selec(rates=rates, idx=20, thresh=20)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plot_m0_m1_phi(model, rates, 4)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "# targets = (target_loc + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=[2*width, height])\n",
    "# # ax[0].hist(targets[:, 0] * 180 / np.pi , bins=32 , histtype='step')\n",
    "# ax[0].hist(errors2, bins=32, histtype='step')\n",
    "# ax[0].set_xlabel('Encoding Errors (°)')\n",
    "\n",
    "# ax[1].hist(errors, bins=32)\n",
    "# ax[1].set_xlabel('Memory Errors (°)')\n",
    "# # ax[1].set_xlim([-45, 45])\n",
    "# plt.show()\n",
    "```\n",
    "\n",
    "Connectivity\n",
    "============\n",
    "\n",
    "``` ipython\n",
    "from src.lr_utils import clamp_tensor\n",
    "# Cij = model.GAIN * model.J_STP * (model.W_stp_T[0]  / torch.sqrt(model.Ka[0])\n",
    "#                                 + model.Wab_train[model.slices[0], model.slices[0]]\n",
    "#                                 * torch.sqrt(model.Ka[0]) / model.Na[0])\n",
    "\n",
    "Cij = model.GAIN * model.J_STP * model.W_stp_T[0] / torch.sqrt(model.Ka[0]) * (1.0 + model.Wab_train[model.slices[0], model.slices[0]])\n",
    "\n",
    "\n",
    "# Cij = model.GAIN * ( model.W_stp_T[0]  + model.Wab_train[model.slices[0], model.slices[0]])\n",
    "Cij = clamp_tensor(Cij, 0, model.slices).cpu().detach().numpy()\n",
    "# Cij = Cij>0\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "from scipy.ndimage import gaussian_filter1d, uniform_filter1d\n",
    "\n",
    "plt.figure(figsize=(2.5*width, 1.5*height))  # Set the figure size (width, height) in inches\n",
    "\n",
    "ax1 = plt.subplot2grid((2, 3), (0, 0), rowspan=2)\n",
    "im = ax1.imshow(Cij, cmap='jet', aspect=1, vmin=0)\n",
    "ax1.set_xlabel(\"Presynaptic\")\n",
    "ax1.set_ylabel(\"Postsynaptic\")\n",
    "\n",
    "# Second column, first row\n",
    "ax2 = plt.subplot2grid((2, 3), (0, 1))\n",
    "Kj = np.sum(Cij, axis=0)  # sum over pres\n",
    "ax2.plot(uniform_filter1d(Kj, size=75))\n",
    "# ax2.set_xticklabels([])\n",
    "ax2.set_ylabel(\"$K_j$\")\n",
    "\n",
    "# # Second column, second row\n",
    "ax3 = plt.subplot2grid((2, 3), (1, 1))\n",
    "Ki = np.sum(Cij, axis=1)  # sum over pres\n",
    "ax3.plot(uniform_filter1d(Ki, size=75))\n",
    "ax3.set_ylabel(\"$K_i$\")\n",
    "\n",
    "ax4 = plt.subplot2grid((2, 3), (0, 2), rowspan=2)\n",
    "diags = []\n",
    "for i in range(int(Cij.shape[0] / 2)):\n",
    "   diags.append(np.trace(Cij, offset=i) / Cij.shape[0])\n",
    "diags = np.array(diags)\n",
    "ax4.plot(diags)\n",
    "ax4.set_xlabel(\"Neuron #\")\n",
    "ax4.set_ylabel(\"$P_{ij}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "Dij = Cij.flatten()\n",
    "np.random.shuffle(Dij)\n",
    "Dij = Dij.reshape(Cij.shape)\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d, uniform_filter1d\n",
    "\n",
    "plt.figure(figsize=(2.5*width, 1.5*height))  # Set the figure size (width, height) in inches\n",
    "\n",
    "ax1 = plt.subplot2grid((2, 3), (0, 0), rowspan=2)\n",
    "im = ax1.imshow(Dij, cmap='jet', aspect=1, vmin=0)\n",
    "ax1.set_xlabel(\"Presynaptic\")\n",
    "ax1.set_ylabel(\"Postsynaptic\")\n",
    "\n",
    "# Second column, first row\n",
    "ax2 = plt.subplot2grid((2, 3), (0, 1))\n",
    "Kj = np.sum(Dij, axis=0)  # sum over pres\n",
    "ax2.plot(uniform_filter1d(Kj, size=75))\n",
    "# ax2.set_xticklabels([])\n",
    "ax2.set_ylabel(\"$K_j$\")\n",
    "\n",
    "# # Second column, second row\n",
    "ax3 = plt.subplot2grid((2, 3), (1, 1))\n",
    "Ki = np.sum(Dij, axis=1)  # sum over pres\n",
    "ax3.plot(uniform_filter1d(Ki, size=75))\n",
    "ax3.set_ylabel(\"$K_i$\")\n",
    "\n",
    "ax4 = plt.subplot2grid((2, 3), (0, 2), rowspan=2)\n",
    "diags = []\n",
    "for i in range(int(Dij.shape[0] / 2)):\n",
    "   diags.append(np.trace(Dij, offset=i) / Dij.shape[0])\n",
    "diags = np.array(diags)\n",
    "ax4.plot(diags)\n",
    "ax4.set_xlabel(\"Neuron #\")\n",
    "ax4.set_ylabel(\"$P_{ij}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "```"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
