{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Settings\n",
    "=================\n",
    "\n",
    "``` ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "%run ../../../notebooks/setup.py\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "REPO_ROOT = \"/home/leon/models/NeuroFlame\"\n",
    "pal = sns.color_palette(\"tab10\")\n",
    "```\n",
    "\n",
    "Imports\n",
    "=======\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from time import perf_counter\n",
    "from scipy.stats import circmean\n",
    "\n",
    "from src.network import Network\n",
    "from src.plot_utils import plot_con\n",
    "from src.decode import decode_bump, circcvl, decode_bump_torch\n",
    "from src.lr_utils import masked_normalize, clamp_tensor, normalize_tensor\n",
    "```\n",
    "\n",
    "Helpers\n",
    "=======\n",
    "\n",
    "plots\n",
    "-----\n",
    "\n",
    "``` ipython\n",
    "def add_vlines(model, ax=None):\n",
    "\n",
    "    if ax is None:\n",
    "        for i in range(len(model.T_STIM_ON)):\n",
    "            plt.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)\n",
    "    else:\n",
    "        for i in range(len(model.T_STIM_ON)):\n",
    "            ax.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)\n",
    "\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def plot_rates_selec(rates, idx=0, thresh=0.5, figname='fig.svg'):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=[2*width, height])\n",
    "        r_max = thresh * np.max(rates[idx])\n",
    "\n",
    "        idx = np.random.randint(0, 96)\n",
    "        ax[0].imshow(rates[idx].T, aspect='auto', cmap='jet', vmin=0, vmax=r_max)\n",
    "        ax[0].set_ylabel('Neuron #')\n",
    "        ax[0].set_xlabel('Step')\n",
    "\n",
    "        idx = np.random.randint(0, 96)\n",
    "        ax[1].imshow(rates[idx].T, aspect='auto', cmap='jet', vmin=0, vmax=r_max)\n",
    "        ax[1].set_ylabel('Neuron #')\n",
    "        ax[1].set_xlabel('Step')\n",
    "        # ax[1].set_ylim([745, 755])\n",
    "        # plt.savefig(figname, dpi=300)\n",
    "        plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def plot_m0_m1_phi(model, rates, idx, figname='fig.svg'):\n",
    "\n",
    "    m0, m1, phi = decode_bump(rates, axis=-1)\n",
    "    # m0, m1, phi = get_fourier_moments(rates, axis=-1)\n",
    "    fig, ax = plt.subplots(1, 3, figsize=[2*width, height])\n",
    "\n",
    "    xtime = np.linspace(0, model.DURATION, m0.shape[-1])\n",
    "    idx = np.random.randint(0, 96, 16)\n",
    "\n",
    "    ax[0].plot(xtime, m0[idx].T)\n",
    "    #ax[0].set_ylim([0, 360])\n",
    "    #ax[0].set_yticks([0, 90, 180, 270, 360])\n",
    "    ax[0].set_ylabel('$\\mathcal{F}_0$ (Hz)')\n",
    "    ax[0].set_xlabel('Time (s)')\n",
    "    add_vlines(model, ax[0])\n",
    "\n",
    "    ax[1].plot(xtime, m1[idx].T)\n",
    "    # ax[1].set_ylim([0, 360])\n",
    "    # ax[1].set_yticks([0, 90, 180, 270, 360])\n",
    "    ax[1].set_ylabel('$\\mathcal{F}_1$ (Hz)')\n",
    "    ax[1].set_xlabel('Time (s)')\n",
    "    add_vlines(model, ax[1])\n",
    "\n",
    "    ax[2].plot(xtime, phi[idx].T * 180 / np.pi, alpha=.5)\n",
    "    ax[2].set_ylim([0, 360])\n",
    "    ax[2].set_yticks([0, 90, 180, 270, 360])\n",
    "    ax[2].set_ylabel('Phase (°)')\n",
    "    ax[2].set_xlabel('Time (s)')\n",
    "    add_vlines(model, ax[2])\n",
    "\n",
    "    plt.savefig(figname, dpi=300)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "Data Split\n",
    "----------\n",
    "\n",
    "``` ipython\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "def split_data(X, Y, train_perc=0.8, batch_size=32, shuffle=True):\n",
    "\n",
    "    # if shuffle:\n",
    "    #     X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "    #                                                         train_size=train_perc,\n",
    "    #                                                         stratify=Y[:, 0].cpu().numpy(),\n",
    "    #                                                         shuffle=True)\n",
    "    # else:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        train_size=train_perc,\n",
    "                                                        stratify=None,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "    plt.hist(Y_train[Y_train!=-999].cpu() * 180 / np.pi, bins=15, label='train')\n",
    "    plt.hist(Y_test[Y_test!=-999].cpu() * 180 / np.pi, bins=15, label='test')\n",
    "    plt.xlabel('Target Loc. (°)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    print(Y_train.shape, Y_test.shape)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "```\n",
    "\n",
    "Optimization\n",
    "------------\n",
    "\n",
    "``` ipython\n",
    "def training_step(dataloader, model, loss_fn, optimizer, penalty=None, lbd=0.001, clip_grad=0, zero_grad=0):\n",
    "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rates = model(X)\n",
    "        loss = loss_fn(rates, y)\n",
    "\n",
    "        # Initialize reg_loss as a scalar tensor\n",
    "        reg_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Only apply the penalty once per step\n",
    "        if penalty is not None:\n",
    "            for param in model.parameters():\n",
    "                if penalty == 'l1':\n",
    "                    reg_loss += torch.sum(torch.abs(param))\n",
    "                elif penalty == 'l2':\n",
    "                    reg_loss += torch.sum(param ** 2)  # Better to use param ** 2\n",
    "\n",
    "        loss = loss + lbd * reg_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        if clip_grad:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "    return avg_loss\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def validation_step(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            rates = model(X)\n",
    "            batch_loss = loss_fn(rates, y)\n",
    "            val_loss += batch_loss.item() * X.size(0)\n",
    "\n",
    "    val_loss /= size\n",
    "    return val_loss\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def optimization(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100, penalty=None, lbd=1, thresh=0.005, zero_grad=0, gamma=0.9):\n",
    "\n",
    "    # Choose one scheduler\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    device = torch.device(DEVICE if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = training_step(train_loader, model, loss_fn, optimizer, penalty, lbd, zero_grad=zero_grad)\n",
    "        val_loss = validation_step(val_loader, model, loss_fn)\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < thresh and loss < thresh:\n",
    "            print(f'Stopping training as loss has fallen below the threshold: {loss}, {val_loss}')\n",
    "            break\n",
    "\n",
    "        if val_loss > 300:\n",
    "            print(f'Stopping training as loss is too high: {val_loss}')\n",
    "            break\n",
    "\n",
    "        if torch.isnan(torch.tensor(loss)):\n",
    "            print(f'Stopping training as loss is NaN.')\n",
    "            break\n",
    "\n",
    "    return loss_list, val_loss_list\n",
    "```\n",
    "\n",
    "Loss\n",
    "----\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "\n",
    "def skewed_gaussian_loss(theta_batch, y_pred, theta_bias, sigma=30, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Asymmetric likelihood loss with skew controlled by alpha.\n",
    "    - theta_batch: True stimulus angles (batch_size)\n",
    "    - y_pred: Network predictions (batch_size)\n",
    "    - sigma: Base noise level (degrees)\n",
    "    - alpha: Skew magnitude/direction (alpha > 0: skew away from theta_bias)\n",
    "    \"\"\"\n",
    "    # Compute angular difference (handling circularity)\n",
    "    # delta = torch.remainder(theta_batch - theta_bias + torch.pi, 2.0 * torch.pi) - torch.pi\n",
    "    delta = theta_batch - theta_bias\n",
    "    delta = (delta + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "\n",
    "    # Determine skew direction: alpha should be positive if stimulus > theta_bias\n",
    "    sign = torch.where(delta > 0, 1.0, -1.0)  # 1 if stimulus is clockwise from bias\n",
    "    alpha_scaled = alpha * sign  # Skew direction depends on stimulus location\n",
    "\n",
    "    # Skewed Gaussian likelihood\n",
    "    delta = theta_batch - y_pred\n",
    "    delta = (delta + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "    z = delta / sigma\n",
    "    likelihood = torch.exp(-0.5 * z**2) * (1 + torch.erf(alpha_scaled * z / torch.sqrt(torch.tensor(2.0).to(y_pred.device))))\n",
    "\n",
    "    # Negative log-likelihood loss\n",
    "    loss = -torch.log(likelihood + 1e-6)\n",
    "    return loss\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def gaussian_loss(theta_batch, y_pred, sigma=30):\n",
    "    delta = y_pred - theta_batch\n",
    "    delta = (delta + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "\n",
    "    likelihood = torch.exp(-0.5 * (delta / sigma)**2)\n",
    "\n",
    "    return -torch.log(likelihood + 1e-6)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def polar_loss(theta_batch, y_pred):\n",
    "        loss = nn.MSELoss(reduction='none')\n",
    "        predicted_sin = torch.sin(y_pred)\n",
    "        predicted_cos = torch.cos(y_pred)\n",
    "\n",
    "        target_sin = torch.sin(theta_batch)\n",
    "        target_cos = torch.cos(theta_batch)\n",
    "\n",
    "        loss_sin = loss(predicted_sin, target_sin)\n",
    "        loss_cos = loss(predicted_cos, target_cos)\n",
    "\n",
    "        loss_angular = (loss_sin + loss_cos)\n",
    "\n",
    "        return loss_angular\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AngularErrorLoss(nn.Module):\n",
    "    def __init__(self, thresh=1, reg_tuning=0.1, class_weight='balanced', sigma_stimulus=30, alpha=0.1, prior=0, sigma_prior=60, reg_weight=1.0, reg_prior=0.5):\n",
    "        super(AngularErrorLoss, self).__init__()\n",
    "        self.loss = nn.MSELoss(reduction='none')\n",
    "        # self.loss = nn.SmoothL1Loss(reduction='none')\n",
    "\n",
    "        self.thresh = thresh\n",
    "        self.reg_tuning = reg_tuning\n",
    "        self.reg_weight = reg_weight\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "        self.theta_bias = torch.tensor(prior * torch.pi / 180.0)\n",
    "        self.sigma_prior = sigma * torch.pi / 180.0\n",
    "        self.reg_prior = reg_prior\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.sigma_stimulus = sigma_stimulus * torch.pi / 180.0\n",
    "\n",
    "    def forward(self, readout, theta_batch):\n",
    "        m0, m1, y_pred = decode_bump_torch(readout, axis=-1)\n",
    "\n",
    "        valid_mask = theta_batch != -999\n",
    "        invalid_mask = ~valid_mask\n",
    "        total_loss = 0\n",
    "\n",
    "        # angular loss (Dcos, Dsin)\n",
    "        loss_polar = polar_loss(theta_batch, y_pred) * valid_mask\n",
    "        loss_angular = loss_polar.sum()\n",
    "\n",
    "        # # adding weights to each target location\n",
    "        # if self.class_weight=='balanced':\n",
    "        #     # weights = self.compute_histogram_weights(theta_batch)\n",
    "        #     weights = self.gaussian_weights(theta_batch)\n",
    "        #     loss_polar *= self.reg_weight * weights\n",
    "        # loss_angular = loss_polar.sum()\n",
    "\n",
    "        # loss_gaussian = gaussian_loss(theta_batch, y_pred, sigma=self.sigma_stimulus) * valid_mask\n",
    "        # loss_angular = loss_gaussian.sum()\n",
    "\n",
    "        # bayesian inference\n",
    "        # Treat θ_bias as a prior and force the network to integrate it with the stimulus.\n",
    "        # theta_bias = self.theta_bias.to(y_pred.device)\n",
    "        # loss_likelihood = skewed_gaussian_loss(theta_batch, y_pred, theta_bias,\n",
    "        #                                        sigma=self.sigma_stimulus, alpha=self.alpha) * valid_mask\n",
    "\n",
    "        # loss_angular = loss_likelihood.sum()\n",
    "        # if self.reg_prior != 0:\n",
    "        #     loss_prior = gaussian_loss(theta_bias, y_pred, sigma=self.sigma_prior) * valid_mask\n",
    "        #     loss_angular += self.reg_prior * loss_prior.sum()\n",
    "\n",
    "        total_loss += loss_angular\n",
    "\n",
    "        # adding bias towards/away from the reference\n",
    "        # theta_bias = torch.tensor(self.reference + torch.pi).to(theta_batch.device)\n",
    "        # loss_bias = self.polar_loss(theta_bias, y_pred) * valid_mask\n",
    "        # total_loss += self.reg_bias * loss_bias.sum()\n",
    "\n",
    "        # imposing tuning strength\n",
    "        regularization = F.relu(self.thresh * m0 - m1) * valid_mask\n",
    "        total_loss += self.reg_tuning * regularization.sum()\n",
    "\n",
    "        # normalize over batch and time points\n",
    "        total_loss /= valid_mask.sum()\n",
    "\n",
    "        # imposing zero tuning in invalid mask\n",
    "        loss_zero = self.loss(m1, 0.0 * m1) * invalid_mask\n",
    "        total_loss += self.reg_tuning * (loss_zero.sum() / invalid_mask.sum())\n",
    "\n",
    "        return total_loss\n",
    "```\n",
    "\n",
    "Other\n",
    "-----\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def continuous_bimodal_phases(N_BATCH, preferred_angle, sigma):\n",
    "    # Sample half from preferred_angle and half from preferred_angle + 180\n",
    "    half_batch = N_BATCH // 2\n",
    "\n",
    "    # Sample from preferred_angle\n",
    "    samples_1 = torch.normal(mean=preferred_angle, std=sigma, size=(half_batch, 1))\n",
    "\n",
    "    # Sample from preferred_angle + 180\n",
    "    samples_2 = torch.normal(mean=(preferred_angle + 180) % 360, std=sigma, size=(N_BATCH - half_batch, 1))\n",
    "\n",
    "    # Combine samples and wrap around 360\n",
    "    phase_samples = torch.cat((samples_1, samples_2), dim=0) % 360\n",
    "\n",
    "    return phase_samples\n",
    "\n",
    "# Example usage\n",
    "# N_BATCH = 500\n",
    "# preferred_angle = 45\n",
    "# sigma = 45\n",
    "\n",
    "# samples = continuous_bimodal_phases(N_BATCH, preferred_angle, sigma)\n",
    "\n",
    "# plt.hist(samples.numpy(), bins='auto', density=True)\n",
    "# plt.xlabel('Phase (degrees)')\n",
    "# plt.ylabel('Probability Density')\n",
    "# plt.title('Bimodal Distribution of Phases')\n",
    "# plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def continuous_biased_phases(N_BATCH, preferred_angle, sigma):\n",
    "    phase_samples = torch.normal(mean=preferred_angle, std=sigma, size=(N_BATCH, 1))\n",
    "    phase_samples = phase_samples % 360\n",
    "\n",
    "    return phase_samples\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "import pickle as pkl\n",
    "\n",
    "def pkl_save(obj, name, path=\".\"):\n",
    "    pkl.dump(obj, open(path + \"/\" + name + \".pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "def pkl_load(name, path=\".\"):\n",
    "    return pkl.load(open(path + \"/\" + name + '.pkl', \"rb\"))\n",
    "\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_weighted_phase_samples(N_BATCH, angles, preferred_angle, sigma):\n",
    "    # Convert angles list to a tensor\n",
    "    angles_tensor = torch.tensor(angles)\n",
    "\n",
    "    # Calculate Gaussian probability distribution centered at preferred_angle\n",
    "    probs = np.exp(-0.5 * ((angles - preferred_angle) / sigma) ** 2)\n",
    "    probs /= probs.sum()  # Normalize to get probabilities\n",
    "\n",
    "    # Create a categorical distribution from the computed probabilities\n",
    "    distribution = torch.distributions.Categorical(torch.tensor(probs))\n",
    "\n",
    "    # Sample from the distribution\n",
    "    indices = distribution.sample((N_BATCH,))\n",
    "\n",
    "    # Map indices to angles and reshape to (N_BATCH, 1)\n",
    "    phase_samples = angles_tensor[indices].reshape(N_BATCH, 1)\n",
    "\n",
    "    return phase_samples\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "def convert_seconds(seconds):\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    return h, m, s\n",
    "```\n",
    "\n",
    "Model\n",
    "=====\n",
    "\n",
    "``` ipython\n",
    "REPO_ROOT = \"/home/leon/models/NeuroFlame\"\n",
    "conf_name = \"train_odr_EI.yml\"\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "IF_BIASED_PHASES = 0\n",
    "IF_BIAS = 0\n",
    "IF_BIMOD = 0\n",
    "\n",
    "IF_RAND_REF = 0\n",
    "reference = 90\n",
    "sigma = 90\n",
    "\n",
    "print('reference', reference, 'sigma', sigma)\n",
    "\n",
    "if IF_BIASED_PHASES:\n",
    "    class_weight = 'balanced'\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "total_batches = 128 * 6\n",
    "batch_size = 128\n",
    "ratio = total_batches // batch_size\n",
    "\n",
    "N_BATCH = int(batch_size * ratio)\n",
    "print('N_BATCH', N_BATCH, 'batch_size', batch_size)\n",
    "\n",
    "seed = np.random.randint(0, 1e6)\n",
    "seed = 1\n",
    "print('seed', seed)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "128*6\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE=DEVICE, SEED=seed, N_BATCH=N_BATCH)\n",
    "\n",
    "if IF_BIAS:\n",
    "    if IF_RAND_REF:\n",
    "        reference = np.random.randint(0, 360)\n",
    "        model_state_dict = torch.load('../models/odr/odr_bias_rand_ref_%d.pth' % seed)\n",
    "        print('rand_ref')\n",
    "    elif IF_BIASED_PHASES:\n",
    "        print('fixed ref')\n",
    "        model_state_dict = torch.load('../models/odr/odr_%d.pth' % seed)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "print(model.random_shifts.shape)\n",
    "plt.hist(model.random_shifts.cpu().numpy() * model.DT)\n",
    "plt.xlabel('Delay (s)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "``` ipython\n",
    "model.J_STP.requires_grad = True\n",
    "\n",
    "if IF_BIAS:\n",
    "    model.J_STP.requires_grad = False\n",
    "\n",
    "if IF_RAND_REF:\n",
    "    reference = np.random.randint(0, 360)\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "``` ipython\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "model.N_BATCH = N_BATCH\n",
    "rwd_mask = torch.zeros((model.N_BATCH, int((model.N_STEPS-model.N_STEADY) / model.N_WINDOW)), device=DEVICE, dtype=torch.bool)\n",
    "print('rwd_mask', rwd_mask.shape)\n",
    "\n",
    "for i in range(model.N_BATCH):\n",
    "    # from first stim onset to second stim onset\n",
    "    mask = torch.arange((model.start_indices[0, i] - model.N_STEADY)/ model.N_WINDOW,\n",
    "                        (model.start_indices[1, i] - model.N_STEADY) / model.N_WINDOW).to(torch.int)\n",
    "    # print(mask)\n",
    "    rwd_mask[i, mask] = True\n",
    "\n",
    "idx = np.random.randint(N_BATCH)\n",
    "print(torch.where(rwd_mask[idx]==1)[0])\n",
    "\n",
    "# rwd_mask = rwd_mask.repeat(N_TARGETS, 1)\n",
    "# print('rwd_mask', rwd_mask.shape)\n",
    "# print(torch.where(rwd_mask[idx+32]==1)[0])\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "```\n",
    "\n",
    "### Inputs and Labels\n",
    "\n",
    "``` ipython\n",
    "total_batches = N_BATCH // batch_size\n",
    "\n",
    "print('total_batches', N_BATCH // batch_size)\n",
    "\n",
    "labels = []\n",
    "for _ in range(total_batches):\n",
    "\n",
    "    if IF_BIASED_PHASES:\n",
    "        if IF_BIMOD:\n",
    "            batch_labels = continuous_bimodal_phases(batch_size, reference, sigma)\n",
    "        else:\n",
    "            batch_labels = continuous_biased_phases(batch_size, reference, sigma)\n",
    "    else:\n",
    "        batch_labels = torch.randint(0, 360, (batch_size, 1)).to(DEVICE)\n",
    "\n",
    "    labels.append(batch_labels)\n",
    "\n",
    "labels = torch.cat(labels, dim=0)\n",
    "print(labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "# if IF_BIASED_PHASES:\n",
    "#          labels = continuous_biased_phases(N_BATCH, reference, sigma)\n",
    "# else:\n",
    "#          labels = torch.randint(0, 360, (N_BATCH, 1)).to(DEVICE)\n",
    "# print(labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "model.PHI0 = torch.ones((N_BATCH, 2, 1), device=DEVICE, dtype=torch.float)\n",
    "model.PHI0[:, 0] = labels * np.pi / 180.0\n",
    "\n",
    "window_size = int((model.N_STEPS-model.N_STEADY) / model.N_WINDOW)\n",
    "labels = labels.repeat(1, window_size) * np.pi / 180.0\n",
    "labels[~rwd_mask] = -999\n",
    "\n",
    "ff_input = model.init_ff_input()\n",
    "print(model.PHI0.shape, ff_input.shape, labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "# N_BATCH = 32\n",
    "# N_SESSION = 8\n",
    "# model.N_BATCH = N_BATCH\n",
    "# print(model.N_BATCH)\n",
    "# ff_input = []\n",
    "# labels = []\n",
    "\n",
    "# model.PHI0 = torch.ones((N_BATCH, 2, 1), device=DEVICE, dtype=torch.float)\n",
    "# window_size = int((model.N_STEPS-model.N_STEADY) / model.N_WINDOW)\n",
    "\n",
    "# for i in range(N_SESSION):\n",
    "#         reference = torch.randint(low=0, high=360, size=(1,), device=DEVICE, dtype=torch.float)\n",
    "#         label = continuous_biased_phases(N_BATCH, reference[0], sigma)\n",
    "#         model.PHI0[:, 0] = label * np.pi / 180.0\n",
    "\n",
    "#         label = label.repeat(1, window_size) * np.pi / 180.0\n",
    "#         label[~rwd_mask[:32]] = -999\n",
    "#         labels.append(label)\n",
    "\n",
    "#         ff_input.append(model.init_ff_input())\n",
    "\n",
    "# labels = torch.vstack(labels)\n",
    "# ff_input = torch.vstack(ff_input)\n",
    "# print('ff_input', ff_input.shape, 'labels', labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "print(labels[labels!=-999].shape)\n",
    "plt.hist(labels[labels!=-999].cpu() * 180 / np.pi, bins=15)\n",
    "plt.xlabel('Target Loc. (°)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Run\n",
    "\n",
    "``` ipython\n",
    "train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size, shuffle=False)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "if IF_BIAS:\n",
    "    criterion = AngularErrorLoss(thresh=0.75, class_weight=None, prior=reference, sigma_prior=60, sigma_stimulus=30, alpha=0.05, reg_prior=0.0)\n",
    "else:\n",
    "    criterion = AngularErrorLoss(thresh=0.75, class_weight=None, prior=reference, sigma_prior=30, sigma_stimulus=30, alpha=0.0, reg_prior=0.0)\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "num_epochs = 15\n",
    "start = perf_counter()\n",
    "loss = optimization(model, train_loader, val_loader, criterion, optimizer, num_epochs, thresh=.005)\n",
    "end = perf_counter()\n",
    "print(\"Elapsed (with compilation) = %dh %dm %ds\" % convert_seconds(end - start))\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "if IF_BIASED_PHASES:\n",
    "    if IF_RAND_REF:\n",
    "        torch.save(model.state_dict(), '../models/odr/odr_bias_rand_ref_%d.pth' % seed)\n",
    "    else:\n",
    "        if class_weight == 'balanced':\n",
    "            torch.save(model.state_dict(), '../models/odr/odr_bias_%d_ref_%d_bal_loss.pth' % (reference, seed) )\n",
    "        else:\n",
    "            torch.save(model.state_dict(), '../models/odr/odr_bias_%d_ref_%d.pth' % (reference, seed) )\n",
    "\n",
    "else:\n",
    "    torch.save(model.state_dict(), '../models/odr/odr_%d.pth' % seed)\n",
    "```\n",
    "\n",
    "Testing\n",
    "=======\n",
    "\n",
    "``` ipython\n",
    "if IF_BIASED_PHASES:\n",
    "    print('Biased ODR')\n",
    "    if IF_RAND_REF:\n",
    "        model_state_dict = torch.load('../models/odr/odr_bias_rand_ref_%d.pth' % seed )\n",
    "    else:\n",
    "        if class_weight == 'balanced':\n",
    "            model_state_dict = torch.load('../models/odr/odr_bias_%d_ref_%d_bal_loss.pth' % (reference, seed) )\n",
    "        else:\n",
    "            model_state_dict = torch.load('../models/odr/odr_bias_%d_ref_%d.pth' % (reference, seed) )\n",
    "else:\n",
    "    model_state_dict = torch.load('../models/odr/odr_%d.pth' % seed);\n",
    "\n",
    "model.load_state_dict(model_state_dict);\n",
    "model.eval();\n",
    "print(model.J_STP)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "model.N_BATCH = N_BATCH\n",
    "if IF_BIAS:\n",
    "    model.PHI0 = torch.zeros(size=(N_BATCH, 3, 1), device=DEVICE, dtype=torch.float)\n",
    "    if IF_BIMOD:\n",
    "        labels = continuous_bimodal_phases(N_BATCH, reference, sigma) * torch.pi / 180.0\n",
    "    else:\n",
    "        labels = continuous_biased_phases(N_BATCH, reference, sigma) * torch.pi / 180.0\n",
    "\n",
    "    model.PHI0[:, 0] = labels\n",
    "else:\n",
    "    labels = torch.randint(0, 360, (N_BATCH, 1)).to(DEVICE) * torch.pi / 180.0\n",
    "    model.PHI0 = torch.ones((N_BATCH, 2, 1), device=DEVICE, dtype=torch.float)\n",
    "    model.PHI0[:, 0] = labels\n",
    "\n",
    "ff_input = model.init_ff_input()\n",
    "print(model.PHI0.shape, ff_input.shape, labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "#     model.N_BATCH = N_BATCH\n",
    "#     ff_input = []\n",
    "#     labels = []\n",
    "\n",
    "#     model.PHI0 = torch.ones((N_BATCH, 2, 1), device=DEVICE, dtype=torch.float)\n",
    "\n",
    "#     for i in range(len(phase_list)):\n",
    "#         model.PHI0[:, 0] = phase_list[i]\n",
    "#         label = torch.ones(model.N_BATCH, device=DEVICE, dtype=torch.float) * phase_list[i] * torch.pi / 180.0\n",
    "\n",
    "#         labels.append(label)\n",
    "#         ff_input.append(model.init_ff_input())\n",
    "\n",
    "#     labels = torch.hstack(labels)\n",
    "#     ff_input = torch.vstack(ff_input)\n",
    "#     print('ff_input', ff_input.shape, 'labels', labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plt.hist(labels[:, 0].cpu() * 180 / np.pi, bins='auto', density=True)\n",
    "plt.xlabel('Target Loc. (°)')\n",
    "plt.ylabel('Density')\n",
    "plt.xticks(np.linspace(0, 360, 5))\n",
    "# plt.savefig('./figs/memhist/targets.svg', dpi=300)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "rates = model.forward(ff_input=ff_input).cpu().detach().numpy()\n",
    "print('ff_input', ff_input.shape)\n",
    "print('rates', rates.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plot_rates_selec(rates=ff_input.cpu().detach().numpy(), idx=20, thresh=.5)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plot_m0_m1_phi(model, ff_input.cpu().numpy()[..., model.slices[0]], 10)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plot_rates_selec(rates, idx=20, thresh=.5)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plot_m0_m1_phi(model, rates, 4)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "m0, m1, phi = decode_bump(rates, axis=-1)\n",
    "print(phi.shape, labels.shape)\n",
    "\n",
    "target_loc = labels.cpu().numpy()\n",
    "# print(target_loc.shape)\n",
    "\n",
    "errors = (phi - target_loc)\n",
    "errors = (errors + np.pi) % (2 * np.pi) - np.pi\n",
    "errors *= 180 / np.pi\n",
    "\n",
    "errors2 = errors[:, int((model.N_STIM_OFF[0].cpu().numpy()-model.N_STEADY) / model.N_WINDOW)]\n",
    "# print(errors2.shape)\n",
    "\n",
    "error_list = []\n",
    "for i in range(errors.shape[0]):\n",
    "    # idx_stim = model.start_indices[1, i%N_TARGETS].cpu().numpy()\n",
    "    idx_stim = model.start_indices[1, i].cpu().numpy()\n",
    "    idx = int((idx_stim - model.N_STEADY) / model.N_WINDOW)\n",
    "\n",
    "    error_list.append(errors[i, idx])\n",
    "# errors = errors[:, int((model.N_STIM_ON[1].cpu().numpy()-model.N_STEADY) / model.N_WINDOW)-1]\n",
    "errors = np.array(error_list)\n",
    "# print(errors.shape, errors2.shape, target_loc.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "targets = (target_loc + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=[2*width, height])\n",
    "# ax[0].hist(targets[:, 0] * 180 / np.pi , bins=32 , histtype='step')\n",
    "ax[0].hist(errors2, bins=32, histtype='step')\n",
    "ax[0].set_xlabel('Encoding Errors (°)')\n",
    "\n",
    "ax[1].hist(errors, bins=32)\n",
    "ax[1].set_xlabel('Memory Errors (°)')\n",
    "# ax[1].set_xlim([-45, 45])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Connectivity\n",
    "============\n",
    "\n",
    "``` ipython\n",
    "if IF_BIASED_PHASES:\n",
    "    print('Biased ODR')\n",
    "    if IF_RAND_REF:\n",
    "        model_state_dict = torch.load('../models/odr/odr_bias_rand_ref_%d.pth' % seed )\n",
    "    elif class_weight == 'balanced':\n",
    "        model_state_dict = torch.load('../models/odr/odr_bias_%d_ref_%d_bal_loss.pth' % (reference, seed) )\n",
    "    else:\n",
    "        model_state_dict = torch.load('../models/odr/odr_bias_%d_ref_%d.pth' % (reference, seed) )\n",
    "else:\n",
    "    print(seed)\n",
    "    model_state_dict = torch.load('../models/odr/odr_%d.pth' % seed);\n",
    "\n",
    "model.load_state_dict(model_state_dict);\n",
    "model.eval();\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "from src.lr_utils import LowRankWeights, clamp_tensor\n",
    "Cij = model.GAIN * ( model.W_stp_T  + model.Wab_train[model.slices[0], model.slices[0]])\n",
    "# Cij = model.Wab_train / model.Na[0] * model.J_STP\n",
    "# Cij[Cij>0]= 1\n",
    "Cij = clamp_tensor(Cij, 0, model.slices).cpu().detach().numpy()\n",
    "\n",
    "if IF_BIAS==0:\n",
    "    Cij0 = Cij\n",
    "    pkl_save(Cij0, 'matrix', path=\".\")\n",
    "    Kj0 = 0\n",
    "    Ki0 = 0\n",
    "else:\n",
    "    Cij0 = pkl_load('matrix', path=\".\")\n",
    "    Kj0 = pkl_load( 'Kj', path=\".\")\n",
    "    Ki0 = pkl_load( 'Ki', path=\".\")\n",
    "```\n",
    "\n",
    "``` example\n",
    "1\n",
    "```\n",
    "\n",
    ":END:\n",
    "\n",
    "``` ipython\n",
    "plt.figure(figsize=(2.5*width, 1.5*height))  # Set the figure size (width, height) in inches\n",
    "\n",
    "ax1 = plt.subplot2grid((2, 3), (0, 0), rowspan=2)\n",
    "im = ax1.imshow(Cij, cmap='jet', aspect=1, vmin=0)\n",
    "ax1.set_xlabel(\"Presynaptic\")\n",
    "ax1.set_ylabel(\"Postsynaptic\")\n",
    "\n",
    "# Second column, first row\n",
    "ax2 = plt.subplot2grid((2, 3), (0, 1))\n",
    "Kj = np.sum(Cij, axis=0)  # sum over pres\n",
    "ax2.plot(circcvl(Kj-Kj0, windowSize=75))\n",
    "# ax2.set_xticklabels([])\n",
    "ax2.set_ylabel(\"$K_j$\")\n",
    "\n",
    "# # Second column, second row\n",
    "ax3 = plt.subplot2grid((2, 3), (1, 1))\n",
    "Ki = np.sum(Cij, axis=1)  # sum over pres\n",
    "ax3.plot(circcvl(Ki-Ki0, windowSize=75))\n",
    "ax3.set_ylabel(\"$K_i$\")\n",
    "\n",
    "ax4 = plt.subplot2grid((2, 3), (0, 2), rowspan=2)\n",
    "diags = []\n",
    "for i in range(int(Cij.shape[0] / 2)):\n",
    "    diags.append(np.trace(Cij, offset=i) / Cij.shape[0])\n",
    "diags = np.array(diags)\n",
    "ax4.plot(diags)\n",
    "ax4.set_xlabel(\"Neuron #\")\n",
    "ax4.set_ylabel(\"$P_{ij}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "if IF_BIAS==0:\n",
    "    pkl_save(Kj, 'Kj', path=\".\")\n",
    "    pkl_save(Ki, 'Ki', path=\".\")\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "fig, ax = plt.subplots(1, 2, figsize=[2*width, height], sharey=1)\n",
    "\n",
    "Dij = Cij.flatten()\n",
    "np.random.shuffle(Dij)\n",
    "Dij = Dij.reshape(Cij.shape)\n",
    "\n",
    "im = ax[0].imshow(Dij, cmap='jet', aspect=1, vmin=0)\n",
    "ax[0].set_xlabel(\"Presynaptic\")\n",
    "ax[0].set_ylabel(\"Postsynaptic\")\n",
    "ax[0].set_title('Naive')\n",
    "# ax[0].set_xticks(np.linspace(0, 750, 4))\n",
    "# ax[0].set_yticks(np.linspace(0, 750, 4))\n",
    "\n",
    "im = ax[1].imshow(Cij, cmap='jet', aspect=1, vmin=0)\n",
    "ax[1].set_xlabel(\"Presynaptic\")\n",
    "ax[1].set_ylabel(\"Postsynaptic\")\n",
    "ax[1].set_title('Trained')\n",
    "# ax[1].set_xticks(np.linspace(0, 750, 4))\n",
    "# ax[1].set_yticks(np.linspace(0, 750, 4))\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "```"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
