{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am training a RNN. THe RNN has a static random connectivity and an additional low rank part. How can I train the net so that the low rank part stays weak in comparison to the random part\n",
    "\n",
    "One way to ensure that the low‐rank part of your recurrent weight matrix\n",
    "remains “weak” compared to the static random part is to regularize or\n",
    "scale its contribution during training. Here are a few strategies you\n",
    "can use:\n",
    "\n",
    "1.  Keep the random part fixed and train only the low‐rank component. By\n",
    "    not altering the static random connectivity, you guarantee that all\n",
    "    adaptation comes from the low‐rank update—and then you can\n",
    "    explicitly control its magnitude.\n",
    "\n",
    "2.  Scale the low‐rank weights by a small constant factor. When you\n",
    "    parameterize the weight matrix as the sum of a random matrix\n",
    "    W<sub>random</sub> and a low‐rank matrix W<sub>lowrank</sub>, you\n",
    "    might write:\n",
    "\n",
    "  W = W<sub>random</sub> + α W<sub>lowrank</sub> Choosing α ≪ 1 ensures\n",
    "that even if W<sub>lowrank</sub> changes during training, its overall\n",
    "contribution remains scaled down relative to the random part.\n",
    "\n",
    "1.  Apply regularization specifically on the low-rank component. You can\n",
    "    add an L2 (or even L1) penalty term to your loss that targets only\n",
    "    the low-rank parameters. That way, during gradient descent, you\n",
    "    penalize large deviations in the low-rank matrix and keep its\n",
    "    effective magnitude small relative to the fixed random connectivity.\n",
    "\n",
    "2.  Use gradient scaling or clipping. Another approach is to scale down\n",
    "    the gradients of the low-rank parameters before applying the update\n",
    "    so that even if the raw gradients are large, the effective update is\n",
    "    small. This prevents the low-rank part from “taking over” too\n",
    "    quickly.\n",
    "\n",
    "3.  Impose explicit constraints or projection steps. For instance, if\n",
    "    you are parameterizing W<sub>lowrank</sub> as an outer product U·Vᵀ,\n",
    "    you might project U and/or V onto balls of a fixed radius after each\n",
    "    update. This can enforce a hard constraint on the norm of the\n",
    "    low-rank part.\n",
    "\n",
    "In summary, by either pre-scaling, regularizing, or explicitly\n",
    "constraining the low-rank part of your network’s weights, you can ensure\n",
    "that the adaptive (or trained) low-rank contributions remain subdued\n",
    "relative to the static random part. Choose the strategy (or combination\n",
    "of strategies) that best fits your training dynamics and desired\n",
    "behavior.\n",
    "\n",
    "### How would you apply regularization?\n",
    "\n",
    "To regularize the low-rank part of your RNN, you can directly add a\n",
    "penalty term to your loss function that targets only the parameters\n",
    "corresponding to that component. Here are some detailed ways to do so:\n",
    "\n",
    "1.  L2 Regularization (Weight Decay):\n",
    "\n",
    " • Suppose your overall weight matrix is W = W<sub>random</sub> +\n",
    "αW<sub>lowrank</sub>, and you keep W<sub>random</sub> fixed while\n",
    "training only W<sub>lowrank</sub> (or its factors, if factorized as U\n",
    "and V). You can add an L2 penalty term to your loss function that\n",
    "penalizes the magnitude of W<sub>lowrank</sub>. For example, if your\n",
    "original loss function is ℒ, you modify it as:   Loss = ℒ +\n",
    "λ‖W<sub>lowrank</sub>‖²  Here, λ is a hyperparameter that determines how\n",
    "strongly you penalize large values in W<sub>lowrank</sub>. Choosing a\n",
    "sufficiently large λ will force the low-rank parameters to remain small.\n",
    "\n",
    "1.  L1 Regularization:\n",
    "\n",
    " • If you prefer to encourage sparsity in W<sub>lowrank</sub>, you can\n",
    "add an L1 penalty instead:   Loss = ℒ + λ‖W<sub>lowrank</sub>‖₁  This\n",
    "encourages many of the parameters in the low-rank part to get pushed\n",
    "toward zero, keeping its overall contribution limited.\n",
    "\n",
    "1.  Regularization on Factorized Components:\n",
    "\n",
    " • When parameterizing your low-rank component as, say,\n",
    "W<sub>lowrank</sub> = U·Vᵀ, you can regularize U and V individually:\n",
    "  Loss = ℒ + λ<sub>U</sub>‖U‖²<sub>F</sub> +\n",
    "λ<sub>V</sub>‖V‖²<sub>F</sub>  The hyperparameters λ<sub>U</sub> and\n",
    "λ<sub>V</sub> allow you to control the norms of U and V separately. You\n",
    "may set these to be relatively large so that the norms of U and V remain\n",
    "small, thereby limiting the overall size of W<sub>lowrank</sub>.\n",
    "\n",
    "1.  Adaptive Regularization:\n",
    "\n",
    " • You can also consider making the regularization coefficient adaptive\n",
    "based on training dynamics. For instance, if you notice that the\n",
    "contribution of the low-rank part is becoming too strong relative to\n",
    "W<sub>random</sub>, you might increase the regularization strength for\n",
    "that part in subsequent training iterations.\n",
    "\n",
    "1.  Implementation Considerations:\n",
    "\n",
    " • In most deep learning frameworks (PyTorch, TensorFlow, etc.), you can\n",
    "specify different parameter groups to apply different learning rates and\n",
    "weight decay values. By isolating the low-rank parameters into their own\n",
    "parameter group, you can assign them a larger weight decay (λ) compared\n",
    "to other parameters.  • Keep in mind that choosing an appropriate\n",
    "regularization coefficient may require some candidate testing or using a\n",
    "validation set to ensure you're not over-penalizing the low-rank part\n",
    "and hindering the network's ability to learn the desired behavior.\n",
    "\n",
    "By applying these regularization techniques, you essentially bias the\n",
    "training process to keep the magnitude of the low-rank updates small\n",
    "relative to the fixed random part, ensuring that your network retains\n",
    "the desired structural properties.\n",
    "\n",
    "### How would you implement this in torch\n",
    "\n",
    "Below is an example of how you might implement a custom RNN cell in\n",
    "PyTorch that uses a fixed random connectivity matrix plus a trainable\n",
    "low‐rank component. In this example, the low‐rank part is parameterized\n",
    "via two factor matrices U and V (so that W<sub>lowrank</sub> = U @ Vᵀ)\n",
    "with a scaling term. We then set up the optimizer to apply weight decay\n",
    "(L2 regularization) only to U and V.\n",
    "\n",
    "──────────────────────────── Code Example ────────────────────────────\n",
    "\\#!/usr/bin/env python3 import torch import torch.nn as nn import\n",
    "torch.optim as optim import torch.nn.functional as F\n",
    "\n",
    "class CustomRNNCell(nn.Module): def\n",
    "\\_<sub>init</sub>\\_<sub>(self,\\ input<sub>size</sub>,\\ hidden<sub>size</sub>,\\ low<sub>rankdim</sub>,\\ alpha=0.1)</sub>:\n",
    "super(CustomRNNCell, self).\\_<sub>init</sub>\\_<sub>()</sub>\n",
    "self.hidden<sub>size</sub> = hidden<sub>size</sub> self.alpha = alpha\n",
    "\n",
    "W<sub>random</sub> = torch.randn(hidden<sub>size</sub>,\n",
    "hidden<sub>size</sub>)\n",
    "self.register<sub>buffer</sub>('W<sub>random</sub>', W<sub>random</sub>)\n",
    "\\# stored as buffer, not trainable\n",
    "\n",
    "self.U = nn.Parameter(torch.randn(hidden<sub>size</sub>,\n",
    "low<sub>rankdim</sub>)) self.V =\n",
    "nn.Parameter(torch.randn(hidden<sub>size</sub>, low<sub>rankdim</sub>))\n",
    "\n",
    "self.input2hidden = nn.Linear(input<sub>size</sub>,\n",
    "hidden<sub>size</sub>)\n",
    "\n",
    "def forward(self, x, h<sub>prev</sub>):\n",
    "\n",
    "W<sub>lowrank</sub> = self.U @ self.V.t()\n",
    "\n",
    "W = self.W<sub>random</sub> + self.alpha \\* W<sub>lowrank</sub>\n",
    "\n",
    "h<sub>new</sub> = torch.tanh(self.input2hidden(x) +\n",
    "torch.matmul(h<sub>prev</sub>, W)) return h<sub>new</sub>\n",
    "\n",
    "Example usage:\n",
    "==============\n",
    "\n",
    "if <span class=\"underline\"><span class=\"underline\">name</span></span> ==\n",
    "\"<span class=\"underline\"><span class=\"underline\">main</span></span>\":\n",
    "\n",
    "input<sub>size</sub> = 20 hidden<sub>size</sub> = 50\n",
    "low<sub>rankdim</sub> = 5 \\# Controls the rank of the adaptive part\n",
    "alpha = 0.1 learning<sub>rate</sub> = 0.001 weight<sub>decay</sub> =\n",
    "1e-3 \\# Regularization strength for the low-rank parameters\n",
    "\n",
    "model = CustomRNNCell(input<sub>size</sub>, hidden<sub>size</sub>,\n",
    "low<sub>rankdim</sub>, alpha)\n",
    "\n",
    "optimizer = optim.Adam(\\[ {'params': \\[model.U, model.V\\],\n",
    "'weight<sub>decay</sub>': weight<sub>decay</sub>}, {'params':\n",
    "model.input2hidden.parameters()} \\# If there are others that might not\n",
    "need regularization. \\], lr=learning<sub>rate</sub>)\n",
    "\n",
    "x = torch.randn(10, input<sub>size</sub>) \\# batch of 10 time steps (or\n",
    "batch size) h = torch.zeros(10, hidden<sub>size</sub>) \\# initial hidden\n",
    "state\n",
    "\n",
    "for epoch in range(5): optimizer.zero<sub>grad</sub>() h = model(x, h)\n",
    "\n",
    "target = torch.zeros<sub>like</sub>(h) loss = F.mse<sub>loss</sub>(h,\n",
    "target)\n",
    "\n",
    "loss.backward() optimizer.step()\n",
    "\n",
    "print(\"Epoch:\", epoch, \"Loss:\", loss.item())\n",
    "\n",
    "──────────────────────────── Explanation ────────────────────────────\n",
    "\n",
    "1.  Fixed Random Component:\n",
    "\n",
    " • W<sub>random</sub> is created as a tensor and then registered as a\n",
    "buffer (via register<sub>buffer</sub>) so that it is part of the module\n",
    "but not updated during training.\n",
    "\n",
    "1.  Low-Rank Component:\n",
    "\n",
    " • Two parameters, U and V, represent the low-rank part. Their product\n",
    "gives W<sub>lowrank</sub>. The overall weight matrix is W =\n",
    "W<sub>random</sub> + α·W<sub>lowrank</sub>.  • The scaling factor α (set\n",
    "here to 0.1) keeps the low-rank contributions small relative to the\n",
    "fixed W<sub>random</sub>.\n",
    "\n",
    "1.  Regularization:\n",
    "\n",
    " • We use the optimizer’s built-in weight decay to apply L2\n",
    "regularization on U and V.  • By placing U and V in a parameter group\n",
    "with a specified weight<sub>decay</sub> value, we ensure that updates to\n",
    "these parameters are penalized. For parameters that should not be\n",
    "regularized (like input2hidden or any others), either omit\n",
    "weight<sub>decay</sub> or set it to 0.  • Alternatively, you could\n",
    "manually add the regularization term to your loss:   loss =\n",
    "original<sub>loss</sub> + λ/(torch.norm(model.U)\\*2 +\n",
    "torch.norm(model.V)\\*2)  but using the optimizer’s parameter groups is\n",
    "more convenient and leverages built-in functionality.\n",
    "\n",
    "1.  Training Loop:\n",
    "\n",
    " • The training loop is a standard PyTorch loop: zero<sub>grad</sub>,\n",
    "forward, compute loss, backward, optimizer.step().\n",
    "\n",
    "This implementation shows one straightforward way to keep the low-rank\n",
    "part of your RNN small via L2-regularization applied directly to its\n",
    "parameters using PyTorch.\n",
    "\n",
    "### What should I use for weight<sub>decay</sub>?\n",
    "\n",
    "There isn’t a one-size-fits-all answer for the value of\n",
    "weight<sub>decay</sub>—it’s a hyperparameter that typically needs to be\n",
    "tuned based on your model architecture, data, and specific training\n",
    "dynamics. However, here are some guidelines:\n",
    "\n",
    "1.  Start with a small value: For many RNN-based models,\n",
    "    weight<sub>decay</sub> values in the range of 1e-4 to 1e-3 often\n",
    "    work well. You might consider starting with 1e-3 if you need the\n",
    "    regularization to be strong, or 1e-4 if you want more relaxed\n",
    "    regularization.\n",
    "\n",
    "2.  Consider the scale of the low-rank component: Since the low-rank\n",
    "    parameters (U and V) are supposed to remain small relative to the\n",
    "    fixed random part, you may want to err on the side of a stronger\n",
    "    weight decay. If the magnitude of U and V tends to grow too much\n",
    "    during training, a higher weight<sub>decay</sub> might be necessary.\n",
    "\n",
    "3.  Monitor training and validation: Keep an eye on both training and\n",
    "    validation losses. A too-high weight<sub>decay</sub> might under-fit\n",
    "    by forcing the parameters to be excessively small, while a too-low\n",
    "    value might allow them to grow beyond what you intend.\n",
    "\n",
    "4.  Use grid search or other hyperparameter tuning methods: Experiment\n",
    "    with a few different values (e.g., 1e-4, 5e-4, 1e-3, etc.) and\n",
    "    validate their performance.\n",
    "\n",
    "5.  Compare with additional regularization techniques: Sometimes a\n",
    "    combination of weight<sub>decay</sub> along with manual scaling or\n",
    "    gradient clipping (for the low-rank parameters) may yield better\n",
    "    tuning control.\n",
    "\n",
    "In summary, begin with a baseline (say 1e-3), and then adjust based on\n",
    "how the low-rank component behaves during training and what you observe\n",
    "on your validation set.\n",
    "\n",
    "### I would like the weights in W<sub>lowrank</sub> to scale as 1/K\n",
    "\n",
    "One straightforward way to ensure that the contribution from the\n",
    "low‑rank part scales as 1/K—where K is the rank (i.e., the\n",
    "dimensionality of your low‑rank factors)—is to explicitly multiply the\n",
    "low‑rank matrix by 1/K in its computation. For example, if you are\n",
    "representing your low‑rank component as an outer product of two matrices\n",
    "U (of size \\[N, K\\]) and V (of size \\[N, K\\]), you can compute it as\n",
    "\n",
    "  W<sub>lowrank</sub> = (1/K) \\* (U @ Vᵀ).\n",
    "\n",
    "This explicitly scales the overall magnitude of the low‑rank part as\n",
    "1/K, regardless of the dimensions of U and V.\n",
    "\n",
    "Below is a modified version of our earlier PyTorch example that\n",
    "incorporates this scaling:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Code Example:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "\\#!/usr/bin/env python3 import torch import torch.nn as nn import\n",
    "torch.optim as optim import torch.nn.functional as F\n",
    "\n",
    "class CustomRNNCell(nn.Module): def\n",
    "\\_<sub>init</sub>\\_<sub>(self,\\ input<sub>size</sub>,\\ hidden<sub>size</sub>,\\ low<sub>rankdim</sub>)</sub>:\n",
    "super(CustomRNNCell, self).\\_<sub>init</sub>\\_<sub>()</sub>\n",
    "self.hidden<sub>size</sub> = hidden<sub>size</sub>\n",
    "self.low<sub>rankdim</sub> = low<sub>rankdim</sub> \\# K\n",
    "\n",
    "W<sub>random</sub> = torch.randn(hidden<sub>size</sub>,\n",
    "hidden<sub>size</sub>)\n",
    "self.register<sub>buffer</sub>('W<sub>random</sub>', W<sub>random</sub>)\n",
    "\n",
    "self.U = nn.Parameter(torch.randn(hidden<sub>size</sub>,\n",
    "low<sub>rankdim</sub>)) self.V =\n",
    "nn.Parameter(torch.randn(hidden<sub>size</sub>, low<sub>rankdim</sub>))\n",
    "\n",
    "self.input2hidden = nn.Linear(input<sub>size</sub>,\n",
    "hidden<sub>size</sub>)\n",
    "\n",
    "def forward(self, x, h<sub>prev</sub>):\n",
    "\n",
    "W<sub>lowrank</sub> = (1.0 / self.low<sub>rankdim</sub>) \\* (self.U @\n",
    "self.V.t())\n",
    "\n",
    "W = self.W<sub>random</sub> + W<sub>lowrank</sub>\n",
    "\n",
    "h<sub>new</sub> = torch.tanh(self.input2hidden(x) +\n",
    "torch.matmul(h<sub>prev</sub>, W)) return h<sub>new</sub>\n",
    "\n",
    "if <span class=\"underline\"><span class=\"underline\">name</span></span> ==\n",
    "\"<span class=\"underline\"><span class=\"underline\">main</span></span>\":\n",
    "\n",
    "input<sub>size</sub> = 20 hidden<sub>size</sub> = 50\n",
    "low<sub>rankdim</sub> = 5 \\# K value learning<sub>rate</sub> = 0.001\n",
    "weight<sub>decay</sub> = 1e-3 \\# Regularization strength for the\n",
    "low-rank parameters\n",
    "\n",
    "model = CustomRNNCell(input<sub>size</sub>, hidden<sub>size</sub>,\n",
    "low<sub>rankdim</sub>)\n",
    "\n",
    "optimizer = optim.Adam(\\[ {'params': \\[model.U, model.V\\],\n",
    "'weight<sub>decay</sub>': weight<sub>decay</sub>}, {'params':\n",
    "model.input2hidden.parameters()} \\], lr=learning<sub>rate</sub>)\n",
    "\n",
    "batch<sub>size</sub> = 10 x = torch.randn(batch<sub>size</sub>,\n",
    "input<sub>size</sub>) h = torch.zeros(batch<sub>size</sub>,\n",
    "hidden<sub>size</sub>)\n",
    "\n",
    "for epoch in range(5): optimizer.zero<sub>grad</sub>() h = model(x, h)\n",
    "target = torch.zeros<sub>like</sub>(h) \\# Just for demonstration\n",
    "purposes. loss = F.mse<sub>loss</sub>(h, target) loss.backward()\n",
    "optimizer.step() print(\"Epoch:\", epoch, \"Loss:\", loss.item())\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Explanation:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "1.  In the forward() method, after computing W<sub>lowrank</sub> as U @\n",
    "    Vᵀ, we multiply by (1.0 / self.low<sub>rankdim</sub>). This enforces\n",
    "    that as the rank dimension (K) increases, the overall magnitude of\n",
    "    the low-rank part scales as 1/K.\n",
    "\n",
    "2.  The fixed random weight matrix remains unchanged, and the overall\n",
    "    recurrent weight matrix is computed as W = W<sub>random</sub> +\n",
    "    W<sub>lowrank</sub>.\n",
    "\n",
    "3.  We still use the optimizer's built-in weight<sub>decay</sub> option\n",
    "    on U and V so that besides the scaling factor of 1/K, the optimizer\n",
    "    discourages U and V from growing too large.\n",
    "\n",
    "With this approach, you ensure that your low‑rank updates intrinsically\n",
    "have the desired scaling behavior.\n",
    "\n",
    "### Can I use regularization to maitain U and V at order/scale 1\n",
    "\n",
    "Yes, you can use regularization—and even projection techniques—to help\n",
    "keep U and V at an order-of-magnitude around 1 during training. Here are\n",
    "a few strategies:\n",
    "\n",
    "1.  L2 Regularization (Weight Decay):\n",
    "\n",
    " • By adding L2 regularization on U and V to your loss function (or\n",
    "using the optimizer’s weight<sub>decay</sub> for those parameters), you\n",
    "penalize large values in these matrices. Often, this encourages their\n",
    "norms to stay around O(1) rather than growing. You control the strength\n",
    "with a hyperparameter (λ) that you might need to tune.\n",
    "\n",
    "1.  Explicit Norm Constraint (Projection):\n",
    "\n",
    " • After each update, you can project U and/or V so that their norms\n",
    "reflect your desired scale. For example, suppose you want each parameter\n",
    "matrix’s Frobenius norm to be near 1; you can do:   if norm(U) \\>\n",
    "threshold: U = U / norm(U)  • This hard constraint forces the matrices\n",
    "to be rescaled after each update, maintaining a roughly constant scale.\n",
    "\n",
    "1.  Adaptive Regularization:\n",
    "\n",
    " • Sometimes it helps to monitor the norms of U and V during training.\n",
    "If you find that their norms are drifting, you can adaptively adjust the\n",
    "regularization strength (or even combine regularization with occasional\n",
    "re-normalization steps) to nudge them back to the desired scale.\n",
    "\n",
    "In PyTorch, using weight<sub>decay</sub> within your optimizer is\n",
    "straightforward and often sufficient if tuned correctly. Here’s a brief\n",
    "snippet that sets weight<sub>decay</sub> for U and V:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Example:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "optimizer = optim.Adam(\\[ {'params': \\[model.U, model.V\\],\n",
    "'weight<sub>decay</sub>': 1e-3}, \\# adjust 1e-3 as needed {'params':\n",
    "model.input2hidden.parameters()} \\], lr=learning<sub>rate</sub>)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "If you want to add an explicit norm projection after each optimizer\n",
    "step, you could do something like this in your training loop:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "for epoch in range(num<sub>epochs</sub>):\n",
    "optimizer.zero<sub>grad</sub>() output = model(x, h) loss =\n",
    "F.mse<sub>loss</sub>(output, target) loss.backward() optimizer.step()\n",
    "\n",
    "with torch.no<sub>grad</sub>(): U<sub>norm</sub> = model.U.norm(p='fro')\n",
    "V<sub>norm</sub> = model.V.norm(p='fro') desired<sub>norm</sub> = 1.0 \\#\n",
    "or any other value you consider “order 1” if U<sub>norm</sub> \\> 0:\n",
    "model.U.mul<sub>(desired<sub>norm</sub>\\ /\\ U<sub>norm</sub>)</sub> if\n",
    "V<sub>norm</sub> \\> 0:\n",
    "model.V.mul<sub>(desired<sub>norm</sub>\\ /\\ V<sub>norm</sub>)</sub>\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "By applying either (or a combination) of these strategies, you can keep\n",
    "the matrices U and V from drifting to large values during training,\n",
    "thereby maintaining their scales close to 1. This in turn helps your\n",
    "low-rank update (scaled appropriately, for instance by 1/K if that’s\n",
    "what you prefer) to stay in the desired regime relative to the rest of\n",
    "your weights.\n",
    "\n",
    "### frobenius is norm 2\n",
    "\n",
    "That's right—the Frobenius norm is essentially an L2 norm for matrices.\n",
    "In PyTorch, when calling tensor.norm(p='fro'), you compute the square\n",
    "root of the sum of the squares of all elements, which is equivalent to\n",
    "treating the matrix as a long vector and computing its L2 norm.\n",
    "\n",
    "If your goal is to keep U and V at a consistent scale (say, unit norm\n",
    "under the Frobenius norm), then regularizing with weight decay penalizes\n",
    "the squared Frobenius norm (since weight decay applies an L2 penalty),\n",
    "or you can use explicit projection after the optimization step.\n",
    "\n",
    "For example, using weight decay means that during optimization the loss\n",
    "gets an extra term proportional to U.norm(p='fro')² (and similarly for\n",
    "V). This pushes the optimizer to keep their magnitude small.\n",
    "Alternatively, if you want a hard constraint, you can project:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "with torch.no<sub>grad</sub>(): desired<sub>norm</sub> = 1.0 \\# Target\n",
    "scale U<sub>norm</sub> = model.U.norm(p='fro') if U<sub>norm</sub> \\> 0:\n",
    "model.U.mul<sub>(desired<sub>norm</sub>\\ /\\ U<sub>norm</sub>)</sub>\n",
    "V<sub>norm</sub> = model.V.norm(p='fro') if V<sub>norm</sub> \\> 0:\n",
    "model.V.mul<sub>(desired<sub>norm</sub>\\ /\\ V<sub>norm</sub>)</sub>\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "This projection forces each update to rescale U and V so that their\n",
    "Frobenius (L2) norm stays at 1.\n",
    "\n",
    "Thus, whether you use weight decay (a soft regularization approach) or\n",
    "explicit projection (a hard constraint), you're effectively controlling\n",
    "the L2, or Frobenius, norm of U and V.\n",
    "\n",
    "### does this makes sure that the norm is not 0\n",
    "\n",
    "The projection code only scales U or V if the norm is greater than zero.\n",
    "For example, in this code:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "with torch.no<sub>grad</sub>(): desired<sub>norm</sub> = 1.0 \\# Target\n",
    "scale U<sub>norm</sub> = model.U.norm(p='fro') if U<sub>norm</sub> \\> 0:\n",
    "model.U.mul<sub>(desired<sub>norm</sub>\\ /\\ U<sub>norm</sub>)</sub>\n",
    "V<sub>norm</sub> = model.V.norm(p='fro') if V<sub>norm</sub> \\> 0:\n",
    "model.V.mul<sub>(desired<sub>norm</sub>\\ /\\ V<sub>norm</sub>)</sub>\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "if the norm is exactly zero, the condition (U<sub>norm</sub> \\> 0) fails\n",
    "and no scaling is applied. That means if U (or V) becomes exactly zero,\n",
    "the projection doesn't change it—it remains zero.\n",
    "\n",
    "In practice, it is quite unlikely for parameters to end up exactly zero\n",
    "during training when using gradient-based methods and standard\n",
    "initializations. However, if you want to absolutely ensure that the norm\n",
    "never becomes exactly zero, you might incorporate a safeguard such as\n",
    "adding a small epsilon before dividing. For example:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "with torch.no<sub>grad</sub>(): desired<sub>norm</sub> = 1.0 \\# Target\n",
    "scale eps = 1e-8 U<sub>norm</sub> = model.U.norm(p='fro')\n",
    "model.U.mul<sub>(desired<sub>norm</sub>\\ /\\ (U<sub>norm</sub>\\ +\\ eps))</sub>\n",
    "V<sub>norm</sub> = model.V.norm(p='fro')\n",
    "model.V.mul<sub>(desired<sub>norm</sub>\\ /\\ (V<sub>norm</sub>\\ +\\ eps))</sub>\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "In this version, even when U<sub>norm</sub> or V<sub>norm</sub> is zero,\n",
    "the addition of epsilon prevents a division by zero error and ensures\n",
    "the operation completes. Note that if the weights are exactly zero,\n",
    "multiplying by desired<sub>norm</sub>/(eps) could suddenly inflate those\n",
    "parameters. So when using such a safeguard, it's important to monitor\n",
    "the effect on training.\n",
    "\n",
    "Thus, while the basic projection code doesn't guarantee the weights\n",
    "won't be zero, in typical scenarios it is sufficient, and you can always\n",
    "add extra safeguards if necessary.\n",
    "\n",
    "**\\***"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
