{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have a gaussian distribution of angles between 0 and 360. It is peaked at a reference. How\n",
    "\n",
    "do I exctract a uniform distribution from it? I was thinking of using\n",
    "Inverse transform sampling\n",
    "\n",
    "To extract a uniform distribution from a Gaussian distribution of\n",
    "angles, you can indeed use the inverse transform sampling method, but\n",
    "with some adjustments given that the original distribution is not\n",
    "uniform.\n",
    "\n",
    "Here’s a concise approach:\n",
    "\n",
    "1.  **Normalize the Gaussian**: Ensure your Gaussian distribution's\n",
    "    probability density function (PDF) is normalized over the range \\[0,\n",
    "    360\\] degrees.\n",
    "\n",
    "2.  **Compute the Cumulative Distribution Function (CDF)**: Determine\n",
    "    the CDF of your Gaussian distribution over the range. This should be\n",
    "    straightforward using numerical integration or using a cumulative\n",
    "    normal distribution formula adjusted for your mean and variance.\n",
    "\n",
    "3.  **Apply Inverse Transform Sampling**:\n",
    "\n",
    "    -   Draw a uniform random sample $ u $ from the interval \\[0, 1\\].\n",
    "    -   Use the inverse of the CDF to find the corresponding angle, that\n",
    "        is, $ \\theta = CDF^{-1}(u) $.\n",
    "\n",
    "Because the original Gaussian is bounded and wraps at 360 degrees, pay\n",
    "careful attention to edge cases where numerical inaccuracies might cause\n",
    "values near 0 or 360 to behave unexpectedly. You may want to numerically\n",
    "compute or piecewise define the inverse if a closed form is not\n",
    "available. Also, ensure that any angles wrap correctly between 0 and 360\n",
    "by using modulo arithmetic where needed.\n",
    "\n",
    "### How do you implement this in python\n",
    "\n",
    "To implement inverse transform sampling in Python to extract samples\n",
    "from a Gaussian distribution of angles and map them to a uniform\n",
    "distribution, you'll typically follow these steps:\n",
    "\n",
    "1.  Define your Gaussian PDF over the range \\[0, 360\\].\n",
    "2.  Compute its CDF.\n",
    "3.  Use the inverse of this CDF (or a numerical approximation) to\n",
    "    transform uniform random samples to follow the Gaussian\n",
    "    distribution.\n",
    "\n",
    "Here is a simple implementation outline using libraries like `numpy` and\n",
    "`scipy`:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Parameters for the Gaussian distribution\n",
    "mean_angle = 180  # example mean angle\n",
    "stddev_angle = 30  # example standard deviation\n",
    "\n",
    "# Define a function for the wrapped Gaussian PDF\n",
    "def wrapped_gaussian_pdf(angles, mean, stddev):\n",
    "    return norm.pdf((angles + 360) % 360, mean, stddev)\n",
    "\n",
    "# Define the CDF using numerical integration\n",
    "def wrapped_gaussian_cdf(angles, mean, stddev):\n",
    "    # Normalization to ensure the distribution sums to 1 over the [0, 360) range\n",
    "    angle_range = np.linspace(0, 360, 1000)\n",
    "    pdf_values = wrapped_gaussian_pdf(angle_range, mean, stddev)\n",
    "    cdf_values = np.cumsum(pdf_values) / np.sum(pdf_values)\n",
    "    return np.interp(angles, angle_range, cdf_values)\n",
    "\n",
    "# Inverse transform sampling\n",
    "def inverse_transform_sampling(num_samples, mean, stddev):\n",
    "    # Generate uniform samples\n",
    "    uniform_samples = np.random.rand(num_samples)\n",
    "\n",
    "    # Map the uniform samples to Gaussian distributed angles using the CDF\n",
    "    angle_range = np.linspace(0, 360, 1000)\n",
    "    cdf_values = wrapped_gaussian_cdf(angle_range, mean, stddev)\n",
    "    sampled_angles = np.interp(uniform_samples, cdf_values, angle_range)  # inverse CDF\n",
    "\n",
    "    return sampled_angles\n",
    "\n",
    "# Example usage\n",
    "num_samples = 1000\n",
    "uniform_samples = inverse_transform_sampling(num_samples, mean_angle, stddev_angle)\n",
    "\n",
    "# 'uniform_samples' should now contain samples extracted from the Gaussian distribution\n",
    "# mapped back into uniform distribution space over [0, 360).\n",
    "print(uniform_samples)\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "-   **Numerical Integration**: The CDF is computed numerically since no\n",
    "    closed form is straight forward due to the wrap-around nature.\n",
    "-   **Interpolation**: `numpy.interp` is used to approximate the inverse\n",
    "    CDF. This works by inverting the mapping from the uniform samples to\n",
    "    the angles.\n",
    "-   **Normalization**: Ensure that your PDF is properly normalized\n",
    "    across the interval to accurately reflect the wrapped nature.\n",
    "\n",
    "This approach uses interpolation and numerical techniques appropriate\n",
    "for the fact that the distribution is bounded and needs careful handling\n",
    "at the edges.\n",
    "\n",
    "### This still gives me a gaussian distribution\n",
    "\n",
    "My apologies for the confusion. It seems I misunderstood your original\n",
    "request. If you want to transform a Gaussian distribution into a uniform\n",
    "distribution, a direct approach like applying inverse transform sampling\n",
    "will not achieve this because it's used to generate samples following\n",
    "the original distribution, rather than transforming a given distribution\n",
    "itself.\n",
    "\n",
    "Instead, if your data is Gaussian-distributed and you want to\n",
    "effectively \"flatten\" it to become uniformly distributed, you can do the\n",
    "following:\n",
    "\n",
    "1.  Normalize your Gaussian distribution data using its CDF to map it to\n",
    "    a uniform distribution in the unit interval \\[0,1\\].\n",
    "2.  Scale this to your desired interval, such as \\[0, 360\\].\n",
    "\n",
    "Here’s how you might achieve this transformation using Python:\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Original Gaussian parameters\n",
    "mean_angle = 180\n",
    "stddev_angle = 30\n",
    "\n",
    "# Generate Gaussian-distributed data\n",
    "num_samples = 1000\n",
    "gaussian_data = np.random.normal(loc=mean_angle, scale=stddev_angle, size=num_samples)\n",
    "\n",
    "# Convert Gaussian data to uniform distribution using its CDF\n",
    "uniform_data_01 = norm.cdf(gaussian_data, loc=mean_angle, scale=stddev_angle)\n",
    "\n",
    "# Map the uniform distribution [0,1] to [0, 360]\n",
    "uniform_data_scaled = uniform_data_01 * 360\n",
    "\n",
    "print(uniform_data_scaled)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "-   **`norm.cdf(...)`**: This step maps the Gaussian data to a uniform\n",
    "    distribution over \\[0,1\\]. The CDF computes the probability\n",
    "    (fraction) that the Gaussian variable is below a certain value,\n",
    "    which maps directly to a uniform distribution.\n",
    "\n",
    "-   **Scaling**: After the data is converted to a uniform \\[0,1\\]\n",
    "    distribution, multiply by 360 to stretch this to the desired angular\n",
    "    range.\n",
    "\n",
    "This method will effectively \"flatten\" the Gaussian to uniform. Please\n",
    "make sure that the distribution properties (e.g., its domain and\n",
    "wrap-around behavior) are correctly accounted for during this process.\n",
    "\n",
    "### But I get the indices from the original distribution that are kept in the new sample? I can't just generate new data.\n",
    "\n",
    "To address your requirement where you need to transform existing\n",
    "Gaussian-distributed data into a uniform distribution while keeping\n",
    "track of indices, you can follow this procedure, which will leverage the\n",
    "cumulative distribution function (CDF) to map your existing data into a\n",
    "uniform distribution:\n",
    "\n",
    "1.  Compute the CDF values of your existing data.\n",
    "2.  Use these CDF values as your new uniform samples.\n",
    "3.  Optionally scale these uniform samples to your desired range, such\n",
    "    as \\[0, 360\\].\n",
    "\n",
    "Here's how you can do this while maintaining the indices of the original\n",
    "data:\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Original Gaussian-distributed data (e.g., angles)\n",
    "mean_angle = 180\n",
    "stddev_angle = 30\n",
    "num_samples = 1000\n",
    "\n",
    "# Simulate Gaussian data (for demonstration; replace with your actual data)\n",
    "gaussian_data = np.random.normal(loc=mean_angle, scale=stddev_angle, size=num_samples)\n",
    "\n",
    "# Sort the data to properly map their CDF values\n",
    "sorted_indices = np.argsort(gaussian_data)\n",
    "sorted_data = gaussian_data[sorted_indices]\n",
    "\n",
    "# Compute the CDF values for the sorted data\n",
    "cdf_values = norm.cdf(sorted_data, loc=mean_angle, scale=stddev_angle)\n",
    "\n",
    "# Map CDF values to [0, 360] to get uniform distribution\n",
    "uniform_mapped = cdf_values * 360\n",
    "\n",
    "# Reorder the uniform distribution to match original indices\n",
    "uniform_data_with_original_indices = np.empty_like(uniform_mapped)\n",
    "uniform_data_with_original_indices[sorted_indices] = uniform_mapped\n",
    "\n",
    "# Output the uniform distribution and original indices\n",
    "print(uniform_data_with_original_indices)\n",
    "```\n",
    "\n",
    "This approach retains the original indices of the data:\n",
    "\n",
    "-   **Sorting**: By sorting the original data, you can calculate the CDF\n",
    "    values in a properly ordered manner, ensuring accuracy in mapping to\n",
    "    a uniform distribution.\n",
    "\n",
    "-   **Index Tracking**: Use `np.argsort` to keep track of the original\n",
    "    indices, and then reorder the mapped data back to the original order\n",
    "    using these indices.\n",
    "\n",
    "This method transforms your existing Gaussian-distributed data to a\n",
    "uniform distribution by leveraging the properties of the CDF to map\n",
    "percentiles properly. This ensures you are not generating new data and\n",
    "can track transformations to the original samples.\n",
    "\n",
    "### This is not exactly what I want. I want to subsample from the original gaussian and the subsample angles to be uniformly distributed\n",
    "\n",
    "To subsample from an existing Gaussian distribution in such a way that\n",
    "the angles in the subsample are uniformly distributed, you can use the\n",
    "following approach:\n",
    "\n",
    "1.  **Calculate the CDF**: Compute the cumulative distribution function\n",
    "    (CDF) for your Gaussian data.\n",
    "\n",
    "2.  **Generate Uniform Random Values**: Generate uniform random values\n",
    "    that cover the complete domain of the CDF.\n",
    "\n",
    "3.  **Inverse of CDF**: Use these uniform random values to find\n",
    "    corresponding values in your Gaussian data by inverting the CDF.\n",
    "\n",
    "Here’s how to implement it in Python:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Original Gaussian-distributed data (angles)\n",
    "mean_angle = 180\n",
    "stddev_angle = 30\n",
    "num_samples = 1000\n",
    "\n",
    "# Simulate Gaussian data (for demonstration; replace with your actual data)\n",
    "gaussian_data = np.random.normal(loc=mean_angle, scale=stddev_angle, size=num_samples)\n",
    "\n",
    "# Sort the data and compute CDF values\n",
    "sorted_indices = np.argsort(gaussian_data)\n",
    "sorted_data = gaussian_data[sorted_indices]\n",
    "cdf_values = norm.cdf(sorted_data, loc=mean_angle, scale=stddev_angle)\n",
    "\n",
    "# Generate uniformly spaced CDF values corresponding to the number of samples you want to extract\n",
    "sample_size = 500  # Number of uniformly distributed samples needed\n",
    "uniform_cdf_values = np.linspace(0, 1, sample_size, endpoint=False)[1:]  # Avoid exact 0 and 1 for CDF\n",
    "\n",
    "# Find the indices where these uniform CDF values would fit into the sorted CDF\n",
    "subsampled_indices = np.searchsorted(cdf_values, uniform_cdf_values)\n",
    "\n",
    "# Get the corresponding samples from the original data\n",
    "uniformly_distributed_samples = sorted_data[subsampled_indices]\n",
    "\n",
    "# Optionally restore to original positions\n",
    "original_indices_subsampled = sorted_indices[subsampled_indices]\n",
    "\n",
    "# Output the subsampled angles and their indices in the original dataset\n",
    "print(\"Subsampled Angles:\", uniformly_distributed_samples)\n",
    "print(\"Original Indices of Subsampled Data:\", original_indices_subsampled)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "-   **Sorting**: First, sort your Gaussian data to compute the proper\n",
    "    CDF values.\n",
    "\n",
    "-   **Uniform CDF Values**: Generate uniformly spaced CDF values\n",
    "    corresponding to the part of the distribution you want to sample.\n",
    "\n",
    "-   **Indexing with `np.searchsorted`**: Find indices in the sorted CDF\n",
    "    that approximate these uniform CDF values, effectively mapping\n",
    "    uniform distribution to your Gaussian data.\n",
    "\n",
    "-   **Subsample Extraction**: Use these indices to extract the subsample\n",
    "    from your sorted dataset, resulting in a uniform distribution of\n",
    "    subsampled angles.\n",
    "\n",
    "This approach allows you to extract a uniformly distributed subsample\n",
    "from an existing set of Gaussian-distributed angles without generating\n",
    "new data.\n",
    "\n",
    "### This does not work\n",
    "\n",
    "I apologize for the previous confusion. Let's try a different approach\n",
    "to subsample from your existing Gaussian distribution such that the\n",
    "subsample should have its values (angles) uniformly distributed across\n",
    "the range. Here’s a method that might better achieve what you’re aiming\n",
    "for:\n",
    "\n",
    "1.  **Sort and Rank Data**: Start by sorting the Gaussian data. This\n",
    "    will allow us to map ranks to intervals.\n",
    "\n",
    "2.  **Partition into Equal Intervals**: The sorted data is partitioned\n",
    "    so that equal numbers of samples fall into each interval that spans\n",
    "    from the minimum to the maximum value.\n",
    "\n",
    "3.  **Randomly Select Within Each Interval**: From each interval,\n",
    "    randomly pick a sample. This ensures uniformity in the overall\n",
    "    subsample distribution.\n",
    "\n",
    "Here is the implementation in Python:\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "\n",
    "# Example Gaussian-distributed data\n",
    "mean_angle = 180\n",
    "stddev_angle = 30\n",
    "num_samples = 1000\n",
    "\n",
    "# Simulate Gaussian data (for demonstration; replace with your actual data)\n",
    "gaussian_data = np.random.normal(loc=mean_angle, scale=stddev_angle, size=num_samples)\n",
    "\n",
    "# Number of uniform samples you want\n",
    "sample_size = 100\n",
    "\n",
    "# Sort data\n",
    "sorted_data = np.sort(gaussian_data)\n",
    "\n",
    "# Partition into uniform intervals\n",
    "step = num_samples // sample_size\n",
    "\n",
    "# Choose one random index from each interval\n",
    "selected_indices = [np.random.randint(i * step, (i + 1) * step) for i in range(sample_size)]\n",
    "subsample = sorted_data[selected_indices]\n",
    "\n",
    "plt.hist(subsample)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "-   **Sorting**: The data is sorted to make partitioning effective.\n",
    "\n",
    "-   **Equal Intervals**: The total number of samples is divided by the\n",
    "    desired sample size, determining step size, which defines the\n",
    "    intervals in the sorted data.\n",
    "\n",
    "-   **Random Selection**: Within each interval, a random selection\n",
    "    guarantees that each interval contributes to the final uniform\n",
    "    subsample, simulating a more uniform distribution overall.\n",
    "\n",
    "This approach ensures the selected subsample is spread across the entire\n",
    "range of the original dataset, approximating a uniform distribution by\n",
    "selecting from equal quantiles. Adjust the `sample_size` parameter to\n",
    "control the granularity of uniformity.\n",
    "\n",
    "### the sample is clearly gaussian\n",
    "\n",
    "I apologize for the oversight. Let's refine the approach to achieve the\n",
    "desired uniform distribution from your Gaussian sample:\n",
    "\n",
    "1.  **Histogram Binning**: Divide the range of your data into\n",
    "    equal-sized bins.\n",
    "2.  **Select a Fixed Number of Samples From Each Bin**: Randomly pick a\n",
    "    fixed number of samples from each bin to ensure uniform\n",
    "    representation across the bins.\n",
    "\n",
    "Here's the corrected implementation:\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Original Gaussian-distributed data\n",
    "mean_angle = 180\n",
    "stddev_angle = 30\n",
    "num_samples = 1000\n",
    "\n",
    "# Simulate Gaussian data (replace with your actual data)\n",
    "gaussian_data = np.random.normal(loc=mean_angle, scale=stddev_angle, size=num_samples)\n",
    "\n",
    "# Number of bins and subsamples per bin\n",
    "num_bins = 10\n",
    "samples_per_bin = 10\n",
    "\n",
    "# Create histogram bins\n",
    "counts, bin_edges = np.histogram(gaussian_data, bins=num_bins)\n",
    "\n",
    "# Collect uniform samples from each bin\n",
    "subsample = []\n",
    "for i in range(num_bins):\n",
    "    bin_mask = (gaussian_data >= bin_edges[i]) & (gaussian_data < bin_edges[i+1])\n",
    "    bin_data = gaussian_data[bin_mask]\n",
    "    if len(bin_data) >= samples_per_bin:\n",
    "        chosen_samples = np.random.choice(bin_data, samples_per_bin, replace=False)\n",
    "    else:\n",
    "        chosen_samples = bin_data  # take whatever is available for smaller bins\n",
    "    subsample.extend(chosen_samples)\n",
    "\n",
    "plt.hist(subsample, bins=num_bins)\n",
    "plt.title(\"Uniform Subsampled Data\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "-   **Histogram Binning**: The data is divided into bins to partition it\n",
    "    into segments.\n",
    "\n",
    "-   **Random Sampling**: A fixed number of points are randomly selected\n",
    "    from each bin to ensure each bin contributes equally to the final\n",
    "    subsample, leading to a uniform distribution.\n",
    "\n",
    "-   **Handling Sparse Bins**: If a bin has fewer points than required,\n",
    "    all available points are taken.\n",
    "\n",
    "This approach attempts to create uniformity by sampling evenly across\n",
    "the distribution’s span. You may need to adjust `num_bins` and\n",
    "`samples_per_bin` to suit your dataset's characteristics.\n",
    "\n",
    "**\\***\n",
    "\n",
    "To extract indices from the original Gaussian sample that would form a\n",
    "uniform distribution when transformed, you can use the following\n",
    "approach:\n",
    "\n",
    "1.  **Generate Gaussian Samples**: First, create your\n",
    "    Gaussian-distributed data.\n",
    "2.  **Sort the Samples**: Sort these samples and associate the original\n",
    "    indices.\n",
    "3.  **Select the Indices Using Uniform Quantiles**: Select indices at\n",
    "    uniform intervals from the sorted array.\n",
    "\n",
    "Here's a Python implementation:\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "\n",
    "# Parameters for the Gaussian distribution\n",
    "mean_angle = 180\n",
    "stddev_angle = 30\n",
    "\n",
    "# Generate Gaussian-distributed data\n",
    "num_samples = 1000\n",
    "gaussian_data = np.random.normal(loc=mean_angle, scale=stddev_angle, size=num_samples)\n",
    "\n",
    "# Track indices of the original data points\n",
    "indices = np.arange(num_samples)\n",
    "\n",
    "# Sort the data and keep track of indices\n",
    "sorted_indices = np.argsort(gaussian_data)\n",
    "sorted_data = gaussian_data[sorted_indices]\n",
    "\n",
    "# Desired number of uniform samples\n",
    "num_uniform_samples = 100\n",
    "\n",
    "# Calculate indices for uniform sampling using quantiles\n",
    "uniform_sampled_indices = np.round(np.linspace(0, num_samples - 1, num_uniform_samples)).astype(int)\n",
    "\n",
    "# Get the original indices that correspond to the uniform samples\n",
    "original_indices_for_uniform = sorted_indices[uniform_sampled_indices]\n",
    "\n",
    "# The samples which will form a uniform distribution\n",
    "uniform_samples = gaussian_data[original_indices_for_uniform]\n",
    "\n",
    "print(\"Uniform Samples:\", uniform_samples)\n",
    "print(\"Original Indices:\", original_indices_for_uniform)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "-   **Sorting & Indexing:** We first sort the Gaussian data and keep\n",
    "    track of the indices. This step allows us to select a specific\n",
    "    number of evenly spaced samples from the sorted Gaussian data.\n",
    "-   **Quantiles:** By selecting indices at regular intervals (using\n",
    "    `np.linspace`), you mimic selecting uniform quantiles from the\n",
    "    original sorted data.\n",
    "-   **Original Indices:** The indices provide a way to reference back to\n",
    "    which original samples net the uniform distribution effect when\n",
    "    sorted.\n",
    "\n",
    "This method allows you to retain the original indices that relate to the\n",
    "samples you keep, helping in tracing back to the original dataset.\n",
    "\n",
    "### This does not work\n",
    "\n",
    "I apologize for any confusion. Let's go through a corrected approach to\n",
    "find indices from the original Gaussian-distributed data that would\n",
    "produce uniformly distributed angles:\n",
    "\n",
    "1.  **Generate Gaussian Data:** Start with your Gaussian distribution.\n",
    "2.  **Sort and Compute CDFs:** Sort the data and calculate the\n",
    "    cumulative distribution function (CDF).\n",
    "3.  **Select Uniformly using Quantiles:** Obtain indices corresponding\n",
    "    to uniform quantile intervals.\n",
    "\n",
    "Here's a more accurate implementation to achieve this:\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Parameters for the Gaussian distribution\n",
    "mean_angle = 180\n",
    "stddev_angle = 30\n",
    "\n",
    "# Generate Gaussian-distributed data\n",
    "num_samples = 1000\n",
    "gaussian_data = np.random.normal(loc=mean_angle, scale=stddev_angle, size=num_samples)\n",
    "\n",
    "# Track indices of the original data points\n",
    "indices = np.arange(num_samples)\n",
    "\n",
    "# Sort the data and indices\n",
    "sorted_indices = np.argsort(gaussian_data)\n",
    "sorted_data = gaussian_data[sorted_indices]\n",
    "\n",
    "# Calculate the CDF of the sorted data\n",
    "cdf_values = norm.cdf(sorted_data, loc=mean_angle, scale=stddev_angle)\n",
    "\n",
    "# Desired number of uniform samples\n",
    "num_uniform_samples = 100\n",
    "\n",
    "# Compute uniform quantiles\n",
    "uniform_quantiles = np.linspace(0, 1, num_uniform_samples, endpoint=False)\n",
    "\n",
    "# Find indices closest to the uniform quantiles\n",
    "uniform_indices = np.searchsorted(cdf_values, uniform_quantiles)\n",
    "\n",
    "# Get the original indices for these quantile positions\n",
    "original_indices_for_uniform = sorted_indices[uniform_indices]\n",
    "\n",
    "# The samples from the Gaussian distribution that map to a uniform distribution via CDF\n",
    "uniform_samples = gaussian_data[original_indices_for_uniform]\n",
    "\n",
    "print(\"Uniform Samples:\", uniform_samples)\n",
    "print(\"Original Indices:\", original_indices_for_uniform)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "-   **CDF Calculation:** By using the CDF, we map our Gaussian data to a\n",
    "    uniform distribution.\n",
    "-   **Uniform Quantiles:** Use these to find the positions in the sorted\n",
    "    CDF that correspond to uniform probabilities.\n",
    "-   **Searchsorted:** Finds where these quantiles would fit in the\n",
    "    sorted CDF, giving indices that you can trace back to the original\n",
    "    data.\n",
    "\n",
    "This approach should help you take indices from the original data such\n",
    "that, when picked, they produce angles distributed uniformly.\n",
    "\n",
    "**\\***\n",
    "\n",
    "To extract a uniform distribution from your angle distribution, you can\n",
    "try the following approach:\n",
    "\n",
    "1.  **Identify the Reference Angle**: Determine the reference angle\n",
    "    where your original distribution is peaked.\n",
    "\n",
    "2.  **Modify Probability Distribution**: Instead of drawing directly\n",
    "    from your original angle distribution, apply a transformation to\n",
    "    alter the probability of selecting each angle. One way is to define\n",
    "    a probability density function (PDF) that favors angles around the\n",
    "    reference + 180°.\n",
    "\n",
    "3.  **Gaussian Transformation**: Create a Gaussian distribution centered\n",
    "    at the reference + 180°. This will serve as your weighting function.\n",
    "\n",
    "    $$\n",
    "      f(\\theta) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\theta - (\\text{reference} + 180^\\circ))^2}{2\\sigma^2}\\right)\n",
    "      $$\n",
    "\n",
    "    Here, $\\sigma$ is the standard deviation that controls the spread of\n",
    "    your weighting function.\n",
    "\n",
    "4.  **Resampling**: Use the weighting function $f(\\theta)$ to adjust the\n",
    "    likelihood of selecting each angle from your distribution. This\n",
    "    means you'll have higher probabilities of selecting angles around\n",
    "    reference + 180°, effectively reducing the contribution from the\n",
    "    peak at the reference angle.\n",
    "\n",
    "5.  **Normalization**: Since you're changing the distribution by\n",
    "    considering a new PDF, ensure the resulting set maintains proper\n",
    "    normalization. You might need to renormalize the resulting weights\n",
    "    to sum up to one when generating the final set of angles.\n",
    "\n",
    "6.  **Inverse Transformation Sampling**: If necessary, use inverse\n",
    "    transform sampling or rejection sampling with these new weights to\n",
    "    generate angles that follow the modified distribution.\n",
    "\n",
    "By applying this strategy, angles around the initial peak become less\n",
    "likely, and those around the reference + 180° become more prominent,\n",
    "helping achieve a more uniform-like distribution across the 0 to 360°\n",
    "range.\n",
    "\n",
    "### How do I implement this in python?\n",
    "\n",
    "To implement the extraction of a uniform-like distribution from your\n",
    "angular distribution in Python, you can follow these steps. I'll provide\n",
    "a basic outline using NumPy, assuming you've established the initial\n",
    "angle distribution and identified the reference angle.\n",
    "\n",
    "### Step-by-step Python Implementation:\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sample distribution of angles with Gaussian noise centered around a reference angle\n",
    "n_samples = 10000  # Number of samples in the distribution\n",
    "reference_angle = 30  # Example reference angle in degrees\n",
    "sigma_original = 10  # Standard deviation of the original angle distribution\n",
    "\n",
    "# Create the original angle distribution centered at the reference angle\n",
    "original_angles = np.random.normal(reference_angle, sigma_original, n_samples) % 360\n",
    "\n",
    "# Set up parameters for the Gaussian weighting function centered at reference + 180 degrees\n",
    "offset_angle = (reference_angle + 180) % 360\n",
    "sigma_weight = 30  # Standard deviation for the weighting function\n",
    "\n",
    "def gaussian_weight(theta, center, sigma):\n",
    "    return np.exp(-0.5 * ((theta - center) % 360)**2 / sigma**2)\n",
    "\n",
    "# Calculate weights for the inverse probability distribution\n",
    "weights = gaussian_weight(original_angles, offset_angle, sigma_weight)\n",
    "\n",
    "# Resample according to the inverse likelihood (e.g., higher weight means less probability to select)\n",
    "prob_weights = 1 - (weights / weights.max())\n",
    "prob_weights /= prob_weights.sum()  # Normalize probabilities\n",
    "\n",
    "# Draw samples based on the calculated probabilities\n",
    "uniform_like_samples = np.random.choice(original_angles, size=n_samples, p=prob_weights)\n",
    "\n",
    "# Plot results to visualize\n",
    "plt.hist(uniform_like_samples, bins=360, range=(0, 360), alpha=0.5, label=\"Transformed Distribution\")\n",
    "plt.hist(original_angles, bins=360, range=(0, 360), alpha=0.5, label=\"Original Distribution\")\n",
    "plt.axvline(reference_angle, color='r', linestyle='dashed', linewidth=1, label='Reference Angle')\n",
    "plt.axvline(offset_angle, color='g', linestyle='dashed', linewidth=1, label='Offset Angle')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Angle (Degrees)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "-   **Original Sample Generation**: Uses a Gaussian distribution to\n",
    "    generate initial angle samples centered at a reference angle.\n",
    "-   **Weighting Function**: Defines a Gaussian weighting function\n",
    "    centered at the reference angle + 180°.\n",
    "-   **Probabilistic Resampling**: Resamples angles using a probability\n",
    "    inversely proportional to the weighting function values, encouraging\n",
    "    uniformity.\n",
    "-   **Result Visualization**: Plots both the original and the\n",
    "    transformed distributions for comparison.\n",
    "\n",
    "This example assumes you have a clear peak in the initial distribution\n",
    "and want to work with the angular space defined in degrees. Adjust\n",
    "`n_samples`, `sigma_original`, and `sigma_weight` as needed to better\n",
    "match the specifics of your dataset or desired results.\n",
    "\n",
    "### There must be an error the resulting distribution is far from uniform and still peaked at the reference\n",
    "\n",
    "It seems like the approach used is not having the intended effect. Let's\n",
    "refine the method to better flatten the distribution. A different\n",
    "strategy involves using the Gaussian weighting function to selectively\n",
    "reduce the sampling probability from the original peak, encouraging\n",
    "selection of angles further away from it. Here is a refined approach:\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "n_samples = 10000  # Number of samples to generate\n",
    "reference_angle = 30  # Example reference angle in degrees\n",
    "sigma_original = 10  # Std deviation of original Gaussian peak\n",
    "sigma_weight = 30  # Std deviation for the weighting function\n",
    "\n",
    "# Generate a sample distribution of angles centered around the reference angle\n",
    "original_angles = np.random.normal(reference_angle, sigma_original, n_samples) % 360\n",
    "\n",
    "# Calculate weights for each angle in the transformed distribution\n",
    "def gaussian_weight(theta, center, sigma):\n",
    "    return np.exp(-0.5 * ((theta - center + 180) % 360 - 180)**2 / sigma**2)\n",
    "\n",
    "weights = gaussian_weight(original_angles, reference_angle, sigma_weight)\n",
    "\n",
    "# Invert the weights to reduce probability near the original peak\n",
    "inverted_weights = 1.0 / (weights + np.finfo(float).eps)  # Avoid division by zero\n",
    "\n",
    "# Normalize the inverted weights to form a probability distribution\n",
    "prob_weights = inverted_weights / inverted_weights.sum()\n",
    "\n",
    "# Resample according to the modified weights\n",
    "uniform_like_samples = np.random.choice(original_angles, size=n_samples, p=prob_weights)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(original_angles, bins=360, range=(0, 360), alpha=0.5, label=\"Original Distribution\")\n",
    "plt.hist(uniform_like_samples, bins=360, range=(0, 360), alpha=0.5, label=\"Transformed Distribution\")\n",
    "plt.axvline(reference_angle, color='r', linestyle='dashed', linewidth=1, label='Reference Angle')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Angle (Degrees)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Original and Transformed Angle Distributions\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Key Changes and Explanation:\n",
    "\n",
    "-   **Weight Inversion**: We're now inverting the weights directly\n",
    "    derived from the Gaussian around the reference angle to reduce their\n",
    "    influence in that region.\n",
    "-   **Normalization**: The inverted weights are normalized to form a\n",
    "    valid probability distribution for resampling.\n",
    "-   **Sampling Process**: The resampling step now uses the modified\n",
    "    weights to select angles with reduced probabilities near the\n",
    "    original peak.\n",
    "\n",
    "This should help in flattening the original peak and result in a more\n",
    "uniform-like distribution, with higher probabilities assigned to angles\n",
    "previously less represented. Adjust `sigma_weight` to control the spread\n",
    "of the new weighting function and achieve the desired level of\n",
    "uniformity.\n",
    "\n",
    "### This is almost working but now it samples to many angles at reference + 180. I am sure that there should be a way to make it uniform\n",
    "\n",
    "To achieve a more uniform distribution, we might need a different\n",
    "approach that avoids biasing the sampling to specific regions, such as\n",
    "the reference + 180°. A potential method involves:\n",
    "\n",
    "1.  Using a cumulative distribution function (CDF) to approximate the\n",
    "    original distribution.\n",
    "2.  Applying transformations to produce a uniform distribution from the\n",
    "    CDF.\n",
    "\n",
    "Here's a step-by-step guide to implement this:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# * Parameters\n",
    "n_samples = 10000\n",
    "reference_angle = 30\n",
    "sigma_original = 75\n",
    "\n",
    "# * Generate the original angle distribution centered at the reference angle\n",
    "original_angles = np.random.normal(reference_angle, sigma_original, n_samples) % 360\n",
    "\n",
    "# * Kernel Density Estimate to approximate the PDF of the original distribution\n",
    "kde = gaussian_kde(original_angles, bw_method='silverman')\n",
    "x_values = np.linspace(0, 360, 1000)\n",
    "pdf_values = kde(x_values)\n",
    "\n",
    "# * Normalize PDF to create a valid probability distribution function\n",
    "pdf_values /= np.sum(pdf_values)\n",
    "\n",
    "# * Compute cumulative distribution function (CDF)\n",
    "cdf_values = np.cumsum(pdf_values)\n",
    "cdf_values /= cdf_values[-1]  # Normalize to ensure it ends at 1\n",
    "\n",
    "# * Use inverse transform sampling to draw samples from a uniform distribution over the CDF\n",
    "random_values = np.random.rand(n_samples)\n",
    "uniform_like_samples = np.interp(random_values, cdf_values, x_values)\n",
    "\n",
    "#* Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(original_angles, bins=360, range=(0, 360), alpha=0.5, label=\"Original Distribution\")\n",
    "plt.hist(uniform_like_samples, bins=360, range=(0, 360), alpha=0.5, label=\"Transformed Distribution\")\n",
    "plt.axvline(reference_angle, color='r', linestyle='dashed', linewidth=1, label='Reference Angle')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Angle (Degrees)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Original and Transformed Angle Distributions\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Key Steps and Explanation:**\n",
    "\n",
    "-   **Kernel Density Estimation (KDE):** Approximates the PDF of the\n",
    "    original angle distribution.\n",
    "-   **CDF Computation:** The CDF is derived from the approximated PDF to\n",
    "    map distribution to a uniform space.\n",
    "-   **Inverse Transform Sampling:** Uses the CDF to draw samples\n",
    "    uniformly across the angle space.\n",
    "\n",
    "This process should help in creating a more balanced angle distribution\n",
    "by transforming the original distribution into uniform-like samples.\n",
    "Adjust the bandwidth in the KDE if necessary for better PDF\n",
    "approximation.\n",
    "\n",
    "### This does not work\n",
    "\n",
    "I apologize for the confusion. Let's revisit the approach to ensure a\n",
    "truly uniform distribution across angles from your original\n",
    "distribution. Here's a refined method that leverages histogram\n",
    "equalization, a classical method to achieve uniform distributions from\n",
    "an existing data distribution:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Parameters\n",
    "n_samples = 10000  # Number of samples to generate\n",
    "reference_angle = 30  # Example reference angle in degrees\n",
    "sigma_original = 75  # Standard deviation of the original distribution\n",
    "\n",
    "# Generate a sample distribution centered around the reference angle\n",
    "original_angles = np.random.normal(reference_angle, sigma_original, n_samples) % 360\n",
    "\n",
    "# Compute histogram of the original angles\n",
    "hist_values, bin_edges = np.histogram(original_angles, bins=360, range=(0, 360), density=True)\n",
    "\n",
    "# Compute the cumulative distribution function (CDF)\n",
    "cdf_values = np.cumsum(hist_values)\n",
    "cdf_values /= cdf_values[-1]  # Normalize to make it a valid CDF\n",
    "\n",
    "# Interpolate the inverse CDF\n",
    "inverse_cdf = interp1d(cdf_values, bin_edges[:-1])\n",
    "\n",
    "# Generate uniform random samples\n",
    "random_values = np.random.rand(n_samples)\n",
    "\n",
    "# Map uniform samples through the inverse CDF to transform them into angle samples\n",
    "uniform_like_samples = inverse_cdf(random_values)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(original_angles, bins=360, range=(0, 360), alpha=0.5, label=\"Original Distribution\")\n",
    "plt.hist(uniform_like_samples, bins=360, range=(0, 360), alpha=0.5, label=\"Transformed Distribution\")\n",
    "plt.axvline(reference_angle, color='r', linestyle='dashed', linewidth=1, label='Reference Angle')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Angle (Degrees)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Original and Transformed Angle Distributions\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "-   **Histogram and CDF Calculation**: This approach uses the histogram\n",
    "    of your original distribution to create a cumulative distribution\n",
    "    function (CDF).\n",
    "\n",
    "-   **Inverse CDF Interpolation**: By interpolating the inverse CDF, we\n",
    "    can map uniform random samples into the angle space of the original\n",
    "    distribution.\n",
    "\n",
    "-   **Uniform Sampling**: The uniform random values are mapped through\n",
    "    the inverse CDF to result in transformed samples that should ideally\n",
    "    spread more uniformly across the entire 0 to 360-degree range.\n",
    "\n",
    "This approach aims at distributing samples more uniformly by equalizing\n",
    "them across the cumulative distribution of the input data. Adjust\n",
    "parameters such as the histogram bins for finer granularity to achieve\n",
    "the desired uniformity.\n",
    "\n",
    "### This raises an error\n",
    "\n",
    "``` example\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[125], line 27\n",
    "     24 random_values = np.random.rand(n_samples)\n",
    "     26 # Map uniform samples through the inverse CDF to transform them into angle samples\n",
    "---> 27 uniform_like_samples = inverse_cdf(random_values)\n",
    "     29 # Plot results\n",
    "     30 plt.figure(figsize=(10, 5))\n",
    "\n",
    "File ~/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/interpolate/_polyint.py:80, in _Interpolator1D.__call__(self, x)\n",
    "     59 \"\"\"\n",
    "     60 Evaluate the interpolant\n",
    "     61\n",
    "   (...)\n",
    "     77\n",
    "     78 \"\"\"\n",
    "     79 x, x_shape = self._prepare_x(x)\n",
    "---> 80 y = self._evaluate(x)\n",
    "     81 return self._finish_y(y, x_shape)\n",
    "\n",
    "File ~/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/interpolate/_interpolate.py:755, in interp1d._evaluate(self, x_new)\n",
    "    753 y_new = self._call(self, x_new)\n",
    "    754 if not self._extrapolate:\n",
    "--> 755     below_bounds, above_bounds = self._check_bounds(x_new)\n",
    "    756     if len(y_new) > 0:\n",
    "    757         # Note fill_value must be broadcast up to the proper size\n",
    "    758         # and flattened to work here\n",
    "    759         y_new[below_bounds] = self._fill_value_below\n",
    "\n",
    "File ~/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/interpolate/_interpolate.py:784, in interp1d._check_bounds(self, x_new)\n",
    "    782 if self.bounds_error and below_bounds.any():\n",
    "    783     below_bounds_value = x_new[np.argmax(below_bounds)]\n",
    "--> 784     raise ValueError(\"A value ({}) in x_new is below \"\n",
    "    785                      \"the interpolation range's minimum value ({}).\"\n",
    "    786                      .format(below_bounds_value, self.x[0]))\n",
    "    787 if self.bounds_error and above_bounds.any():\n",
    "    788     above_bounds_value = x_new[np.argmax(above_bounds)]\n",
    "\n",
    "ValueError: A value (0.0015298750278947004) in x_new is below the interpolation range's minimum value (0.004300000000000001).\n",
    "```\n",
    "\n",
    "The error occurs when attempting to interpolate values of\n",
    "`random_values` that exceed the bounds of `cdf_values`. This happens\n",
    "because `random_values` can generate values as close as 0 or 1, but\n",
    "interpolation can't handle values outside the CDF's defined range. To\n",
    "fix this, ensure `inverse_cdf` can map from 0 to 1 properly by modifying\n",
    "the interpolation setup.\n",
    "\n",
    "Here’s a revised version of the code to address this:\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Parameters\n",
    "n_samples = 10000  # Number of samples to generate\n",
    "reference_angle = 30  # Example reference angle in degrees\n",
    "sigma_original = 75  # Standard deviation of the original distribution\n",
    "\n",
    "# Generate a sample distribution centered around the reference angle\n",
    "original_angles = np.random.normal(reference_angle, sigma_original, n_samples) % 360\n",
    "\n",
    "# Compute histogram of the original angles\n",
    "hist_values, bin_edges = np.histogram(original_angles, bins=360, range=(0, 360), density=True)\n",
    "\n",
    "# Compute the cumulative distribution function (CDF)\n",
    "cdf_values = np.cumsum(hist_values)\n",
    "cdf_values /= cdf_values[-1]  # Normalize to make it a valid CDF\n",
    "\n",
    "# Adjust bin edges and CDF values for interpolation\n",
    "extended_bin_edges = np.concatenate(([0], bin_edges[1:], [360]))\n",
    "extended_cdf_values = np.concatenate(([0], cdf_values, [1]))\n",
    "\n",
    "# Interpolate the inverse CDF over its full range\n",
    "inverse_cdf = interp1d(extended_cdf_values, extended_bin_edges, bounds_error=False, fill_value=(0, 360))\n",
    "\n",
    "# Generate uniform random samples\n",
    "random_values = np.random.rand(n_samples)\n",
    "\n",
    "# Map uniform samples through the inverse CDF to transform them into angle samples\n",
    "uniform_like_samples = inverse_cdf(random_values)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(original_angles, bins=360, range=(0, 360), alpha=0.5, label=\"Original Distribution\")\n",
    "plt.hist(uniform_like_samples, bins=360, range=(0, 360), alpha=0.5, label=\"Transformed Distribution\")\n",
    "plt.axvline(reference_angle, color='r', linestyle='dashed', linewidth=1, label='Reference Angle')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Angle (Degrees)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Original and Transformed Angle Distributions\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Key Changes:\n",
    "\n",
    "-   **Extended CDF and Bin Edges:** Adding `[0]` to the start and\n",
    "    `[360]` to the end of the bin edges, accompanied by `0` and `1` for\n",
    "    the CDF, ensures that the interpolation is defined across the entire\n",
    "    `[0, 1]` interval.\n",
    "-   **Fill Value for Interpolation:** Set the `fill_value` parameter in\n",
    "    `interp1d` to extrapolate values appropriately to ensure no\n",
    "    out-of-bound errors occur.\n",
    "\n",
    "This revised code should generate a distribution uniformly sampled\n",
    "across 0 to 360 degrees without encountering interpolation boundary\n",
    "issues.\n",
    "\n",
    "**\\***\n",
    "\n",
    "To balance the distribution and prevent over-sampling around reference +\n",
    "180°, we can refine the method by combining the current weight inversion\n",
    "technique with a broader and more moderate adjustment of probabilities.\n",
    "This can be done by weighting less heavily against the region around the\n",
    "reference and more uniformly reducing the differences across the range.\n",
    "\n",
    "Here's an updated version:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "n_samples = 10000  # Number of samples to generate\n",
    "reference_angle = 30  # Example reference angle in degrees\n",
    "sigma_original = 10  # Std deviation of original Gaussian peak\n",
    "sigma_adjust = 180  # Increased standard deviation for adjustment\n",
    "\n",
    "# Generate a sample distribution of angles centered around the reference angle\n",
    "original_angles = np.random.normal(reference_angle, sigma_original, n_samples) % 360\n",
    "\n",
    "# Weight adjustment function\n",
    "def uniform_adjustment(theta, center, sigma):\n",
    "    # Create a broader distribution that moderately favors uniformity\n",
    "    adjustment = np.exp(-0.5 * ((theta - center + 180) % 360 - 180)**2 / sigma**2)\n",
    "    return 1.0 / (adjustment + np.finfo(float).eps)\n",
    "\n",
    "# Calculate moderate adjustment weights\n",
    "weights = uniform_adjustment(original_angles, reference_angle, sigma_adjust)\n",
    "prob_weights = weights / weights.sum()  # Normalize\n",
    "\n",
    "# Resample according to the adjusted weights\n",
    "uniform_like_samples = np.random.choice(original_angles, size=n_samples, p=prob_weights)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(original_angles, bins=360, range=(0, 360), alpha=0.5, label=\"Original Distribution\")\n",
    "plt.hist(uniform_like_samples, bins=360, range=(0, 360), alpha=0.5, label=\"Transformed Distribution\")\n",
    "plt.axvline(reference_angle, color='r', linestyle='dashed', linewidth=1, label='Reference Angle')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Angle (Degrees)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Original and Transformed Angle Distributions\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Key Adjustments:\n",
    "\n",
    "-   **Broader Sigma (`sigma_adjust`)**: Instead of strictly focusing on\n",
    "    reducing probabilities around the initial peak and reference + 180°,\n",
    "    apply a broader smoothing to moderately make the distribution more\n",
    "    uniform without overly biasing.\n",
    "-   **Adjust Weights Gradually**: By setting `sigma_adjust` to a larger\n",
    "    value (e.g., 180), we're applying a gentler adjustment, allowing\n",
    "    more gradual changes across angles rather than sharply favoring any\n",
    "    specific range. This should smooth the transition and help stabilize\n",
    "    the tendency around reference + 180°.\n",
    "-   **Normalized Probability Weights**: Ensure the weights are properly\n",
    "    normalized to provide a valid probability distribution for sampling\n",
    "    the angles.\n",
    "\n",
    "These changes should help prevent excessive concentration of samples\n",
    "around reference + 180°. Adjusting the `sigma_adjust` parameter can\n",
    "further fine-tune the spread and smoothness of the distribution.\n",
    "\n",
    "**\\***\n",
    "\n",
    "To extract a uniform distribution from a peaked distribution of angles,\n",
    "you can follow a few potential methods. Here are a couple of approaches:\n",
    "\n",
    "1.  **Random Sampling**: If you only need a uniform sample as opposed to\n",
    "    reshaping the entire distribution into a uniform one, you can\n",
    "    randomly sample from the angle space between 0 and 360 degrees,\n",
    "    ignoring the shape of the original distribution.\n",
    "\n",
    "2.  **Inverse Transform Sampling**:\n",
    "\n",
    "    -   Calculate the CDF (Cumulative Distribution Function) of the\n",
    "        original distribution.\n",
    "    -   Generate uniform random numbers between 0 and 1.\n",
    "    -   Use the inverse of the CDF to map these uniform numbers to your\n",
    "        original distribution's range of angles. This method transforms\n",
    "        the non-uniform distribution into a target distribution (in this\n",
    "        case, you want the output to follow a uniform distribution\n",
    "        perhaps for simplicity or computational reasons).\n",
    "\n",
    "3.  **Histogram Equalization** (if you need a processed dataset):\n",
    "\n",
    "    -   Treat the problem as a histogram equalization task. Convert your\n",
    "        angles into a form where their cumulative histogram becomes\n",
    "        approximately linear, which corresponds to a uniform\n",
    "        distribution.\n",
    "\n",
    "4.  **Data Rejection**:\n",
    "\n",
    "    -   Use rejection sampling: generate a candidate angle uniformly\n",
    "        over 0-360, and accept it with probability inversely\n",
    "        proportional to the value of the original distribution’s\n",
    "        probability density (e.g., if the density is high, then the\n",
    "        acceptance probability is lower, balancing it to form a flat\n",
    "        distribution).\n",
    "\n",
    "Choose the method that aligns best with your specific needs (whether\n",
    "it's maintaining data fidelity or simply generating\n",
    "uniformly-distributed samples).\n",
    "\n",
    "### How do I implement 2 in pyhton\n",
    "\n",
    "To implement inverse transform sampling in Python to generate a uniform\n",
    "distribution from a given non-uniform (peaked) distribution over angles,\n",
    "you can follow these steps:\n",
    "\n",
    "1.  Generate points based on the CDF of the original distribution.\n",
    "2.  Use the inverse of the CDF to map uniform samples to your\n",
    "    distribution.\n",
    "\n",
    "Here's a basic implementation using numpy and scipy:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Let's assume `angles` is your array of angles (in degrees) from the peaked distribution.\n",
    "# Replace it with your actual data.\n",
    "angles = np.random.normal(loc=180, scale=10, size=1000) % 360  # example data\n",
    "\n",
    "# Step 1: Calculate the histogram and CDF of the original distribution\n",
    "hist, bin_edges = np.histogram(angles, bins=360, range=(0, 360), density=True)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "# Step 2: Generate uniform random numbers between 0 and 1\n",
    "uniform_randoms = np.random.rand(len(angles))\n",
    "\n",
    "# Step 3: Invert the CDF to map uniform randoms to angles\n",
    "# Create an inverse CDF function using interpolation\n",
    "inverse_cdf = interp1d(cdf, bin_edges[1:], kind='linear', fill_value='extrapolate')\n",
    "\n",
    "# Map uniform randoms to the angle domain using the inverse CDF\n",
    "uniform_angles = inverse_cdf(uniform_randoms)\n",
    "\n",
    "# `uniform_angles` now follows a uniform distribution between 0 and 360\n",
    "print(uniform_angles)\n",
    "```\n",
    "\n",
    "This script will give you a set of angles where the original peaked\n",
    "distribution is transformed into a uniform distribution using inverse\n",
    "transform sampling. Adjust the binning appropriately based on your\n",
    "dataset size and characteristics for better accuracy.\n",
    "\n",
    "### What I need is the indices of the angles that were kept from the original vector of angles\n",
    "\n",
    "To extract indices of the original angles such that their distribution\n",
    "becomes uniform, you can employ a technique similar to inverse transform\n",
    "sampling to repeatedly select indices. Here's an approach to achieve\n",
    "that:\n",
    "\n",
    "1.  **Calculate the Histogram and CDF**: Compute the CDF of your\n",
    "    original angle data.\n",
    "2.  **Inverse Sampling on the CDF**: Use the CDF to select indices such\n",
    "    that the resulting distribution is uniform.\n",
    "\n",
    "Here's how you can implement this in Python:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Let's assume `angles` is your array of angles (in degrees) from the peaked distribution.\n",
    "# Replace it with your actual data.\n",
    "angles = np.random.normal(loc=180, scale=10, size=1000) % 360  # Example data\n",
    "\n",
    "# Sort angles and get original indices\n",
    "sorted_indices = np.argsort(angles)\n",
    "sorted_angles = angles[sorted_indices]\n",
    "\n",
    "# Step 1: Calculate the histogram and CDF of the sorted distribution\n",
    "hist, bin_edges = np.histogram(sorted_angles, bins=360, range=(0, 360), density=True)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "# Step 2: Generate uniform random numbers between 0 and 1\n",
    "uniform_randoms = np.random.rand(len(angles))\n",
    "\n",
    "# Step 3: Use the CDF to select indices\n",
    "# Create an inverse CDF (for index selection) function using interpolation\n",
    "inverse_cdf = interp1d(cdf, bin_edges[1:], kind='linear', bounds_error=False, fill_value=(0, 360))\n",
    "\n",
    "# Map uniform randoms to indices\n",
    "selected_bins = np.digitize(inverse_cdf(uniform_randoms), bin_edges) - 1\n",
    "selected_bins = np.clip(selected_bins, 0, len(hist) - 1)  # Ensure valid bin indices\n",
    "selected_indices = np.unique(sorted_indices[selected_bins])  # Unique indices\n",
    "\n",
    "# `selected_indices` now contains indices that would help create a uniform distribution\n",
    "print(selected_indices)\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "-   We first sort the angles and keep track of their original indices.\n",
    "-   The CDF is computed on the sorted data.\n",
    "-   We perform inverse sampling by generating uniform random values and\n",
    "    interpolating via the inverse CDF.\n",
    "-   The result is the set of indices from the sorted angles that can be\n",
    "    used to yield a uniformly distributed set of angles.\n",
    "\n",
    "Note: This technique attempts to achieve uniformity; however, due to\n",
    "discrete nature and histogram binning, results might not be perfectly\n",
    "uniform unless adjusted for bin specifics and domain constraints.\n",
    "\n",
    "### This technique does not seem to be working. the distribution of selected angles is still peaked\n",
    "\n",
    "Let's refine the approach to better balance the distribution. The goal\n",
    "is to select data points such that the resulting set of angles is\n",
    "uniformly distributed, which can be challenging due to sampling\n",
    "variability and the discrete nature of data. Here's one strategy to try:\n",
    "\n",
    "1.  **Equal-width Bin Selection**: Divide the angle range into\n",
    "    equal-width bins and randomly pick angles from each bin.\n",
    "\n",
    "2.  **Random Sampling from Bins**:\n",
    "\n",
    "    -   Bin the angles.\n",
    "    -   Randomly sample equal numbers of angles from each bin, if\n",
    "        possible.\n",
    "\n",
    "Here's a Python implementation of this strategy:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "\n",
    "# Let's assume `angles` is your array of angles (in degrees) from the peaked distribution.\n",
    "angles = np.random.normal(loc=180, scale=10, size=1000) % 360  # Example data\n",
    "\n",
    "# Number of bins (you can adjust this based on your data size and spread)\n",
    "num_bins = 20\n",
    "\n",
    "# Step 1: Bin the data\n",
    "bin_edges = np.linspace(0, 360, num_bins + 1)\n",
    "binned_indices = np.digitize(angles, bin_edges) - 1\n",
    "\n",
    "# Step 2: Select equal number of samples (or proportionate to bin population) from each bin\n",
    "selected_indices = []\n",
    "\n",
    "for i in range(num_bins):\n",
    "    # Get indices of angles in the current bin\n",
    "    indices_in_bin = np.where(binned_indices == i)[0]\n",
    "\n",
    "    # Sample a fixed number or all if fewer samples are available\n",
    "    if len(indices_in_bin) > 0:\n",
    "        num_to_sample = min(5, len(indices_in_bin))  # Adjust `5` as needed\n",
    "        sampled_indices = np.random.choice(indices_in_bin, num_to_sample, replace=False)\n",
    "        selected_indices.extend(sampled_indices)\n",
    "\n",
    "selected_indices = np.array(selected_indices)\n",
    "\n",
    "# `selected_indices` contains indices of the original angles in the desired uniform manner\n",
    "print(selected_indices)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "-   **Binning**: Divide the data into a specified number of bins. This\n",
    "    defines \"horizontal\" slices of the data, giving a structure to\n",
    "    select from.\n",
    "\n",
    "-   **Random Sampling from Bins**: From each bin, select a fixed number\n",
    "    of samples (e.g., 5 per bin, adjustable). This ensures that each\n",
    "    section of your distribution has a roughly equal representation in\n",
    "    the sampled set.\n",
    "\n",
    "-   **Final Indices**: Compile selected indices. The resulting set of\n",
    "    angles should approximate a more uniform distribution when plotted.\n",
    "\n",
    "Note that the number of bins and the number of samples per bin can be\n",
    "adjusted according to your data size and distribution characteristics to\n",
    "achieve a better uniformity balance.\n",
    "\n",
    "**\\***\n",
    "\n",
    "### Does bins needs to be 360 in hist, bin<sub>edges</sub> = np.histogram(sorted<sub>angles</sub>, bins=360, range=(0, 360), density=True)\n",
    "\n",
    "The choice of 360 bins in the histogram calculation is related to how\n",
    "finely you want to discretize your angle space. In theory, using 360\n",
    "bins corresponds to separating each degree individually, which might be\n",
    "appropriate given the range of angles from 0 to 360.\n",
    "\n",
    "However, whether you need 360 bins depends on:\n",
    "\n",
    "1.  **Data Density**: If you have a large number of data points (e.g.,\n",
    "    thousands of angles) and they cover the range densely, 360 bins\n",
    "    could be appropriate. For sparse data, fewer bins might help prevent\n",
    "    empty or near-empty bins.\n",
    "\n",
    "2.  **Desired Resolution**: If you need a finer granularity in\n",
    "    representing the angle distribution, you might opt for more bins.\n",
    "    Conversely, fewer bins (e.g., 36 or 72) could be used for broader\n",
    "    grouping of angles.\n",
    "\n",
    "3.  **Computational Considerations**: More bins increase computational\n",
    "    complexity and may require more adjustments in mapping.\n",
    "\n",
    "You might start with a smaller number of bins and adjust based on the\n",
    "resolution your analysis requires. If uniformity isn't achieved to\n",
    "satisfaction, tweaking the number of bins and analyzing the results\n",
    "could help optimize the outcomes.\n",
    "\n",
    "**\\***\n",
    "\n",
    "### Is sorting necessary?\n",
    "\n",
    "No, sorting isn't strictly necessary if you only want to extract indices\n",
    "creating a uniform distribution. However, sorting is a common step for\n",
    "clarity and ease when calculating cumulative distributions in a\n",
    "straightforward manner since it helps easily map sample values back to\n",
    "their original indices.\n",
    "\n",
    "If you want to avoid sorting and directly work with indices, here's an\n",
    "adjusted approach:\n",
    "\n",
    "1.  Compute the histogram and CDF directly from unsorted data.\n",
    "2.  Using uniform random values, directly sample indices from the CDF to\n",
    "    achieve a uniform selection.\n",
    "\n",
    "Here's how you could implement it:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Example data: angles in degrees from a peaked distribution\n",
    "angles = np.random.normal(loc=180, scale=10, size=1000) % 360  # Replace with your actual data\n",
    "\n",
    "# Step 1: Calculate the histogram and CDF of the original distribution\n",
    "hist, bin_edges = np.histogram(angles, bins=360, range=(0, 360), density=False)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "# Step 2: Generate uniform random numbers between 0 and 1\n",
    "uniform_randoms = np.random.rand(len(angles))\n",
    "\n",
    "# Step 3: Map uniform random numbers to indices in the original array using the inverse CDF\n",
    "inverse_cdf = interp1d(cdf, np.arange(len(cdf)), kind='nearest', bounds_error=False, fill_value='extrapolate')\n",
    "selected_bins = inverse_cdf(uniform_randoms).astype(int)\n",
    "\n",
    "# Find the original indices corresponding to these bins\n",
    "selected_indices = []\n",
    "for b in selected_bins:\n",
    "    # Find the indices for angles in this bin\n",
    "    bin_indices = np.where((angles >= bin_edges[b]) & (angles < bin_edges[b+1]))[0]\n",
    "    if len(bin_indices) > 0:\n",
    "        selected_indices.append(np.random.choice(bin_indices))\n",
    "\n",
    "# Convert selected indices to a numpy array and remove duplicates\n",
    "selected_indices = np.unique(selected_indices)\n",
    "\n",
    "print(selected_indices)\n",
    "```\n",
    "\n",
    "Explanation of changes:\n",
    "\n",
    "-   The key difference here is that rather than sorting the angles and\n",
    "    then referencing indices, we calculate the CDF with the data in\n",
    "    place and sample from it to get indices.\n",
    "-   Instead of sorting, we rely on identifying the bin each uniform\n",
    "    random number corresponds to, and then we select indices directly\n",
    "    based on those bins.\n",
    "-   We then resolve specific indices within each bin using sampling,\n",
    "    ensuring unique selections.\n",
    "\n",
    "This approach should provide you with a set of indices that can sample a\n",
    "uniform-like distribution from your original data.\n",
    "\n",
    "**\\***\n",
    "\n",
    "### You are a computational neuroscientist. You have a dataset composed of neuronal activities. Neurons are selective to angular locations and you are trying to decode the neurons preferred location given the fact that a non uniform distribution of stimulus is used at each trial.\n",
    "\n",
    "You came up with this to account for the non uniform dist\n",
    "\n",
    "``` ipython\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "def generate_balanced_subsample(firing_rates, angles_rad, num_bins=96, sample_size=0.5):\n",
    "    # Discretize the angles into bins\n",
    "    bins = np.linspace(0, 2 * np.pi, num_bins + 1)\n",
    "    digitized = np.digitize(angles_rad, bins, right=True)\n",
    "\n",
    "    # List to store subsampled indices\n",
    "    subsample_indices = []\n",
    "\n",
    "    # Sample equally from each bin\n",
    "    for b in range(1, len(bins)):\n",
    "        bin_indices = np.where(digitized == b)[0]\n",
    "        # Sample a balanced number from each bin (floor of the sample_size proportion)\n",
    "        num_to_sample = int(sample_size * len(bin_indices))\n",
    "        if num_to_sample > 0 and len(bin_indices) > 0:\n",
    "            subsample_indices.extend(np.random.choice(bin_indices, size=num_to_sample, replace=False))\n",
    "\n",
    "    # Convert to numpy array\n",
    "    subsample_indices = np.asarray(subsample_indices)\n",
    "    # print(subsample_indices.shape)\n",
    "\n",
    "    # Create subsample\n",
    "    X_sub = firing_rates[subsample_indices]\n",
    "    y_sub = angles_rad[subsample_indices]\n",
    "\n",
    "    return X_sub, y_sub\n",
    "\n",
    "def average_neuron_selectivity_uniform(firing_rates, angles_rad, num_subsamples=1000, num_bins=96, sample_size=0.5):\n",
    "    coeffs_cos = []\n",
    "    coeffs_sin = []\n",
    "\n",
    "    for _ in range(num_subsamples):\n",
    "        # Generate a balanced subsample\n",
    "        X_sub, y_sub = generate_balanced_subsample(firing_rates, angles_rad, num_bins, sample_size)\n",
    "\n",
    "        # Prepare targets for cosine and sine components\n",
    "        Y_sub = np.column_stack((np.cos(y_sub), np.sin(y_sub)))\n",
    "        # Train Ridge regression\n",
    "        reg = RidgeCV()\n",
    "        reg.fit(X_sub, Y_sub)\n",
    "\n",
    "        # print(X_sub.shape, y_sub.shape, reg.coef_.shape)\n",
    "\n",
    "        # Store coefficients\n",
    "        coeffs_cos.append(reg.coef_[0])  # Cosine coefficients\n",
    "        coeffs_sin.append(reg.coef_[1])  # Sine coefficients\n",
    "\n",
    "    # Average coefficients\n",
    "    avg_coeffs_cos = np.mean(coeffs_cos, axis=0)\n",
    "    avg_coeffs_sin = np.mean(coeffs_sin, axis=0)\n",
    "\n",
    "    predicted_angles_rad = np.arctan2(avg_coeffs_sin, avg_coeffs_cos)\n",
    "    return np.degrees(predicted_angles_rad) % 360\n",
    "```\n",
    "\n",
    "But I believe that the subsampling approach is not solving the imbalance\n",
    "problem\n",
    "\n",
    "Your approach addresses the imbalance by generating balanced subsamples,\n",
    "but it's true that subsampling might not fully solve the problem or be\n",
    "the most efficient solution. Here are a few potential concerns and\n",
    "alternatives:\n",
    "\n",
    "1.  **Data Reduction**: By subsampling, you effectively discard a\n",
    "    significant portion of your data, which might lead to a loss of\n",
    "    valuable information, especially if some bins are already sparsely\n",
    "    populated.\n",
    "\n",
    "2.  **Limited Re-sampling**: If certain bins are very underrepresented,\n",
    "    random sampling might not capture the underlying distribution well\n",
    "    enough, even after many subsamples. It could lead to high variance\n",
    "    in the estimated coefficients.\n",
    "\n",
    "3.  **Alternatives to Subsampling**:\n",
    "\n",
    "    -   **Weighting**: Instead of balancing through subsampling, apply\n",
    "        weights to the samples during training, inversely proportional\n",
    "        to their frequency. This way, rare samples have a greater\n",
    "        influence without discarding data.\n",
    "    -   **Data Augmentation**: Create synthetic samples for\n",
    "        underrepresented bins through techniques like bootstrapping or\n",
    "        generating new samples using the statistics of existing data.\n",
    "    -   **Smoothed Regression**: Incorporate priors or regularization\n",
    "        terms that account for known biases or patterns in the data\n",
    "        distribution directly in the model.\n",
    "\n",
    "4.  **Cross-validation and Diagnostics**: Always validate the\n",
    "    performance improvements through cross-validated prediction metrics\n",
    "    and consider visual diagnostics to ensure the approach truly\n",
    "    addresses the imbalance.\n",
    "\n",
    "If weighting interests you, many machine learning libraries, including\n",
    "scikit-learn, provide support for weighting during model fitting, which\n",
    "might be beneficial in your case.\n",
    "\n",
    "**\\*** Your approach aims to estimate the preferred angular location of\n",
    "neurons by accounting for a non-uniform distribution of stimuli across\n",
    "trials. Here’s a brief explanation of your method:\n",
    "\n",
    "1.  **Balanced Subsampling:**\n",
    "    -   You discretize stimulus angles into equal-width bins.\n",
    "    -   For each bin, you sample a proportion (indicated by\n",
    "        `sample_size`) of instances to ensure balanced representation,\n",
    "        handling the non-uniform distribution.\n",
    "2.  **Cosine and Sine Regression:**\n",
    "    -   You prepare targets as cosine and sine components of the\n",
    "        subsampled angles.\n",
    "    -   Using Ridge regression, you fit these components to the\n",
    "        corresponding firing rates, obtaining coefficients indicative of\n",
    "        selectivity.\n",
    "3.  **Coefficient Averaging:**\n",
    "    -   You perform subsampling and regression multiple times\n",
    "        (`num_subsamples`), collecting coefficients for both components\n",
    "        across iterations.\n",
    "    -   The final preference angle for each neuron is computed by\n",
    "        averaging these coefficients, and you convert these averaged\n",
    "        values back to angles using `arctan2`.\n",
    "\n",
    "This technique helps manage variability due to stimulus imbalance by\n",
    "emphasizing equal representation across different stimulus geometries,\n",
    "hence targeting a more robust estimation of neuronal preferences. If you\n",
    "have any specific questions or concerns about this method, feel free to\n",
    "ask!\n",
    "\n",
    "### How could this be improved?\n",
    "\n",
    "Here are a few suggestions to potentially improve or extend your current\n",
    "approach:\n",
    "\n",
    "1.  **Cross-Validation:**\n",
    "    -   Implement cross-validation to better estimate the robustness of\n",
    "        your ridge regression model. This will help ensure the model\n",
    "        generalizes well and that predicted angles are not due to\n",
    "        overfitting.\n",
    "2.  **Hyperparameter Tuning:**\n",
    "    -   Expand beyond the default hyperparameter selection of RidgeCV by\n",
    "        evaluating different regularization strengths, or use\n",
    "        cross-validated methods like GridSearchCV for fine-tuning.\n",
    "3.  **Bootstrap Aggregation (Bagging):**\n",
    "    -   In addition to averaging coefficients, consider bootstrapping\n",
    "        the selection of samples and combining predictions for better\n",
    "        stability and more robust estimates.\n",
    "4.  **Increase Sample Size Dynamically:**\n",
    "    -   Instead of a fixed proportion (as defined by `sample_size`),\n",
    "        adaptively determine the sample size based on the total\n",
    "        available samples in each bin to maximize the data usage while\n",
    "        maintaining balance.\n",
    "5.  **Neuron-Specific Adjustments:**\n",
    "    -   You could explore neuron-specific adjustments or weightings if\n",
    "        you suspect particular neurons have differing levels of\n",
    "        reliability or selectivity.\n",
    "6.  **Non-Linear Models:**\n",
    "    -   Experiment with non-linear models, such as kernel ridge\n",
    "        regression or neural networks, if neuronal responses exhibit\n",
    "        non-linearities not captured by linear regression.\n",
    "7.  **Data Augmentation:**\n",
    "    -   If feasible, augment the dataset using techniques such as\n",
    "        jittering (adding small noise) to potentially improve model\n",
    "        robustness and performance.\n",
    "8.  **Feature Engineering:**\n",
    "    -   Explore additional features that may provide predictive power,\n",
    "        such as temporal dynamics if your dataset is time-resolved, or\n",
    "        interaction terms between neurons.\n",
    "9.  **Alternative Binning Strategies:**\n",
    "    -   Experiment with adaptive binning techniques or clustering for\n",
    "        stimulus angles that account for uneven distribution and\n",
    "        differences in angle density more naturally.\n",
    "10. **Consider Using Circular Statistics:**\n",
    "    -   Given the circular nature of the data, consider incorporating\n",
    "        circular statistical methods or models that can directly handle\n",
    "        such data, like circular-linear regression techniques.\n",
    "\n",
    "Implementing any of these ideas could help refine your model, improve\n",
    "its accuracy, and contribute to the robustness of your preferred\n",
    "location estimates.\n",
    "\n",
    "### How would you implement\n",
    "\n",
    "1.  **Increase Sample Size Dynamically:**\n",
    "\n",
    "To implement the idea of dynamically adjusting the sample size based on\n",
    "the total available samples in each bin, the sampling should consider\n",
    "the number of stimuli in each bin relative to the overall dataset,\n",
    "potentially maximizing the available data while maintaining balance.\n",
    "Here’s how you might adjust the existing code:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "def generate_balanced_subsample_dynamic(firing_rates, angles_rad, num_bins=96):\n",
    "    # Discretize the angles into bins\n",
    "    bins = np.linspace(0, 2 * np.pi, num_bins + 1)\n",
    "    digitized = np.digitize(angles_rad, bins, right=True)\n",
    "\n",
    "    # List to store subsampled indices\n",
    "    subsample_indices = []\n",
    "\n",
    "    # Calculate the intended sample size from global sample size as a proportion\n",
    "    global_sample_size = len(angles_rad) // num_bins\n",
    "\n",
    "    # Sample proportionally from each bin\n",
    "    for b in range(1, len(bins)):\n",
    "        bin_indices = np.where(digitized == b)[0]\n",
    "        # Calculate a sample size for each bin as the minimum between what can be taken and the desired global target per bin\n",
    "        num_to_sample = min(global_sample_size, len(bin_indices))\n",
    "        if num_to_sample > 0:\n",
    "            subsample_indices.extend(np.random.choice(bin_indices, size=num_to_sample, replace=False))\n",
    "\n",
    "    # Convert to numpy array\n",
    "    subsample_indices = np.asarray(subsample_indices)\n",
    "\n",
    "    # Create subsample\n",
    "    X_sub = firing_rates[subsample_indices]\n",
    "    y_sub = angles_rad[subsample_indices]\n",
    "\n",
    "    return X_sub, y_sub\n",
    "\n",
    "def average_neuron_selectivity_uniform(firing_rates, angles_rad, num_subsamples=1000, num_bins=96):\n",
    "    coeffs_cos = []\n",
    "    coeffs_sin = []\n",
    "\n",
    "    for _ in range(num_subsamples):\n",
    "        # Generate a balanced subsample\n",
    "        X_sub, y_sub = generate_balanced_subsample_dynamic(firing_rates, angles_rad, num_bins)\n",
    "\n",
    "        # Prepare targets for cosine and sine components\n",
    "        Y_sub = np.column_stack((np.cos(y_sub), np.sin(y_sub)))\n",
    "\n",
    "        # Train Ridge regression\n",
    "        reg = RidgeCV()\n",
    "        reg.fit(X_sub, Y_sub)\n",
    "\n",
    "        # Store coefficients\n",
    "        coeffs_cos.append(reg.coef_[0])  # Cosine coefficients\n",
    "        coeffs_sin.append(reg.coef_[1])  # Sine coefficients\n",
    "\n",
    "    # Average coefficients\n",
    "    avg_coeffs_cos = np.mean(coeffs_cos, axis=0)\n",
    "    avg_coeffs_sin = np.mean(coeffs_sin, axis=0)\n",
    "\n",
    "    predicted_angles_rad = np.arctan2(avg_coeffs_sin, avg_coeffs_cos)\n",
    "    return np.degrees(predicted_angles_rad) % 360\n",
    "```\n",
    "\n",
    "### Key Changes and Explanation:\n",
    "\n",
    "1.  **Dynamic Sampling in Each Bin:**\n",
    "    -   Instead of a fixed `sample_size` proportion, we calculate a\n",
    "        `global_sample_size`, which is the average intended samples per\n",
    "        bin (total data divided by number of bins).\n",
    "2.  **Adaptive Selection:**\n",
    "    -   For each bin, we select samples as the minimum of available data\n",
    "        in the bin or the `global_sample_size`, ensuring the maximum\n",
    "        data usage available in each bin while maintaining relative\n",
    "        balance.\n",
    "\n",
    "This approach preserves balance while maximizing sample usage from bins\n",
    "that have more available data, potentially improving the estimated\n",
    "neuron selectivity by better exploiting the dataset.\n",
    "\n",
    "**\\***\n",
    "\n",
    "1.  **Alternative Binning Strategies:**\n",
    "\n",
    "To implement alternative binning strategies, you can use density-based\n",
    "or adaptive binning methods that account for the distribution of your\n",
    "angular data more naturally. One approach is to use techniques like\n",
    "clustering to find natural groupings in the data. Here’s how you could\n",
    "do this using a simple clustering approach:\n",
    "\n",
    "### Implementation Steps\n",
    "\n",
    "1.  **Use Clustering for Binning:**\n",
    "\n",
    "    You can apply a clustering algorithm, such as K-Means, to identify\n",
    "    clusters of angular locations. This method will adaptively determine\n",
    "    bin sizes based on data density rather than using fixed-width bins.\n",
    "\n",
    "2.  **Clustering Implementation:**\n",
    "\n",
    "    ``` python\n",
    "    import numpy as np\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "\n",
    "    def generate_clustered_subsample(firing_rates, angles_rad, num_clusters=10, sample_size=0.5):\n",
    "        # Reshape angles for clustering\n",
    "        angles_rad_reshaped = angles_rad.reshape(-1, 1)\n",
    "\n",
    "        # Apply K-Means clustering\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(angles_rad_reshaped)\n",
    "\n",
    "        # List to store subsampled indices\n",
    "        subsample_indices = []\n",
    "\n",
    "        # Sample equally from each cluster\n",
    "        for cluster_id in range(num_clusters):\n",
    "            cluster_indices = np.where(clusters == cluster_id)[0]\n",
    "            num_to_sample = int(sample_size * len(cluster_indices))\n",
    "            if num_to_sample > 0 and len(cluster_indices) > 0:\n",
    "                subsample_indices.extend(\n",
    "                    np.random.choice(cluster_indices, size=num_to_sample, replace=False)\n",
    "                )\n",
    "\n",
    "        # Convert to numpy array\n",
    "        subsample_indices = np.asarray(subsample_indices)\n",
    "\n",
    "        # Create subsample\n",
    "        X_sub = firing_rates[subsample_indices]\n",
    "        y_sub = angles_rad[subsample_indices]\n",
    "\n",
    "        return X_sub, y_sub\n",
    "\n",
    "    def average_neuron_selectivity_clustered(firing_rates, angles_rad, num_subsamples=1000, num_clusters=10, sample_size=0.5):\n",
    "        coeffs_cos = []\n",
    "        coeffs_sin = []\n",
    "\n",
    "        for _ in range(num_subsamples):\n",
    "            # Generate a clustered subsample\n",
    "            X_sub, y_sub = generate_clustered_subsample(firing_rates, angles_rad, num_clusters, sample_size)\n",
    "\n",
    "            # Prepare targets for cosine and sine components\n",
    "            Y_sub = np.column_stack((np.cos(y_sub), np.sin(y_sub)))\n",
    "\n",
    "            # Train Ridge regression\n",
    "            reg = RidgeCV()\n",
    "            reg.fit(X_sub, Y_sub)\n",
    "\n",
    "            # Store coefficients\n",
    "            coeffs_cos.append(reg.coef_[0])  # Cosine coefficients\n",
    "            coeffs_sin.append(reg.coef_[1])  # Sine coefficients\n",
    "\n",
    "        # Average coefficients\n",
    "        avg_coeffs_cos = np.mean(coeffs_cos, axis=0)\n",
    "        avg_coeffs_sin = np.mean(coeffs_sin, axis=0)\n",
    "\n",
    "        predicted_angles_rad = np.arctan2(avg_coeffs_sin, avg_coeffs_cos)\n",
    "        return np.degrees(predicted_angles_rad) % 360\n",
    "    ```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "-   **Clustering with K-Means:**\n",
    "    -   The angles are reshaped and clustered into a specified number of\n",
    "        clusters. Each cluster represents a group of angles that are\n",
    "        close to each other, allowing for naturally defined \"bins\".\n",
    "-   **Subsampling from Clusters:**\n",
    "    -   Similar to your original subsample method, you sample equally\n",
    "        from each cluster, ensuring a balanced representation of\n",
    "        neuronal firing rates for the angles within each cluster.\n",
    "-   **Robust Coefficient Estimation:**\n",
    "    -   With the new subsampled data, you continue to apply Ridge\n",
    "        regression and average the coefficients across subsamples.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "-   **Dynamic Adaptation:**\n",
    "    -   The clusters naturally adapt to the distribution of your data,\n",
    "        potentially improving the balance and robustness of your\n",
    "        preferred location estimation.\n",
    "-   **Flexibility:**\n",
    "    -   This method can handle cases where angle distributions are\n",
    "        highly non-uniform by concentrating samples in denser areas.\n",
    "\n",
    "Feel free to modify parameters like `num_clusters` based on your dataset\n",
    "and analysis needs.\n",
    "\n",
    "**\\***\n",
    "\n",
    "1.  **Bootstrap Aggregation (Bagging):**\n",
    "\n",
    "To implement bootstrap aggregation (bagging) for your neuronal activity\n",
    "data, you can repeatedly sample with replacement from your dataset to\n",
    "build multiple models and then average their predictions. Here is how\n",
    "you might incorporate bagging into your existing pipeline:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "def bootstrap_aggregation_predict(firing_rates, angles_rad, num_models=100, num_bins=96, sample_size=0.5):\n",
    "    coeffs_cos = []\n",
    "    coeffs_sin = []\n",
    "\n",
    "    for _ in range(num_models):\n",
    "        # Discretize angles into bins\n",
    "        bins = np.linspace(0, 2 * np.pi, num_bins + 1)\n",
    "        digitized = np.digitize(angles_rad, bins, right=True)\n",
    "\n",
    "        # Create a bootstrap sample\n",
    "        boot_indices = np.random.choice(len(angles_rad), size=len(angles_rad), replace=True)\n",
    "        boot_firing_rates = firing_rates[boot_indices]\n",
    "        boot_angles_rad = angles_rad[boot_indices]\n",
    "\n",
    "        # Balance the sample across angle bins\n",
    "        X_sub, y_sub = generate_balanced_subsample(boot_firing_rates, boot_angles_rad, num_bins, sample_size)\n",
    "\n",
    "        # Prepare targets for cosine and sine components\n",
    "        Y_sub = np.column_stack((np.cos(y_sub), np.sin(y_sub)))\n",
    "\n",
    "        # Train Ridge regression\n",
    "        reg = RidgeCV()\n",
    "        reg.fit(X_sub, Y_sub)\n",
    "\n",
    "        # Store coefficients\n",
    "        coeffs_cos.append(reg.coef_[0])  # Cosine coefficients\n",
    "        coeffs_sin.append(reg.coef_[1])  # Sine coefficients\n",
    "\n",
    "    # Average coefficients\n",
    "    avg_coeffs_cos = np.mean(coeffs_cos, axis=0)\n",
    "    avg_coeffs_sin = np.mean(coeffs_sin, axis=0)\n",
    "\n",
    "    predicted_angles_rad = np.arctan2(avg_coeffs_sin, avg_coeffs_cos)\n",
    "    return np.degrees(predicted_angles_rad) % 360\n",
    "\n",
    "# Ensure reusability by defining the helper function outside.\n",
    "def generate_balanced_subsample(firing_rates, angles_rad, num_bins=96, sample_size=0.5):\n",
    "    bins = np.linspace(0, 2 * np.pi, num_bins + 1)\n",
    "    digitized = np.digitize(angles_rad, bins, right=True)\n",
    "    subsample_indices = []\n",
    "\n",
    "    for b in range(1, len(bins)):\n",
    "        bin_indices = np.where(digitized == b)[0]\n",
    "        num_to_sample = int(sample_size * len(bin_indices))\n",
    "        if num_to_sample > 0 and len(bin_indices) > 0:\n",
    "            subsample_indices.extend(np.random.choice(bin_indices, size=num_to_sample, replace=False))\n",
    "\n",
    "    subsample_indices = np.asarray(subsample_indices)\n",
    "    X_sub = firing_rates[subsample_indices]\n",
    "    y_sub = angles_rad[subsample_indices]\n",
    "\n",
    "    return X_sub, y_sub\n",
    "\n",
    "# Usage\n",
    "# predicted_angles = bootstrap_aggregation_predict(firing_rates, angles_rad)\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1.  **Bootstrap Sampling:** We select subsets of the data with\n",
    "    replacement to create multiple models.\n",
    "2.  **Balanced Subsample:** For each bootstrap sample, we maintain\n",
    "    balance across angle bins before fitting the model.\n",
    "3.  **Model Building:** Fit a Ridge regression model on each balanced\n",
    "    sample and collect the cosine and sine coefficients.\n",
    "4.  **Aggregation:** Average the coefficients from all models to compute\n",
    "    the final predicted angles.\n",
    "\n",
    "This approach can improve robustness by reducing variance and capturing\n",
    "more diverse aspects of the data, thanks to varied training sets\n",
    "achieved through bootstrapping.\n",
    "\n",
    "**\\***"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
