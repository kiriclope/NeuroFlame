{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have a RNN. It is a sparse network of NxN units where the probability of Cij being 1 is K/N 0 otherwise.\n",
    "\n",
    "It sounds like you have a sparsely connected Recurrent Neural Network\n",
    "(RNN) where the connectivity matrix $C$ has dimensions $N \\times N$.\n",
    "Each element $C_{ij}$ in this matrix is 1 with a probability of $K/N$\n",
    "and 0 otherwise. This means, on average, each unit is connected to $K$\n",
    "other units. If you have further questions or need assistance with\n",
    "implementing or analyzing this network, feel free to ask!\n",
    "\n",
    "### Can I constrain the training to give back a sparse matrix\n",
    "\n",
    "Yes, you can impose sparsity constraints during the training of your\n",
    "Recurrent Neural Network (RNN) to maintain a sparse connectivity matrix.\n",
    "Here are common methods to achieve this:\n",
    "\n",
    "1.  **Regularization**: Use L1 regularization on the weight matrices to\n",
    "    encourage sparsity. In many deep learning frameworks, this can be\n",
    "    done by adding an L1 penalty to the loss function.\n",
    "\n",
    "2.  **Pruning**: Iteratively prune the smallest weights during or after\n",
    "    training. This involves zeroing out weights that are below a certain\n",
    "    threshold.\n",
    "\n",
    "3.  **Sparse Initialization**: Initialize the weight matrix sparsely at\n",
    "    the beginning of the training and enforce sparsity throughout the\n",
    "    training process.\n",
    "\n",
    "4.  **Custom Training Algorithms**: Implement custom training algorithms\n",
    "    that update only a subset of weights based on certain criteria\n",
    "    (e.g., keeping a fixed number of largest absolute weight values).\n",
    "\n",
    "5.  **Hard Thresholding**: Periodically apply a hard threshold to the\n",
    "    weights, setting small weights to zero during training.\n",
    "\n",
    "Choose the method that best suits your problem and implement it within\n",
    "your RNN training pipeline.\n",
    "\n",
    "**\\***\n",
    "\n",
    "### I would like to train the network on some labeled inputs but in a way that respect the sparsity. Ideally, I would like the network to learn sqrt(K) extra weights per neuron, so that the in-degree becomes K +- sqrt(K)\n",
    "\n",
    "To train your sparsely connected RNN while maintaining the desired\n",
    "sparsity and in-degree constraints, you can follow these steps:\n",
    "\n",
    "1.  **Initialization**: Ensure the initial connectivity matrix $C$\n",
    "    respects the sparsity condition where each element $C_{ij}$ is 1\n",
    "    with probability $K/N$.\n",
    "\n",
    "2.  **Learning Extra Weights**:\n",
    "\n",
    "    -   For each neuron, randomly select $\\sqrt{K}$ additional incoming\n",
    "        connections on top of the initial $K$ connections.\n",
    "    -   Ensure that for each neuron, the total number of incoming\n",
    "        connections is approximately $K + \\sqrt{K}$.\n",
    "\n",
    "3.  **Weight Update Rule**:\n",
    "\n",
    "    -   Use traditional RNN training techniques, such as Backpropagation\n",
    "        Through Time (BPTT) or Real-Time Recurrent Learning (RTRL),\n",
    "        while ensuring that updates only occur for the selected\n",
    "        connections.\n",
    "\n",
    "4.  **Maintain Sparsity During Training**:\n",
    "\n",
    "    -   During each training iteration, apply updates only to the\n",
    "        non-zero elements in the connectivity matrix $C$.\n",
    "    -   Periodically re-evaluate the sparsity to ensure the in-degree\n",
    "        remains around $K \\pm \\sqrt{K}$ for each neuron. Adjust\n",
    "        connections if necessary.\n",
    "\n",
    "Here's a simplified pseudocode that outlines this process:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "N = 100  # number of units\n",
    "K = 10   # average in-degree\n",
    "sqrtK = int(np.sqrt(K))\n",
    "\n",
    "# Initialize sparse connectivity matrix\n",
    "C = np.random.rand(N, N) < (K / N)\n",
    "\n",
    "# Ensure diagonal is zero (no self-connections)\n",
    "np.fill_diagonal(C, 0)\n",
    "\n",
    "# Add sqrt(K) connections per neuron\n",
    "for i in range(N):\n",
    "    additional_connections = np.random.choice(np.where(C[i] == 0)[0], sqrtK, replace=False)\n",
    "    C[i, additional_connections] = 1\n",
    "\n",
    "# Initialize weights for existing connections\n",
    "W = np.random.randn(N, N) * C\n",
    "\n",
    "def train_rnn(X, y, W, C, learning_rate=0.01, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(X.shape[0]):\n",
    "            # Forward pass\n",
    "            # Apply RNN equations, ensuring to use only weights W[i, j] where C[i, j] == 1\n",
    "\n",
    "            # Compute loss\n",
    "            loss = compute_loss(predicted_output, y[t])\n",
    "\n",
    "            # Backpropagate\n",
    "            delta_W = compute_gradients(loss, W, C)\n",
    "\n",
    "            # Update weights, ensuring changes only occur where C[i, j] == 1\n",
    "            W -= learning_rate * delta_W * C\n",
    "        # Periodically adjust connections if necessary to maintain K ± sqrt(K) in-degrees\n",
    "\n",
    "def compute_loss(predicted, target):\n",
    "    # Define your loss function\n",
    "    pass\n",
    "\n",
    "def compute_gradients(loss, W, C):\n",
    "    # Implement backpropagation to compute gradients w.r.t. W\n",
    "    pass\n",
    "```\n",
    "\n",
    "This pseudocode initializes the sparse network and adds the extra\n",
    "weights, while ensuring the training process respects the sparsity\n",
    "constraints. You'll need to implement specific details for your RNN's\n",
    "forward pass, loss computation, and gradient calculations based on your\n",
    "particular architecture and problem.\n",
    "\n",
    "### How can I do this in torch\n",
    "\n",
    "Training a sparsely connected RNN in PyTorch while respecting your\n",
    "specified sparsity constraints involves custom implementation due to the\n",
    "need to maintain the sparse structure throughout the training process.\n",
    "Below is a concise approach on how to train such a network:\n",
    "\n",
    "1.  **Initialize the sparse RNN**:\n",
    "    -   Create the sparse connectivity matrix.\n",
    "    -   Initialize weights only for the connections that exist in this\n",
    "        matrix.\n",
    "    -   Extend connections to have $ K + \\sqrt{K} $ in-degree per\n",
    "        neuron.\n",
    "2.  **Define the forward pass**:\n",
    "    -   Compute the forward pass using only the non-zero connections.\n",
    "3.  **Backpropagation and Weight Update**:\n",
    "    -   Perform backpropagation and update weights only for the existing\n",
    "        connections.\n",
    "\n",
    "Here's an example implementation in PyTorch:\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "N = 100  # number of units\n",
    "K = 10  # average in-degree\n",
    "sqrtK = int(np.sqrt(K))\n",
    "\n",
    "# Create connectivity matrix\n",
    "C = (torch.rand(N, N) < (K / N)).float()\n",
    "C.fill_diagonal_(0)\n",
    "\n",
    "# Add extra sqrt(K) connections\n",
    "for i in range(N):\n",
    "    zero_indices = (C[i] == 0).nonzero(as_tuple=True)[0]\n",
    "    additional_indices = zero_indices[torch.randperm(len(zero_indices))[:sqrtK]]\n",
    "    C[i, additional_indices] = 1\n",
    "\n",
    "# Initialize weights for non-zero connections\n",
    "W = torch.nn.Parameter(torch.randn(N, N) * C)\n",
    "\n",
    "class SparseRNN(nn.Module):\n",
    "    def __init__(self, W, C):\n",
    "        super(SparseRNN, self).__init__()\n",
    "        self.W = W\n",
    "        self.C = C\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Use only non-zero connections\n",
    "        h_next = torch.tanh(torch.matmul(self.W * self.C, h) + x)\n",
    "        return h_next\n",
    "\n",
    "# Instantiate model\n",
    "model = SparseRNN(W, C)\n",
    "\n",
    "# Define a simple training loop\n",
    "def train(model, data, target, epochs=100, lr=0.01):\n",
    "    optimizer = optim.Adam([model.W], lr=lr)  # Only need to optimize W\n",
    "    loss_fn = nn.MSELoss()  # Example loss function\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        h = torch.zeros(N)  # Initial hidden state\n",
    "\n",
    "        for t in range(data.size(0)):\n",
    "            x = data[t]\n",
    "            y = target[t]\n",
    "\n",
    "            # Forward pass\n",
    "            h = model(x, h)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(h, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/data.size(0)}')\n",
    "\n",
    "# Example data\n",
    "data = torch.randn(10, N)  # Example input data\n",
    "target = torch.randn(10, N)  # Example target data\n",
    "\n",
    "# Train the model\n",
    "train(model, data, target)\n",
    "\n",
    "```\n",
    "\n",
    "1.  Explanation:\n",
    "\n",
    "    1.  **Connectivity Matrix `C`**:\n",
    "        -   Matrix `C` is created with a 1/(N/K) probability of being 1.\n",
    "        -   Diagonal is set to zero to avoid self-connections.\n",
    "        -   Additional $\\sqrt{K}$ connections are added to ensure the\n",
    "            desired in-degree.\n",
    "    2.  **Weights Initialization**:\n",
    "        -   Weights `W` are initialized only where connections exist.\n",
    "    3.  **SparseRNN Class**:\n",
    "        -   Defines the forward pass where only the connected weights\n",
    "            (non-zero entries in `C`) are used.\n",
    "    4.  **Training Loop**:\n",
    "        -   We perform the training with Adam optimizer, calculating\n",
    "            loss with Mean Squared Error (MSE) in this example.\n",
    "\n",
    "    By following this implementation, the network will respect the\n",
    "    sparse connectivity constraints as specified and adjust weights\n",
    "    during training accordingly.\n",
    "\n",
    "### I don t understand\n",
    "\n",
    "this zero<sub>indices</sub> = (C\\[i\\] ==\n",
    "0).nonzero(as<sub>tuple</sub>=True)\\[0\\]\n",
    "\n",
    "Sure, let me further explain this part of the code. The line:\n",
    "\n",
    "``` python\n",
    "zero_indices = (C[i] == 0).nonzero(as_tuple=True)[0]\n",
    "```\n",
    "\n",
    "performs the following steps:\n",
    "\n",
    "1.  **`C[i] =` 0=**:\n",
    "    -   This creates a boolean tensor where `True` indicates positions\n",
    "        in the `i`-th row of `C` that are zero (no connection).\n",
    "2.  **`.nonzero(as_tuple=True)`**:\n",
    "    -   `.nonzero(as_tuple=True)` returns the indices of the non-zero\n",
    "        (or `True`) elements. When applied to a boolean tensor of zeros,\n",
    "        it effectively gives indices where the elements are `False` (or\n",
    "        zero in this context).\n",
    "    -   `as_tuple=True` ensures the function returns the indices as a\n",
    "        tuple of tensors, one for each dimension. Since we're working on\n",
    "        a single row `C[i]`, it will be a 1-D tensor of indices.\n",
    "3.  **`[0]`**:\n",
    "    -   Extracts the first (and only) tensor from the tuple, giving a\n",
    "        1-D tensor of column indices in the `i`-th row where there are\n",
    "        no connections (i.e., the elements are zero).\n",
    "\n",
    "Here's a slightly expanded version:\n",
    "\n",
    "``` python\n",
    "# Boolean tensor where zero connections are marked with True\n",
    "zero_bool_tensor = (C[i] == 0)\n",
    "\n",
    "# Get the indices where C[i] is 0\n",
    "zero_indices_tensor = zero_bool_tensor.nonzero(as_tuple=True)\n",
    "\n",
    "# Extract the first (and only) tensor of indices from the tuple\n",
    "zero_indices = zero_indices_tensor[0]\n",
    "```\n",
    "\n",
    "These zero indices are then used to randomly select $\\sqrt{K}$\n",
    "additional indices to create new connections:\n",
    "\n",
    "``` python\n",
    "additional_indices = zero_indices[torch.randperm(len(zero_indices))[:sqrtK]]\n",
    "C[i, additional_indices] = 1\n",
    "```\n",
    "\n",
    "Here’s a conceptual breakdown of the whole process:\n",
    "\n",
    "1.  Identify positions in the current row `C[i]` that lack a connection\n",
    "    (zero values).\n",
    "2.  From these positions, randomly select $\\sqrt{K}$ indices.\n",
    "3.  Set these selected positions in the connectivity matrix `C` to 1,\n",
    "    thereby adding new connections.\n",
    "\n",
    "Feel free to ask any further clarifications!\n",
    "\n",
    "**\\***"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
