{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Settings\n",
    "\n",
    "``` ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "%run ../../notebooks/setup.py\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "```\n",
    "\n",
    "# Imports\n",
    "\n",
    "``` ipython\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "\n",
    "from src.network import Network\n",
    "from src.decode import decode_bump\n",
    "from src.utils import clear_cache\n",
    "```\n",
    "\n",
    "# Helpers\n",
    "\n",
    "``` ipython\n",
    "def convert_seconds(seconds):\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    return h, m, s\n",
    "```\n",
    "\n",
    "# Parameters\n",
    "\n",
    "``` ipython\n",
    "REPO_ROOT = '/home/leon/models/NeuroTorch/'\n",
    "conf_name = 'config_ringEI.yml'\n",
    "```\n",
    "\n",
    "# Exploring Parameter Space\n",
    "\n",
    "To find parameters for which we have a multistable ring attractor, we\n",
    "use torch **batching** capabilities to run parallel simulations across\n",
    "the parameter space. The idea is that we will create \"batches\" of\n",
    "parameters and pass them to the model.\n",
    "\n",
    "## Batching a single parameter\n",
    "\n",
    "``` ipython\n",
    "model = Network(conf_name, REPO_ROOT, IF_STP=1, DT=0.001, VERBOSE=0, LIVE_FF_UPDATE=1, I0=[1, 0])\n",
    "```\n",
    "\n",
    "With torch we can easily pass lists of parameters or batches to the\n",
    "model. Here, let's batch the recurrent strenght $J_{EE}$.\n",
    "\n",
    "``` ipython\n",
    "N_BATCH = 20\n",
    "# Here we pass a list of parameters to J_STP which is JEE for the model with stp\n",
    "model.J_STP = torch.linspace(0, 20, N_BATCH, dtype=torch.float32, device='cuda')\n",
    "\n",
    "# For consistency we need to add a dummy extra dimension\n",
    "# This is so that the models performs dot products correctly\n",
    "# In the model J_STP is multiplied by rates of size (N_BATCH * N_NEURON)\n",
    "# (N_BATCH * 1) * (N_BATCH * N_NEURON) = (N_BATCH * N_NEURON)\n",
    "\n",
    "model.J_STP = model.J_STP.unsqueeze(-1)\n",
    "\n",
    "# We set the number of batches\n",
    "model.N_BATCH = N_BATCH\n",
    "# and run the model\n",
    "\n",
    "rates_Jee = model(RET_STP=1).cpu().detach().numpy()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "m0, m1, phi = decode_bump(rates_Jee, axis=-1)\n",
    "print(m0.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "fig, ax = plt.subplots(1, 2, figsize=[2*width, height])\n",
    "\n",
    "mean_rates = rates_Jee[:,-1].mean(-1)\n",
    "\n",
    "ax[0].plot(model.J_STP.cpu(), mean_rates)\n",
    "ax[0].set_xlabel('$J_{EE}$')\n",
    "ax[0].set_ylabel('$<Rates>_i$')\n",
    "\n",
    "ax[1].plot(rates_Jee.mean(-1).T)\n",
    "ax[1].set_xlabel('$J_{EE}$')\n",
    "ax[1].set_ylabel('Rates')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "print(model.J_STP.shape, m1.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "fig, ax = plt.subplots(1, 2, figsize=[2*width, height])\n",
    "\n",
    "ax[0].plot(model.J_STP.cpu(), m1[:, -1])\n",
    "ax[0].set_xlabel('$J_{EE}$')\n",
    "ax[0].set_ylabel('$\\mathcal{F}_1$')\n",
    "\n",
    "ax[1].plot(m1.T)\n",
    "ax[1].set_xlabel('$Step$')\n",
    "ax[1].set_ylabel('$\\mathcal{F}_1$')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Here, for example, with J<sub>STP</sub>=10 we have a ring attractor!\n",
    "\n",
    "``` ipython\n",
    "\n",
    "```\n",
    "\n",
    "## Batching multiple parameters\n",
    "\n",
    "Sometimes we won't be so lucky and need to search harder over multiple\n",
    "parameters. In order to **batch** over multiple parameters, we need to\n",
    "carefully create each parameter batch. Here, let's batch the recurrent\n",
    "strenght $J_{EE}$ and the feedforward strength $J_{E0}$.\n",
    "\n",
    "``` ipython\n",
    "model = Network(conf_name, REPO_ROOT, IF_STP=1, DT=0.001, VERBOSE=0, LIVE_FF_UPDATE=1, N_BATCH=1, I0=[1, 0])\n",
    "```\n",
    "\n",
    "First we create the lists of parameters to sweep\n",
    "\n",
    "``` ipython\n",
    "N_JEE = 20\n",
    "N_JE0 = 20\n",
    "\n",
    "JEE_list = torch.linspace(5, 15, N_JEE, device='cuda')\n",
    "print('Jee list', JEE_list)\n",
    "JE0_list = torch.linspace(0, 10, N_JE0, device='cuda')\n",
    "print('Je0 list', JE0_list)\n",
    "```\n",
    "\n",
    "Now we need to expand these lists into tensors with the correct shapes.\n",
    "To do so we create a two new tensors J<sub>EE</sub> and J<sub>E0</sub>\n",
    "of size (N<sub>JEE</sub>, N<sub>JE0</sub>) where each row of\n",
    "J<sub>EE</sub> is a repetition of Jee list and each column of Je0 is a\n",
    "copy of Je0 list. In that way, all the values of J<sub>EE</sub> are\n",
    "associated once with a value of Je0.\n",
    "\n",
    "``` ipython\n",
    "JEE = JEE_list.unsqueeze(0).expand(N_JE0, N_JEE)\n",
    "print('JEE cols', JEE[0])\n",
    "\n",
    "JE0 = JE0_list.unsqueeze(1).expand(N_JE0, N_JEE)\n",
    "print('JE0 rows', JE0[:, 0])\n",
    "```\n",
    "\n",
    "Torch models need a single batch dimension so we concatenate the two\n",
    "dimensions into tensors of size\n",
    "(N<sub>BATCH</sub>=N<sub>JEE</sub>\\*N<sub>JE0</sub>, 1) We need the\n",
    "extra dummy dimension so that in the model dot products are done\n",
    "properly.\n",
    "\n",
    "``` ipython\n",
    "JEE = JEE.reshape((-1, 1)) \n",
    "print('JEE', JEE.shape)\n",
    "\n",
    "JE0 = JE0.reshape((-1, 1)) * model.M0  # Here M0 is a scaling factor that we have to add\n",
    "print('JE0', JE0.shape)\n",
    "```\n",
    "\n",
    "Now we need to set the number of batches and copy our tensors to the\n",
    "model\n",
    "\n",
    "``` ipython\n",
    "N_BATCH = N_JE0 * N_JEE\n",
    "# Here we need to do some work on Ja0 first,\n",
    "# since it has two dimensions for E and I and we need to repeat the I values\n",
    "Ja0 = model.Ja0.repeat((N_BATCH, 1, 1))\n",
    "print('Ja0', Ja0.shape)\n",
    "\n",
    "# now we can pass JE0 to Ja0\n",
    "Ja0[:,0] = JE0\n",
    "\n",
    "# and pass N_BATCH, Ja0 and Jee to the model\n",
    "model.N_BATCH = N_BATCH\n",
    "# copy Ja0\n",
    "model.Ja0 = Ja0\n",
    "# in the model with stp, JEE is J_STP\n",
    "model.J_STP = JEE  \n",
    "```\n",
    "\n",
    "Let's run the simulations\n",
    "\n",
    "``` ipython\n",
    "start = perf_counter()\n",
    "rates = model().cpu().detach().numpy()\n",
    "end = perf_counter()\n",
    "print(\"Elapsed (with compilation) = %dh %dm %ds\" % convert_seconds(end - start))\n",
    "print('rates', rates.shape)\n",
    "```\n",
    "\n",
    "Let's compute the fourier moments of the population activity and reshape\n",
    "them\n",
    "\n",
    "``` ipython\n",
    "m0, m1, phi = decode_bump(rates, axis=-1)\n",
    "\n",
    "m0 = m0.reshape(N_JE0, N_JEE, -1)\n",
    "m1 = m1.reshape(N_JE0, N_JEE, -1)  \n",
    "```\n",
    "\n",
    "``` ipython\n",
    "JEE = np.linspace(5, 10, N_JEE)\n",
    "JE0 = np.linspace(0, 10, N_JE0)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "fig, ax = plt.subplots(1, 2, figsize=[2*width, height])\n",
    "\n",
    "ax[0].imshow(m0[..., -1], cmap='jet', origin='lower', vmin=0, aspect='auto', extent=[JEE[0], JEE[-1], JE0[0], JE0[-1]])\n",
    "ax[0].set_xlabel('$J_{EE}$')\n",
    "ax[0].set_ylabel('$J_{E0}$')\n",
    "\n",
    "ax[1].imshow(m1[...,-1]/m0[...,-1], cmap='jet', origin='lower', vmin=0, vmax=3, aspect='auto', extent=[JEE[0], JEE[-1], JE0[0], JE0[-1]])\n",
    "ax[1].set_xlabel('$J_{EE}$')\n",
    "ax[1].set_ylabel('$J_{E0}$')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "idx=9\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=[2*width, height])\n",
    "\n",
    "ax[0].plot(m1[idx].T)\n",
    "ax[0].set_ylabel('$\\mathcal{F}_1$')\n",
    "ax[0].set_xlabel('step')\n",
    "ax[0].set_title('Varying $J_{EE}$')\n",
    "\n",
    "ax[1].plot(m1[:, idx].T)\n",
    "ax[1].set_ylabel('$\\mathcal{F}_1$')\n",
    "ax[1].set_xlabel('step')\n",
    "ax[1].set_title('Varying $J_{E0}$')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "The parameters corresponding to (row 3, col -1) work!\n",
    "\n",
    "We can get their values from their matrix form\n",
    "\n",
    "``` ipython\n",
    "JEE = torch.linspace(5, 10, N_JEE, device='cuda')\n",
    "JE0 = torch.linspace(0, 10, N_JE0, device='cuda')\n",
    "\n",
    "JEE = JEE.unsqueeze(0).expand(N_JE0, N_JEE)\n",
    "JE0 = JE0.unsqueeze(1).expand(N_JE0, N_JEE)\n",
    "\n",
    "print('JE0', JE0[3, -1].item())\n",
    "print('JEE', JEE[3, -1].item())\n",
    "```\n",
    "\n",
    "or directly from the original lists\n",
    "\n",
    "``` ipython\n",
    "JE0 = torch.linspace(0, 10, N_JE0, device='cuda')\n",
    "print('JE0', JE0[3].item())\n",
    "\n",
    "JEE = torch.linspace(5, 10, N_JEE, device='cuda')\n",
    "print('JEE', JEE[-1].item())\n",
    "```\n",
    "\n",
    "Let's test them.\n",
    "\n",
    "``` ipython\n",
    "model = Network(conf_name, REPO_ROOT, TASK='odr',\n",
    "                VERBOSE=0, DEVICE='cuda', seed=0, N_BATCH=1, LIVE_FF_UPDATE=1)\n",
    "\n",
    "# model.Ja0[:, 0] = JE0[3] \n",
    "# model.J_STP = JEE[-1]\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "rates_test = model().cpu().numpy()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "m0, m1, phi = decode_bump(rates_test, axis=-1)\n",
    "print('m0', m0.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "fig, ax = plt.subplots(1, 2, figsize=(2*width, height))\n",
    "\n",
    "r_max = 30\n",
    "\n",
    "ax[0].imshow(rates_test[0].T, aspect='auto', cmap='jet', vmin=0, vmax=r_max, origin='lower')\n",
    "ax[0].set_ylabel('Neuron #')\n",
    "ax[0].set_xlabel('Step')\n",
    "\n",
    "ax[1].plot(m1.T)\n",
    "ax[1].set_ylabel('m1')\n",
    "ax[1].set_xlabel('Step')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "# Serial bias\n",
    "\n",
    "Now that we have found a ring attractor we can investigate the biases in\n",
    "the model\n",
    "\n",
    "``` ipython\n",
    "model = Network(conf_name, REPO_ROOT, TASK='odr',\n",
    "                VERBOSE=0, DEVICE='cuda', seed=0, N_BATCH=1, LIVE_FF_UPDATE=1,\n",
    "                I0=[1.0, -5.0, 1.0],\n",
    "                T_STIM_ON=[1, 4, 7],\n",
    "                T_STIM_OFF=[2, 5, 8],\n",
    "                SIGMA0=[1.0, 0.0, 1.0],\n",
    "                PHI0=[180, 0, 180])\n",
    "```\n",
    "\n",
    "## Simulations\n",
    "\n",
    "``` ipython\n",
    "N_PHASE = 512\n",
    "print(model.PHI0.shape)\n",
    "\n",
    "PHI0 = model.PHI0.unsqueeze(-1).repeat((N_PHASE, 1, 1))\n",
    "\n",
    "print(PHI0.shape)\n",
    "PHI0[:, -1] = torch.randint(0, 360, (N_PHASE,), device=model.device).unsqueeze(1)\n",
    "# PHI0[:, 0] = torch.randint(0, 360, (N_PHASE,), device=model.device).unsqueeze(1)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "model.PHI0 = PHI0\n",
    "model.N_BATCH = N_PHASE\n",
    "rates = model().cpu().numpy()\n",
    "print(rates.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "m0, m1, phi = decode_bump(rates, axis=-1)\n",
    "print(phi.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "fig, ax = plt.subplots(1, 2, figsize=[2*width, height])\n",
    "r_max = np.max(rates[0]) *2\n",
    "\n",
    "ax[0].imshow(rates[1].T, aspect='auto', cmap='jet', vmin=0, vmax=r_max, origin='lower')\n",
    "ax[0].set_ylabel('Pref. Location (°)')\n",
    "ax[0].set_yticks(np.linspace(0, model.Na[0].cpu(), 5), np.linspace(0, 360, 5).astype(int))\n",
    "ax[0].set_xlabel('Step')\n",
    "\n",
    "ax[1].plot(phi.T * 180 / np.pi)\n",
    "ax[1].set_yticks(np.linspace(0, 360, 5).astype(int), np.linspace(0, 360, 5).astype(int))\n",
    "ax[1].set_ylabel('Pref. Location (°)')\n",
    "ax[1].set_xlabel('Step')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "target_loc = model.PHI0[:, -1].cpu().numpy()\n",
    "rel_loc = model.PHI0[:, 0].cpu().numpy() - target_loc\n",
    "rel_loc = (rel_loc / 180 * np.pi + np.pi) % (2*np.pi) - np.pi\n",
    "errors = phi - target_loc * np.pi / 180.0\n",
    "errors = (errors + np.pi) % (2*np.pi) - np.pi\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plt.hist(rel_loc * 180 / np.pi)\n",
    "plt.xlabel('Relative Loc (°)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "plt.hist(errors[:, -1] * 180/np.pi, bins='auto')\n",
    "plt.xlabel('errors (°)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Systematic biases\n",
    "\n",
    "``` ipython\n",
    "plt.plot(target_loc[:, 0], errors[:,-1] * 180 / np.pi, 'o')\n",
    "plt.xlabel('Target Loc. (°)')\n",
    "plt.ylabel('Error (°)')\n",
    "\n",
    "from scipy.stats import binned_statistic\n",
    "stt = binned_statistic(target_loc[:,0], errors[:,-1] * 180/np.pi, statistic='mean', bins=30, range=[0, 360])\n",
    "dstt = np.mean(np.diff(stt.bin_edges))\n",
    "plt.plot(stt.bin_edges[:-1]+dstt/2,stt.statistic,'r')\n",
    "\n",
    "plt.axhline(color='k', linestyle=\":\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Serial biases\n",
    "\n",
    "``` ipython\n",
    "plt.plot(rel_loc[:, 0] * 180 / np.pi, errors[:,-1] * 180 / np.pi, 'o')\n",
    "plt.xlabel('Rel. Loc. (°)')\n",
    "plt.ylabel('Error (°)')\n",
    "# plt.ylim([-60, 60])\n",
    "\n",
    "from scipy.stats import binned_statistic\n",
    "stt = binned_statistic(rel_loc[:,0]*180/np.pi, errors[:,-1]*180/np.pi, statistic='mean', bins=40, range=[-360, 360])\n",
    "dstt = np.mean(np.diff(stt.bin_edges))\n",
    "plt.plot(stt.bin_edges[:-1]+dstt/2,stt.statistic,'r')\n",
    "\n",
    "plt.axhline(color='k', linestyle=\":\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "\n",
    "```"
   ],
   "id": "37d7ea82-d34f-41c5-82c1-a2eed63bc0b9"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
