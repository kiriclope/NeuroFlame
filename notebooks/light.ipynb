{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Settings\n",
    "=================\n",
    "\n",
    "``` ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "%run ../notebooks/setup.py\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "REPO_ROOT = \"/home/leon/models/NeuroFlame\"\n",
    "pal = sns.color_palette(\"tab10\")\n",
    "```\n",
    "\n",
    "Imports\n",
    "=======\n",
    "\n",
    "``` ipython\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "DEVICE = 'cuda:1'\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from time import perf_counter\n",
    "from scipy.stats import circmean\n",
    "\n",
    "from src.network import Network\n",
    "from src.lrnet import LRNet\n",
    "\n",
    "from src.plot_utils import plot_con\n",
    "from src.decode import decode_bump, circcvl\n",
    "from src.lr_utils import masked_normalize, clamp_tensor, normalize_tensor\n",
    "```\n",
    "\n",
    "Helpers\n",
    "=======\n",
    "\n",
    "Data Split\n",
    "----------\n",
    "\n",
    "``` ipython\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "def split_data(X, Y, train_perc=0.8, batch_size=32):\n",
    "\n",
    "  if Y.ndim==3:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        train_size=train_perc,\n",
    "                                                        stratify=Y[:, 0, 0].cpu().numpy(),\n",
    "                                                        shuffle=True)\n",
    "  else:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        train_size=train_perc,\n",
    "                                                        stratify=Y[:, 0].cpu().numpy(),\n",
    "                                                        shuffle=True)\n",
    "  print(X_train.shape, X_test.shape)\n",
    "  print(Y_train.shape, Y_test.shape)\n",
    "\n",
    "  train_dataset = TensorDataset(X_train, Y_train)\n",
    "  val_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "  # Create data loaders\n",
    "  train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  return train_loader, val_loader\n",
    "```\n",
    "\n",
    "Configuration\n",
    "=============\n",
    "\n",
    "``` ipython\n",
    "REPO_ROOT = \"/home/leon/models/NeuroFlame\"\n",
    "conf_name = \"config_light.yml\"\n",
    "DEVICE = 'cuda:1'\n",
    "seed = np.random.randint(0, 1e6)\n",
    "print(seed)\n",
    "#seed = 760946\n",
    "```\n",
    "\n",
    "Model\n",
    "=====\n",
    "\n",
    "``` ipython\n",
    "model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE=DEVICE, SEED=seed, N_BATCH=16)\n",
    "```\n",
    "\n",
    "Dataset\n",
    "=======\n",
    "\n",
    "``` ipython\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "model.LR_TRAIN = 1\n",
    "model.LR_READOUT=1\n",
    "```\n",
    "\n",
    "Testing the network on steps from sample odor offset to test odor onset\n",
    "\n",
    "``` ipython\n",
    "steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)\n",
    "\n",
    "mask = (steps >= (model.N_STIM_OFF[0] - model.N_STEADY)) & (steps <= (model.N_STEPS - model.N_STEADY))\n",
    "rwd_idx = np.where(mask)[0]\n",
    "print('rwd', rwd_idx)\n",
    "\n",
    "model.lr_eval_win = rwd_idx.shape[0]\n",
    "\n",
    "stim_mask = (steps >= (model.N_STIM_ON[0] - model.N_STEADY)) & (steps < (model.N_STIM_OFF[0] - model.N_STEADY))\n",
    "\n",
    "zero_idx = np.where(~mask & ~stim_mask )[0]\n",
    "print('zero', zero_idx)\n",
    "```\n",
    "\n",
    "### Inputs and Labels\n",
    "\n",
    "``` ipython\n",
    "model.N_BATCH = 64\n",
    "\n",
    "model.I0[0] = 2.0\n",
    "model.I0[1] = 0\n",
    "model.I0[2] = 0\n",
    "\n",
    "A = model.init_ff_input()\n",
    "\n",
    "model.I0[0] = -2.0\n",
    "model.I0[1] = 0\n",
    "model.I0[2] = 0\n",
    "\n",
    "B = model.init_ff_input()\n",
    "\n",
    "ff_input = torch.cat((A, B))\n",
    "print(ff_input.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "labels_A = torch.ones((model.N_BATCH, 1))\n",
    "labels_B = torch.zeros((model.N_BATCH, 1))\n",
    "labels = torch.cat((labels_A, labels_B))\n",
    "\n",
    "print('labels', labels.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "batch_size = 16\n",
    "train_loader, val_loader = split_data(ff_input.to(DEVICE), labels.to(DEVICE), train_perc=0.8, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "Run\n",
    "===\n",
    "\n",
    "``` ipython\n",
    "autoencoder = LRNet(model)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "y_pred = autoencoder()\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "print(y_pred.shape)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# Init ModelCheckpoint callback, monitoring 'val_loss'\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "\n",
    "# Add your callback to the callbacks list\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "trainer = L.Trainer(devices=[1], max_epochs=30, num_sanity_val_steps=0, callbacks=[checkpoint_callback], enable_progress_bar=0)\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader);\n",
    "```\n",
    "\n",
    "``` example\n",
    "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
    "\n",
    "  | Name      | Type              | Params\n",
    "------------------------------------------------\n",
    "0 | model     | Network           | 2.5 K\n",
    "1 | linear    | Linear            | 501\n",
    "2 | criterion | BCEWithLogitsLoss | 0\n",
    "------------------------------------------------\n",
    "3.0 K     Trainable params\n",
    "0         Non-trainable params\n",
    "3.0 K     Total params\n",
    "0.012     Total estimated model params size (MB)\n",
    "Epoch 0 - Training loss: 0.20096835494041443 - Validation loss: 0.12238327413797379\n",
    "```\n",
    "\n",
    "``` ipython\n",
    "train_loss = trainer.logged_metrics.get('train_loss')\n",
    "val_loss = trainer.logged_metrics.get('val_loss')\n",
    "\n",
    "print(f'Training Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "```"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
