#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session dual :kernel torch

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run ../notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'

  REPO_ROOT = "/home/leon/models/NeuroTorch"
  pal = sns.color_palette("tab10")
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Helpers
** Training
*** split data

#+begin_src ipython
  def split_data(X, Y, train_perc=0.8, batch_size=32):

    # Split the dataset into training and validation sets
    train_size = int(train_perc * X.shape[0])

    X_train = X[:train_size]
    X_test = X[train_size:]

    # X_train, X_mean, X_std = standard_scaler(X_train, IF_RETURN=1)
    # X_test = (X_test - X_mean) / X_std

    Y_train = Y[:train_size]    
    Y_test = Y[train_size:]

    # Y_train, Y_mean, Y_std = standard_scaler(Y_train, IF_RETURN=1)
    # Y_test = (Y_test - Y_mean) / Y_std

    # Create data sets
    # train_dataset = TensorDataset(X_train_scaled, Y_train_scaled)
    # val_dataset = TensorDataset(X_test_scaled, Y_test_scaled)

    # print('X_train', X_train.shape, 'y_train', Y_train.shape)
    train_dataset = TensorDataset(X_train, Y_train)

    # print('X_test', X_test.shape, 'y_test', Y_test.shape)
    val_dataset = TensorDataset(X_test, Y_test)

    # Create data loaders
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

    # sequence_length = 14  # or any other sequence length you want
    # stride = 1  # or any other stride you want

    # sliding_window_dataset = SlidingWindowDataset(X, sequence_length, stride)
    # train_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)
    # val_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)

    return train_loader, val_loader
#+end_src

#+RESULTS:

** Optimization

#+begin_src ipython
  def train(dataloader, model, loss_fn, optimizer, penalty=None, lbd=1):
      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      model.train()
      for batch, (X, y) in enumerate(dataloader):
          optimizer.zero_grad()

          X, y = X.to(device), y.to(device)
          # Compute prediction error
          y_pred = model(X)
          loss = loss_fn(y_pred, y)

          if penalty is not None:
              reg_loss = 0
              for param in model.parameters():
                  if penalty=='l1':
                      reg_loss += torch.sum(torch.abs(param))
                  else:
                      reg_loss += torch.sum(torch.square(param))

                  loss = loss + lbd * reg_loss

          # Backpropagation
          loss.backward()
          optimizer.step()

      return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def test(dataloader, model, loss_fn):
      size = len(dataloader.dataset)
      num_batches = len(dataloader)

      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      # Validation loop.
      model.eval()
      val_loss = 0.0
      with torch.no_grad():
          for data, targets in dataloader:
              data, targets = data.to(device), targets.to(device)
              
              outputs = model(data)
              loss = loss_fn(outputs, targets)
              val_loss += loss.item() * data.size(0)
          val_loss /= size

      return val_loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def run_optim(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100, penalty=None, lbd=1):

    # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)
    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    # Training loop.
    for epoch in range(num_epochs):
        loss = train(train_loader, model, loss_fn, optimizer, penalty, lbd)
        val_loss = test(val_loader, model, loss_fn)
        scheduler.step(val_loss)
        
        # if epoch % int(num_epochs  / 10) == 0:
        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

#+end_src

#+RESULTS:

** Loss

#+begin_src ipython
  def correlation_loss(output, target):
      # Subtract the mean of each vector
      output_mean = output - torch.mean(output)
      target_mean = target - torch.mean(target)
    
      # Compute the covariance between output and target
      covariance = torch.mean(output_mean * target_mean)
      
      # Compute the standard deviations of the vectors
      output_std = torch.std(output)
      target_std = torch.std(target)
    
      # Calculate the Pearson correlation coefficient
      correlation = covariance / (output_std * target_std)
    
      # Since we want to increase the correlation, we minimize its negative
      loss = -correlation  # Maximizing correlation by minimizing its negative
    
      return loss
#+end_src

#+RESULTS:

#+begin_src ipython
    import torch
    import torch.nn as nn

    def sign_constrained_loss(output, xi, target_sign):
        dot_product = torch.dot(output.flatten(), xi.flatten())
        if target_sign > 0:
            loss = torch.relu(-dot_product)  # Encourages positive dot product
        else:
            loss = torch.relu(dot_product)   # Encourages negative dot product
        return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  class CosineLoss(nn.Module):
      def __init__(self):
          super(CosineLoss, self).__init__()
          self.cosine_similarity = nn.CosineSimilarity(dim=-1)
          
      def forward(self, input1, input2):
          # Calculate cosine similarity
          cosine_sim = self.cosine_similarity(input1, input2)
          # Calculate the loss as 1 - cosine_similarity
          loss = 1 - cosine_sim
          # Return the mean loss over the batch
          return loss.mean()
#+end_src

#+RESULTS:


#+RESULTS:

** Other

#+begin_src ipython
  def get_theta(a, b, GM=0, IF_NORM=0):

      u, v = a, b

      if GM:          
          v = b - np.dot(b, a) / np.dot(a, a) * a
          
      if IF_NORM:
          u = a / np.linalg.norm(a)
          v = b / np.linalg.norm(b)

      return np.arctan2(v, u)
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_idx(model):
      ksi = model.U.cpu().detach().numpy().T
      print(ksi.shape)
      
      idx = np.arange(0, len(ksi[0]))
      theta = get_theta(ksi[0][:model.Na[0]], ksi[1][:model.Na[0]], GM=0, IF_NORM=1)

      return theta.argsort()
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_overlap(model, rates):
      ksi = model.PHI0.cpu().detach().numpy()
      return rates @ ksi.T / rates.shape[-1]
  
#+end_src

#+RESULTS:

#+begin_src ipython
  import scipy.stats as stats

  def plot_smooth(data, ax, color):
      mean = data.mean(axis=0)  
      ci = smooth.std(axis=0, ddof=1) * 1.96
      
      # Plot
      ax.plot(mean, color=color)
      ax.fill_between(range(data.shape[1]), mean - ci, mean + ci, alpha=0.25, color=color)

#+end_src

#+RESULTS:

#+begin_src ipython
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

* Imports

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader
#+end_src

#+RESULTS:

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  import pandas as pd
  import torch.nn as nn
  from time import perf_counter  
  from scipy.stats import circmean

  from src.network import Network
  from src.plot_utils import plot_con
  from src.decode import decode_bump, circcvl
#+end_src

#+RESULTS:

* Train RNN
** Parameters

#+Begin_src ipython
  REPO_ROOT = "/home/leon/models/NeuroTorch"
  conf_name = "config_EI.yml"
  name = "dual"
#+end_src

#+RESULTS:

** Model

#+begin_src ipython
  start = perf_counter()
  name = "dual_single"
  model = Network(conf_name, name, REPO_ROOT, VERBOSE=0, DEVICE='cuda', SEED=2)
#+end_src

#+RESULTS:

#+begin_src ipython
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name, param.data)
#+end_src

#+RESULTS:
#+begin_example
  U tensor([[ 2.8170,  0.8250],
          [-0.6454,  0.6088],
          [ 1.2480,  0.0898],
          ...,
          [-1.1573,  0.8064],
          [ 0.6220,  0.1639],
          [ 1.0931, -0.1276]], device='cuda:0')
  lr_kappa tensor([3.0735])
  linear.weight tensor([[-1.3414e-02,  2.2292e-02, -3.3652e-02,  8.8285e-05, -2.7914e-02,
           -2.2706e-02,  3.3987e-02, -1.9835e-02, -2.6323e-02,  2.0118e-02,
            7.1548e-03, -2.9215e-02,  1.7650e-02, -3.0055e-02, -3.1824e-02,
            3.9900e-03,  4.5980e-03, -1.8655e-02,  2.0681e-02,  1.8434e-02,
           -3.4675e-02,  2.3409e-02, -2.3212e-02,  2.9986e-02, -1.6496e-02,
            2.3428e-03,  9.7338e-05,  3.1663e-02,  2.8224e-02,  1.6310e-02,
            5.0186e-03,  1.3307e-02,  2.0737e-03, -3.1395e-03, -1.5015e-02,
           -1.3737e-02, -2.2468e-02, -2.9115e-02,  1.5928e-02, -2.4741e-02,
            9.0732e-03, -3.3532e-02, -1.8074e-02, -3.0280e-02, -2.7278e-02,
            5.3060e-03, -2.2231e-02, -3.4290e-02,  2.0885e-02,  8.3934e-03,
           -3.1686e-03,  1.7568e-02,  1.5384e-02,  9.6962e-03, -1.5848e-02,
            3.2266e-02, -3.3651e-02, -5.5181e-03, -2.7921e-02,  1.9717e-02,
           -2.6086e-02,  2.6467e-02,  2.4014e-03,  2.9312e-02, -2.9859e-02,
           -2.8394e-02,  6.0239e-03,  3.1710e-03, -4.9807e-03, -2.0283e-02,
            1.7555e-03,  1.3255e-02, -4.4711e-03,  2.9796e-03,  1.5843e-02,
           -1.5665e-02,  3.2316e-02,  2.6559e-02,  1.1043e-02, -1.6837e-02,
           -2.9016e-03, -2.4146e-02, -2.6653e-02, -3.3597e-03,  2.7507e-02,
            2.0810e-02,  2.8688e-02, -3.3411e-03,  3.4992e-03,  5.7650e-03,
            1.5669e-02,  3.5088e-02, -1.9276e-02,  1.6669e-02, -2.2966e-02,
            9.0376e-03,  9.8022e-03, -2.2629e-02, -3.0682e-03,  2.7867e-02,
           -3.2435e-02,  1.0974e-02,  9.2450e-03,  3.1718e-02,  3.4326e-02,
           -3.3943e-02,  2.3703e-02, -2.4853e-02,  1.1541e-02, -2.5097e-02,
           -9.0574e-04,  5.5340e-03,  2.9987e-02,  2.7766e-02, -1.9554e-02,
           -9.4093e-04, -3.5001e-02, -2.6325e-02, -1.1638e-02, -2.8008e-03,
           -6.0850e-03, -5.2519e-03, -1.5500e-02,  3.2747e-02, -1.2916e-02,
            1.4964e-02, -1.1238e-04, -3.0715e-02,  2.6480e-02, -3.0119e-02,
           -1.9677e-03,  3.0846e-02, -2.6173e-02,  2.4848e-02,  1.2736e-02,
            1.5651e-02,  4.4392e-03, -3.3819e-02, -1.2898e-02,  1.3808e-02,
           -3.4128e-02,  3.4342e-02,  9.3869e-03,  3.0812e-02, -8.1207e-04,
           -2.7613e-02,  2.5550e-02,  3.3577e-02, -2.6063e-02, -2.2778e-02,
            2.9430e-02, -2.2092e-02, -3.4526e-02,  1.8211e-03, -2.2499e-02,
           -2.3157e-03,  3.4503e-02, -2.6187e-02,  4.5097e-05,  3.9784e-03,
           -8.1154e-03,  1.5935e-02,  3.3893e-02,  2.7065e-02, -2.6942e-03,
           -5.8747e-03,  1.7758e-02, -4.6914e-03,  3.0123e-02,  2.0045e-03,
           -2.3037e-03,  2.5934e-02,  7.4770e-03,  1.9367e-03,  3.4673e-03,
            1.8300e-02,  2.1766e-02, -1.9157e-02, -2.2356e-02, -1.2783e-02,
            2.9876e-02, -1.7447e-02,  1.4452e-02, -2.7817e-02,  1.9373e-02,
            1.2936e-02,  3.0616e-02,  3.8952e-03,  5.6742e-03,  1.2893e-02,
            1.4592e-02, -3.2435e-02,  3.4899e-02,  6.8297e-03, -2.2347e-02,
            3.1330e-02, -2.6095e-02, -4.9173e-03, -7.1016e-03, -2.7564e-03,
           -1.0637e-02, -6.3956e-03,  2.3094e-02, -1.5013e-02,  4.3933e-03,
            5.8707e-03,  2.7919e-02,  1.2339e-02,  3.3098e-02,  2.1251e-02,
            2.3321e-02, -2.1361e-02,  6.1450e-03, -2.8359e-02, -1.7213e-02,
           -2.2041e-02, -3.5187e-02,  2.1709e-02,  3.4166e-02,  1.9697e-02,
            1.5635e-02, -9.0356e-04, -2.0885e-02,  2.8362e-02,  2.4309e-02,
            1.7198e-02,  1.4079e-02, -2.4403e-03, -2.2919e-02, -1.1820e-02,
            8.6090e-03, -1.9748e-02, -2.7515e-02,  1.2137e-02, -1.1163e-02,
            2.3068e-02,  8.0674e-03,  1.1031e-02, -3.4251e-02, -1.1200e-02,
           -1.1008e-02, -3.1748e-02, -2.8388e-02, -3.3507e-02,  3.0984e-02,
            1.1965e-02,  3.0770e-02, -1.6059e-02,  2.1011e-02,  1.6027e-02,
            1.8576e-02, -7.6561e-03, -2.8241e-02, -2.8612e-02, -1.2619e-02,
            1.8153e-02, -6.2043e-03, -2.7590e-02,  2.5087e-02,  1.4483e-02,
           -4.9296e-04,  2.3715e-02,  3.3282e-02,  3.1226e-03,  3.4027e-02,
            2.2562e-03, -2.0062e-02,  3.2295e-02, -7.0601e-04,  2.1724e-02,
           -5.9986e-03, -2.6718e-02,  3.3617e-02, -1.8944e-02,  1.1394e-02,
            3.4106e-02, -2.4008e-02,  1.8062e-02, -1.1287e-02,  3.3477e-02,
            2.1861e-02,  3.5189e-03, -2.8705e-02,  1.0702e-02,  7.7477e-04,
           -5.5419e-03, -2.2888e-02,  3.0067e-02,  2.2781e-03,  2.3590e-02,
           -5.8626e-03,  2.5113e-02, -1.5222e-02, -1.8122e-02, -4.3590e-03,
           -1.9219e-02,  2.8815e-02,  6.4629e-03,  3.2942e-02, -1.3795e-02,
           -2.4845e-02,  1.7991e-02,  3.3183e-02, -3.0681e-02,  3.5293e-02,
            2.6712e-02,  3.2495e-02, -1.6061e-02,  3.3880e-02, -1.2590e-02,
            2.0821e-02,  8.6573e-03,  2.3449e-02,  5.6342e-04, -2.7438e-02,
            1.8548e-02,  1.5115e-02,  1.6180e-02,  3.3134e-02, -1.4810e-02,
           -4.6599e-04,  7.9041e-03, -1.4063e-02,  6.4533e-03, -4.9172e-03,
           -3.4422e-02,  6.1628e-03,  5.1364e-03, -2.5090e-02, -8.2852e-03,
            2.1598e-02,  2.4567e-02, -2.2181e-02,  2.5296e-03,  1.2265e-02,
           -6.7438e-03,  2.2769e-02,  1.1965e-02, -2.7545e-02, -1.3927e-02,
           -7.6479e-03, -1.7925e-02, -2.3706e-02,  3.2441e-02, -8.4225e-03,
           -7.6180e-03,  1.5281e-03, -1.8754e-02, -8.5401e-04, -1.5822e-02,
           -1.2358e-02, -2.8053e-02, -2.2105e-02, -7.7181e-03, -1.4462e-02,
           -2.2968e-02,  2.7413e-03,  9.6143e-03, -2.5415e-02, -3.3433e-02,
            1.0004e-02,  9.9218e-03, -3.8744e-03,  8.5217e-03, -1.1201e-02,
           -7.2927e-03,  2.1460e-02,  3.3339e-02,  1.7500e-02,  1.5346e-02,
           -7.4143e-03, -1.7420e-02, -1.8679e-02,  1.7502e-02,  1.1291e-02,
           -1.2605e-02, -3.0312e-02, -1.0112e-02, -6.6639e-04,  1.4098e-02,
            1.4953e-02, -2.7830e-02, -3.0763e-02, -7.7763e-03,  5.2739e-04,
            2.0367e-02,  3.3482e-02, -2.9464e-02,  9.7599e-03, -2.7109e-02,
            1.7691e-02,  1.9049e-02,  3.1885e-02, -7.0997e-03, -2.8610e-02,
           -1.8985e-02,  6.4588e-03, -1.1693e-02,  3.0322e-02,  9.3614e-03,
           -3.0264e-02,  2.2693e-02, -1.4488e-02,  1.1494e-02,  4.4544e-03,
           -1.9727e-02, -1.7810e-02,  2.9138e-02, -2.0203e-02, -1.2166e-02,
           -2.7216e-02,  2.3453e-02,  1.2148e-02, -4.7580e-03, -5.7243e-03,
           -1.6306e-02, -3.5028e-02,  6.7793e-03, -1.9719e-02, -8.6863e-03,
            1.6022e-02,  1.3118e-02,  1.1171e-02, -3.2072e-02, -8.2525e-03,
           -2.0898e-02, -3.7723e-03, -2.0502e-02,  3.1384e-02, -2.3011e-02,
            1.5595e-02,  1.6619e-02, -5.1312e-03, -3.3143e-03,  4.6629e-03,
            2.5025e-02, -1.3548e-02,  2.5947e-02, -2.1209e-02,  2.4449e-02,
            2.6143e-02, -3.5043e-02,  2.7640e-02, -5.3060e-03, -2.5713e-04,
           -2.2551e-02, -2.5854e-02,  1.3285e-02,  3.5096e-02, -3.1407e-02,
            4.4703e-04, -2.1378e-02,  2.5649e-02,  1.1278e-02, -2.4332e-02,
           -1.6825e-02, -4.1986e-03, -2.3438e-02, -3.0462e-02,  2.4260e-02,
           -3.4846e-02,  2.3200e-02, -1.7538e-02,  1.6638e-02,  1.5026e-02,
            6.0195e-03, -3.0927e-02,  2.2306e-03,  1.1786e-02,  3.2046e-02,
            1.6135e-02, -1.5331e-02, -2.7791e-02, -5.5735e-03,  2.0570e-02,
           -3.0980e-02, -3.0129e-02,  2.7024e-02, -1.5204e-02, -2.1726e-02,
            1.3639e-02,  2.5088e-02,  1.4993e-02, -2.5441e-02, -1.2538e-02,
           -1.1028e-02, -1.2795e-02, -1.1467e-02,  2.1487e-02,  2.6211e-02,
            1.1089e-02,  2.3402e-02,  3.3138e-02,  2.1660e-02, -3.4835e-02,
           -1.7019e-02, -3.1366e-02, -1.1076e-02, -2.6930e-02,  2.5249e-03,
            6.7098e-03,  3.4577e-02, -1.5383e-04,  2.7353e-02,  7.8649e-03,
            3.1318e-02, -1.9897e-02, -1.4718e-02, -1.4255e-02,  2.9152e-02,
            3.3796e-02, -1.9375e-02,  1.7342e-02, -2.9039e-02,  9.8544e-03,
           -2.4049e-02,  1.8897e-02, -2.7154e-02,  8.5444e-03,  2.1654e-02,
            2.9840e-02,  2.9837e-02,  3.4210e-02,  2.3864e-02, -9.6442e-03,
            2.6882e-03, -1.6822e-03, -2.4079e-02,  2.8954e-02,  4.4580e-04,
           -1.7571e-02, -2.0623e-02, -1.7656e-02, -1.8979e-02, -2.9691e-02,
           -2.1647e-02, -3.5600e-03,  1.3561e-02, -2.4869e-02, -2.5115e-02,
           -3.4352e-03,  1.3114e-02, -2.3098e-03, -3.7569e-03, -1.1818e-05,
            3.4590e-02,  1.3105e-02,  1.8611e-02,  3.1803e-02,  2.4882e-02,
            1.9106e-02,  2.0132e-02, -5.5301e-03,  1.5074e-02, -1.2797e-02,
            2.8694e-02,  1.2517e-02,  2.6179e-02, -1.0652e-02, -2.5975e-02,
           -1.1558e-03, -1.8630e-02,  1.1667e-02,  2.7457e-02,  2.5125e-02,
           -3.1858e-03,  1.9040e-02,  3.0717e-02,  1.9511e-03,  4.3152e-03,
           -2.6358e-02, -2.4120e-02,  2.1430e-02,  1.1173e-02,  1.6387e-02,
            2.0197e-02,  3.4408e-02, -5.4347e-03, -8.3412e-03,  1.9107e-02,
           -2.3547e-02, -2.2750e-02, -2.6454e-02,  2.5723e-02,  2.4557e-02,
            1.9785e-02,  3.2785e-02, -3.3016e-02, -1.3182e-02,  3.1559e-02,
           -1.8370e-02,  1.8809e-02, -1.4540e-02,  1.8669e-02,  2.6686e-02,
            6.9959e-03,  3.7311e-03,  3.2892e-03,  2.3227e-02, -7.7299e-03,
           -1.2714e-02,  1.7342e-02, -1.8623e-02, -2.4024e-02, -1.0248e-02,
           -2.6464e-02,  3.7452e-03,  3.2838e-02, -1.1088e-02,  2.7784e-02,
            2.8139e-02, -3.3656e-02,  6.1302e-03,  2.1331e-02,  2.0221e-02,
            1.1922e-02, -3.0296e-02, -3.2138e-02, -2.0589e-03,  8.5305e-03,
            1.1254e-03, -2.8948e-02,  1.1335e-02,  9.4117e-03, -1.2910e-03,
           -2.7821e-03,  7.7561e-03,  1.8881e-02, -1.4890e-03,  2.5622e-02,
            2.6829e-02, -2.9505e-03,  6.3592e-03,  5.4363e-03,  2.4829e-02,
           -8.3313e-03, -1.2321e-02,  2.0653e-02,  1.1584e-02,  2.9761e-02,
           -4.6434e-04, -1.9841e-02,  5.4529e-03, -3.3396e-02,  1.1948e-02,
           -9.3939e-03, -6.2372e-03, -1.3040e-02,  1.1585e-02,  2.7852e-02,
            1.4024e-03, -1.0054e-02,  1.8365e-02,  1.2866e-02,  4.4631e-03,
           -3.4553e-02, -1.1331e-05,  4.4898e-03, -1.0373e-02,  4.8208e-03,
           -3.4282e-02,  2.7069e-02,  2.2749e-03,  1.6687e-02, -1.6418e-02,
           -2.3010e-02, -1.8886e-02, -2.9605e-02,  3.0419e-03, -4.4463e-03,
            2.2380e-02,  3.3511e-02,  3.1705e-02,  3.8028e-03,  6.1957e-03,
           -3.0211e-02, -2.5378e-02, -1.9431e-02,  1.2212e-02, -7.0565e-03,
            2.7147e-02,  7.2957e-04, -3.5222e-02, -1.8875e-02, -4.5532e-03,
            2.3981e-03,  2.1337e-03, -9.4551e-03, -3.0355e-02,  1.6750e-02,
           -3.2751e-02,  1.2495e-02,  2.0078e-03,  1.3581e-02, -3.0365e-02,
            3.1505e-02,  1.5175e-02, -1.5265e-02, -2.7448e-02, -7.9191e-04,
            1.3857e-03, -3.8856e-03, -3.4655e-02, -2.5873e-02, -3.3930e-03,
            3.1252e-02, -3.4467e-02, -2.5464e-02,  2.1866e-02, -1.4295e-02,
           -2.0498e-02,  9.5111e-03, -8.6361e-04,  9.8030e-03, -2.1925e-02,
            1.9814e-02, -2.9498e-02, -4.7097e-03, -2.1551e-02,  2.6957e-02,
            3.1159e-03, -1.3948e-02,  3.0139e-02, -3.0391e-02, -2.8446e-02,
            2.1720e-02, -2.8358e-02, -8.4915e-03, -2.9049e-02,  3.4500e-02,
           -7.4384e-03, -8.7058e-03, -1.9631e-02,  1.9638e-02,  4.0720e-03,
            2.9302e-02, -2.1443e-02,  4.8605e-03, -1.3759e-02, -3.1110e-03,
            1.5549e-02,  2.1977e-02,  7.9114e-03, -3.4369e-02,  1.2890e-02,
            1.6666e-02,  1.1706e-02, -3.5077e-02, -1.7487e-02,  4.9261e-03,
           -1.6433e-02,  1.3167e-03, -2.1094e-02, -1.8803e-02,  2.5781e-03,
            1.2719e-02, -1.9884e-02, -3.5196e-02,  1.7059e-02,  1.9645e-02,
            1.1716e-02, -2.8093e-03, -1.3819e-03,  3.4158e-02, -3.0283e-04,
            1.1351e-02, -6.5761e-03,  3.3920e-02, -1.9521e-02, -3.1635e-02,
            1.7894e-03,  2.8648e-02, -1.6372e-02, -1.7930e-02, -2.0339e-02,
           -1.2664e-02,  1.0270e-02,  3.3453e-02, -2.1133e-02, -6.4289e-03,
            1.5340e-02,  3.0483e-02, -2.3883e-02,  2.5655e-02, -2.3966e-02,
            2.8463e-02, -2.1448e-02, -2.3649e-02, -2.5792e-02,  3.6304e-03,
           -8.5281e-03, -4.8854e-03,  1.0260e-02,  7.5564e-03,  2.7374e-02,
           -1.6280e-02, -1.8730e-02, -2.1250e-03,  2.8595e-02,  2.3902e-02]],
         device='cuda:0')
#+end_example

** Inputs and labels

#+begin_src ipython
  model.N_BATCH = 32
  model.I0[0] = 1
  model.I0[1] = 1 

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = -1 

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = -1 
  
  BD_pair = model.init_ff_input()
  
  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print(ff_input.shape)
#+end_src

#+RESULTS:
: torch.Size([128, 675, 1000])

#+begin_src ipython
  labels_pair = torch.zeros(2 * model.N_BATCH)
  labels_unpair = torch.ones(2 * model.N_BATCH)
  
  labels = torch.cat((labels_pair, labels_unpair))
  print(ff_input.shape, labels.shape)
#+end_src

#+RESULTS:
: torch.Size([128, 675, 1000]) torch.Size([128])

** Train

#+begin_src ipython
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  batch_size = 32
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)

  learning_rate = 0.005
  # criterion = CosineLoss()
  criterion = nn.BCELoss()
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
  # optimizer = optim.SGD(model.parameters(), lr=learning_rate)
  
  num_epochs = 100
  run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+End_src

#+RESULTS:
#+begin_example
  Epoch 1/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 2/100, Training Loss: 66.6667, Validation Loss: 0.0000
  Epoch 3/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 4/100, Training Loss: 66.6667, Validation Loss: 0.0000
  Epoch 5/100, Training Loss: 33.4737, Validation Loss: 0.0000
  Epoch 6/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 7/100, Training Loss: 66.6667, Validation Loss: 0.0000
  Epoch 8/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 9/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 10/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 11/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.
  Epoch 12/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 13/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 14/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 15/100, Training Loss: 0.0000, Validation Loss: 0.0000
  Epoch 16/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 17/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 18/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 19/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 20/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 21/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 22/100, Training Loss: 0.0000, Validation Loss: 0.0000
  Epoch 00023: reducing learning rate of group 0 to 5.0000e-05.
  Epoch 23/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 24/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 25/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 26/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 27/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 28/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 29/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 30/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 31/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 32/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 33/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 00034: reducing learning rate of group 0 to 5.0000e-06.
  Epoch 34/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 35/100, Training Loss: 66.6667, Validation Loss: 0.0000
  Epoch 36/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 37/100, Training Loss: 66.6667, Validation Loss: 0.0000
  Epoch 38/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 39/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 40/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 41/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 42/100, Training Loss: 0.0000, Validation Loss: 0.0000
  Epoch 43/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 44/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 00045: reducing learning rate of group 0 to 5.0000e-07.
  Epoch 45/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 46/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 47/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 48/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 49/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 50/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 51/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 52/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 53/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 54/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 55/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 00056: reducing learning rate of group 0 to 5.0000e-08.
  Epoch 56/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 57/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 58/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 59/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 60/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 61/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 62/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 63/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 64/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 65/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 66/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 00067: reducing learning rate of group 0 to 5.0000e-09.
  Epoch 67/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 68/100, Training Loss: 0.0000, Validation Loss: 0.0000
  Epoch 69/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 70/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 71/100, Training Loss: 66.6667, Validation Loss: 0.0000
  Epoch 72/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 73/100, Training Loss: 66.6667, Validation Loss: 0.0000
  Epoch 74/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 75/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 76/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 77/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 78/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 79/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 80/100, Training Loss: 0.0000, Validation Loss: 0.0000
  Epoch 81/100, Training Loss: 66.6667, Validation Loss: 0.0000
  Epoch 82/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 83/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 84/100, Training Loss: 66.6667, Validation Loss: 0.0000
  Epoch 85/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 86/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 87/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 88/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 89/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 90/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 91/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 92/100, Training Loss: 0.0000, Validation Loss: 0.0000
  Epoch 93/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 94/100, Training Loss: 50.0000, Validation Loss: 0.0000
  Epoch 95/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 96/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 97/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 98/100, Training Loss: 16.6667, Validation Loss: 0.0000
  Epoch 99/100, Training Loss: 33.3333, Validation Loss: 0.0000
  Epoch 100/100, Training Loss: 0.0000, Validation Loss: 0.0000
#+end_example
#+RESULTS:

** Result

#+begin_src ipython
  ksi = model.U.T
  print(ksi.shape)
  cosine = torch.arccos(nn.CosineSimilarity(dim=0)(ksi[0], ksi[1])) * 180 / torch.pi
  print(cosine)
  var = torch.var(ksi, axis=-1)
  print(var)
#+end_src

#+RESULTS:
: torch.Size([2, 1000])
: tensor(85.4398, device='cuda:0', grad_fn=<DivBackward0>)
: tensor([0.9819, 0.9490], device='cuda:0', grad_fn=<VarBackward0>)

#+begin_src ipython
  lr = model.mask * (1.0 + model.U @ model.U.T / torch.sqrt(model.Ka[0]))
  weights = model.Wab.T * lr
  weights = weights.cpu().detach().numpy()
#+end_src

#+RESULTS:

#+begin_src ipython  
#  plot_con(weights)
#+end_src

#+RESULTS:

#+begin_src ipython
  readout = model.linear.weight.data[0]
  print(readout.shape)
#+end_src

#+RESULTS:
: torch.Size([800])

#+begin_src ipython
  read0 = nn.CosineSimilarity(dim=0)(model.U[:model.Na[0],0], readout).cpu().detach().numpy()
  read1 = nn.CosineSimilarity(dim=0)(model.U[:model.Na[0],1], readout).cpu().detach().numpy()

  print(np.arccos(read0)*180/np.pi, np.arccos(read1)*180/np.pi)
#+end_src

#+RESULTS:
: 100.18552210425968 105.1274371763307

** Eval

#+begin_src ipython
  model.eval()
  model.N_BATCH = 1
  model.VERBOSE=1
  model.LR_TRAIN=0
  print(model.ff_input.shape)
  print(ff_input.shape)
#+end_src

#+RESULTS:
: torch.Size([128, 675, 1000])
: torch.Size([128, 675, 1000])

#+begin_src ipython
  rates = model.forward(model.ff_input, REC_LAST_ONLY=0).cpu().detach().numpy()
#+end_src

#+RESULTS:
#+begin_example
  times (s) 0.0 rates (Hz) [0.0, 1.1]
  times (s) 0.22 rates (Hz) [0.0, 1.09]
  times (s) 0.44 rates (Hz) [0.0, 1.1]
  times (s) 0.67 rates (Hz) [0.0, 1.11]
  times (s) 0.89 rates (Hz) [11.75, 1.11]
  times (s) 1.11 rates (Hz) [2.93, 4.88]
  times (s) 1.33 rates (Hz) [2.65, 4.58]
  times (s) 1.56 rates (Hz) [2.51, 4.44]
  times (s) 1.78 rates (Hz) [1.0, 4.44]
  times (s) 2.0 rates (Hz) [1.23, 2.48]
  times (s) 2.22 rates (Hz) [1.42, 2.67]
  times (s) 2.44 rates (Hz) [1.43, 2.73]
  times (s) 2.67 rates (Hz) [1.45, 2.74]
  times (s) 2.89 rates (Hz) [1.44, 2.66]
  times (s) 3.11 rates (Hz) [4.74, 2.65]
  times (s) 3.33 rates (Hz) [3.33, 5.08]
  times (s) 3.56 rates (Hz) [3.07, 4.94]
  times (s) 3.78 rates (Hz) [3.05, 4.92]
  times (s) 4.0 rates (Hz) [1.38, 5.02]
  times (s) 4.22 rates (Hz) [1.27, 2.61]
  times (s) 4.44 rates (Hz) [1.35, 2.62]
  times (s) 4.67 rates (Hz) [1.41, 2.63]
  times (s) 4.89 rates (Hz) [1.42, 2.67]
  times (s) 5.11 rates (Hz) [1.45, 2.72]
  times (s) 5.33 rates (Hz) [1.52, 2.77]
  Elapsed (with compilation) = 0.09711435809731483s
#+end_example

#+begin_src ipython
  print(rates.shape)
  plt.imshow(rates[:, 0].T, aspect='auto', cmap='jet', vmin=0, vmax=2)
  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: (128, 25, 800)
[[file:./.ob-jupyter/7946189ec65b72477d1e6e15135735ea401d2756.png]]
:END:

#+begin_src ipython
  idx = get_idx(model)

  print(idx.shape)
  print(rates.shape)
  
  ordered = rates[..., idx]
  print(ordered.shape)
#+end_src

#+RESULTS:
: (2, 1000)
: (800,)
: (128, 25, 800)
: (128, 25, 800)

#+begin_src ipython
  plt.imshow(ordered[:, -1].T, aspect='auto', cmap='jet', vmin=0, vmax=2.0)
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/bd010f9632437997c29f88e3c5b6d8df80fb9452.png]]

#+begin_src ipython
  y_pred = model.linear.weight.data.cpu().detach().numpy()[0]
  print(y_pred.shape)
  
  overlap = (rates @ y_pred) / rates.shape[-1]
  print(overlap.shape)
  plt.plot(overlap)
  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: (800,)
: (128, 25)
[[file:./.ob-jupyter/7875e64fb58b81bd35a3c55b2c3172f6bc210114.png]]
:END:


