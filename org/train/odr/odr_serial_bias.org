#+STARTUP: fold
#+TITLE: Serial Biases in the ODR
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session odr_sb :kernel torch :exports results :output-dir ./figures/odr :file (lc/org-babel-tangle-figure-filename)

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run ../notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'

  REPO_ROOT = "/home/leon/models/NeuroFlame"
  pal = sns.color_palette("tab10")
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.optim as optim
  import torch.nn.functional as F
  from torch.utils.data import Dataset, TensorDataset, DataLoader
#+end_src

#+RESULTS:

#+RESULTS:

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  import pandas as pd
  import torch.nn as nn
  from time import perf_counter
  from scipy.stats import circmean

  from src.network import Network
  from src.plot_utils import plot_con
  from src.decode import decode_bump, circcvl, decode_bump_torch
  from src.lr_utils import masked_normalize, clamp_tensor, normalize_tensor
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  import pickle as pkl
  import os

  def pkl_save(obj, name, path="."):
      os.makedirs(path, exist_ok=True)
      destination = path + "/" + name + ".pkl"
      print("saving to", destination)
      pkl.dump(obj, open(destination, "wb"))


  def pkl_load(name, path="."):
      source = path + "/" + name + '.pkl'
      print('loading from', source)
      return pkl.load(open( source, "rb"))

#+end_src

#+RESULTS:

* Helpers

#+begin_src ipython
def get_fourier_moments(signal, axis=-1):
    # Perform the FFT
    fft_coeffs = np.fft.fft(signal, axis=axis)

    # Calculate the zero, first, and second Fourier moments
    zero_moment = fft_coeffs[..., 0]
    first_moment = fft_coeffs[..., 1]

    # Calculate magnitude m0, m1, and m2
    m0 = np.abs(zero_moment) / signal.shape[axis]  # Normalize m0 by the signal length
    m1 = 2.0 * np.abs(first_moment) / signal.shape[axis]

    # Calculate the phase of the signal
    phases = np.angle(first_moment) % (2.0 * torch.pi)

    return m0, m1, phases
#+end_src

#+RESULTS:

#+begin_src ipython
def add_vlines(model, ax=None):

    if ax is None:
        for i in range(len(model.T_STIM_ON)):
            plt.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)
    else:
        for i in range(len(model.T_STIM_ON)):
            ax.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)

#+end_src

#+RESULTS:

* Bias targets

#+begin_src ipython
import torch
import numpy as np

def generate_weighted_phase_samples(N_BATCH, angles, preferred_angle, sigma):
    # Convert angles list to a tensor
    angles_tensor = torch.tensor(angles)

    # Calculate Gaussian probability distribution centered at preferred_angle
    probs = np.exp(-0.5 * ((angles - preferred_angle) / sigma) ** 2)
    probs /= probs.sum()  # Normalize to get probabilities

    # Create a categorical distribution from the computed probabilities
    distribution = torch.distributions.Categorical(torch.tensor(probs))

    # Sample from the distribution
    indices = distribution.sample((N_BATCH,))

    # Map indices to angles and reshape to (N_BATCH, 1)
    phase_samples = angles_tensor[indices].reshape(N_BATCH, 1)

    return phase_samples

# Parameters
N_BATCH = 10000
angles = np.array([0., 45., 90., 135., 180., 225., 270., 315.])
reference = 90.  # Example preferred angle
sigma = 75.  # Standard deviation of the Gaussian

# Generate samples
phase_samples = generate_weighted_phase_samples(N_BATCH, angles, reference, sigma)
print(phase_samples.shape)
plt.hist(phase_samples[:,0], bins=8);
#+end_src

#+RESULTS:
:RESULTS:
: torch.Size([10000, 1])
[[./figures/odr/figure_6.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

* Model

#+begin_src ipython
REPO_ROOT = "/home/leon/models/NeuroFlame"
conf_name = "test_odr_EI.yml"
DEVICE = 'cuda:1'
seed = np.random.randint(0, 1e6)
print(seed)

seed = 181678
seed = 975532

#+end_src

#+RESULTS:
: 269396

#+begin_src ipython
model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE=DEVICE, SEED=seed, N_BATCH=1, LIVE_FF_UPDATE=1)
#+end_src

#+RESULTS:

#+begin_src ipython
model_state_dict = torch.load('models/odr_%d.pth' % seed)
# model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE=DEVICE, SEED=seed, N_BATCH=4)
model.load_state_dict(model_state_dict)
model.eval()  # Set to evaluation mode
#+end_src

#+RESULTS:
: Network(
:   (dropout): Dropout(p=0.0, inplace=False)
: )

#+begin_src ipython
print(model.J_STP)
print(model.N_BATCH)
print(model.RANDOM_DELAY)
#+end_src

#+RESULTS:
: Parameter containing:
: tensor(41.7043, device='cuda:1', requires_grad=True)
: 1
: 0

* Batching Live Inputs

#+begin_src ipython
  N_PHASE = 8
  N_BATCH = 160 * N_PHASE

  PHI0 = model.PHI0.unsqueeze(-1).repeat((N_BATCH, 1, 1))

  # phases =  [  0.,  45.,  90., 135., 180., 225., 270., 315.]
  # phases_tensor = torch.tensor(phases)
  # PHI0[:, 0] = phases_tensor[torch.randint(0, len(phases), (N_BATCH,))].unsqueeze(1)
  # PHI0[:, -1] = phases_tensor[torch.randint(0, len(phases), (N_BATCH,))].unsqueeze(1)
  # print(PHI0.shape)

  PHI0[:, 0] = torch.randint(0, 360, (N_BATCH, 1)).to(DEVICE)
  PHI0[:, -1] = torch.randint(0, 360, (N_BATCH, 1)).to(DEVICE)

  # PHI0[:, 0] = generate_weighted_phase_samples(N_BATCH, angles, reference, sigma)
  # PHI0[:, 1] = generate_weighted_phase_samples(N_BATCH, angles, reference, sigma)
  # print(PHI0.shape)
 #+end_src

#+RESULTS:
: torch.Size([1, 3])

#+begin_src ipython
fig, ax = plt.subplots(1, 2, figsize=[2*width, height])
ax[0].hist(PHI0[:, 0, 0].cpu(), bins=15)
ax[0].set_xlabel('Prev. Stim')
ax[1].hist(PHI0[:,-1, 0].cpu(), bins=15)
ax[1].set_xlabel('Curr. Stim')
plt.show()
#+end_src

#+RESULTS:
[[./figures/odr/figure_13.png]]

#+begin_src ipython
  model.PHI0 = PHI0
  model.N_BATCH = N_PHASE
  rates = model().cpu().detach().numpy()
  print(rates.shape)
#+end_src

#+RESULTS:
: (1280, 81, 750)

#+begin_src ipython
  #  m0, m1, phi = decode_bump(rates, axis=-1)
  m0, m1, phi = get_fourier_moments(rates, axis=-1)
  print(phi.shape)
#+end_src

#+RESULTS:
: (1280, 81)

* Batching Inputs Sequentially

#+begin_src ipython
    model.N_BATCH = N_BATCH
    labels = torch.randint(0, 360, (N_BATCH, 1)).to(DEVICE)
    model.PHI0 = torch.ones((N_BATCH, 2, 1), device=DEVICE, dtype=torch.float)
    model.PHI0[:, 0] = labels

    ff_input = model.init_ff_input()
    print(model.PHI0.shape, ff_input.shape, labels.shape)
#+end_src

#+begin_src ipython
# model.N_BATCH = 96
# ff_input = []
# labels = []

# phase_list =  torch.tensor([  0.,  45.,  90., 135., 180., 225., 270., 315.], device=DEVICE)

# model.PHI0 = torch.ones((model.N_BATCH, 3, 1), device=DEVICE, dtype=torch.float
#                         )

# for i in range(len(phase_list)):
#     model.PHI0[:, 0] = phase_list[i]
#     model.PHI0[:, -1] = phase_list[torch.randint(0, len(phase_list), (model.N_BATCH,))].unsqueeze(1)

#     label0 = torch.ones(model.N_BATCH, device=DEVICE, dtype=torch.float) * model.PHI0[:, 0, 0] * torch.pi / 180.0
#     label1 = torch.ones(model.N_BATCH, device=DEVICE, dtype=torch.float) * model.PHI0[:, -1, 0] * torch.pi / 180.0

#     labels.append(torch.vstack((label0, label1)))
#     ff_input.append(model.init_ff_input())

# labels = torch.hstack(labels).T
# ff_input = torch.vstack(ff_input)
# print('ff_input', ff_input.shape, 'labels', labels.shape)
# PHI0 = labels.unsqueeze(-1)
#+end_src

#+RESULTS:

#+begin_src ipython
# fig, ax = plt.subplots(1, 2, figsize=[2*width, height])
# ax[0].hist(PHI0[:, 0, 0].cpu(), bins=15)
# ax[1].hist(PHI0[:,-1, 0].cpu(), bins=15)
# plt.show()
 #+end_src

#+RESULTS:

#+begin_src ipython
# rates = model.forward(ff_input=ff_input).cpu().detach().numpy()
# print('ff_input', ff_input.shape)
# print('rates', rates.shape)
#+end_src

#+RESULTS:

#+begin_src ipython
  # m0, m1, phi = decode_bump(rates, axis=-1)
  # print(phi.shape)
#+end_src

#+RESULTS:

* Results
** Rates

#+begin_src ipython
fig, ax = plt.subplots(1, 3, figsize=[2.5*width, height])

idx = np.random.randint(0, model.N_BATCH)
ax[0].imshow(rates[idx].T, aspect='auto', cmap='jet', vmin=0, vmax=2, origin='lower', extent=[0, model.DURATION, 0, model.Na[0].cpu()])
ax[0].set_ylabel('Pref. Location (°)')
ax[0].set_yticks(np.linspace(0, model.Na[0].cpu(), 5), np.linspace(0, 360, 5).astype(int))
ax[0].set_xlabel('Time (s)')

xtime = np.linspace(0, model.DURATION, phi.shape[-1])
idx = np.random.randint(0, model.N_BATCH, 8)
ax[1].plot(xtime, m1[idx].T)
ax[1].set_ylabel('m1 (Hz)')
ax[1].set_xlabel('Time (s)')
add_vlines(model, ax[1])

ax[2].plot(xtime, phi[idx].T * 180 / np.pi, alpha=0.5)
ax[2].set_yticks(np.linspace(0, 360, 5).astype(int), np.linspace(0, 360, 5).astype(int))
ax[2].set_ylabel('Bump Center (°)')
ax[2].set_xlabel('Time (s)')
add_vlines(model, ax[2])
plt.show()
#+end_src

#+RESULTS:
[[./figures/odr/figure_20.png]]

#+begin_src ipython
PHI0 = PHI0.cpu().detach().numpy()
print(PHI0.shape)
#+end_src

#+RESULTS:
: (1280, 3, 1)

** errors

#+begin_src ipython
target_loc = PHI0[:, -1]

rel_loc = (PHI0[:, 0] - target_loc) * np.pi / 180.0
rel_loc = (rel_loc + np.pi) % (2 * np.pi) - np.pi
rel_loc *= 180 / np.pi

ref_loc = (PHI0[:, 0] - reference) * np.pi / 180.0
ref_loc = (ref_loc + np.pi) % (2 * np.pi) - np.pi
ref_loc *= 180 / np.pi

errors = (phi - target_loc * np.pi / 180.0)
errors = (errors + np.pi) % (2 * np.pi) - np.pi
errors *= 180 / np.pi

errors2 = (phi - PHI0[:, 0] * np.pi / 180.0)
errors2 = (errors2 + np.pi) % (2 * np.pi) - np.pi
errors2 *= 180 / np.pi

print(errors.shape, target_loc.shape, rel_loc.shape, ref_loc.shape)
#+end_src

#+RESULTS:
: (1280, 81) (1280, 1) (1280, 1) (1280, 1)

#+begin_src ipython
fig, ax = plt.subplots(1, 2, figsize=[2*width, height])
ax[0].plot(np.linspace(0, model.DURATION, errors.shape[-1]), errors2[:32].T)
add_vlines(model, ax[0])
# ax[0].set_xlim([2.5, 4.5])
ax[0].set_xlabel('t')
ax[0].set_ylabel('prev. error (°)')

ax[1].plot(np.linspace(0, model.DURATION, errors.shape[-1]), errors[:32].T)
add_vlines(model, ax[1])
ax[1].set_xlabel('t')
ax[1].set_ylabel('curr. error (°)')
plt.show()
#+end_src

#+RESULTS:
[[./figures/odr/figure_23.png]]

#+begin_src ipython
fig, ax = plt.subplots(1, 3, figsize=[2.75*width, height])
ax[0].hist(rel_loc[:, 0], bins='auto')
ax[0].set_xlabel('Rel. Location (°)')

ax[1].hist(errors2[:, int((model.N_STIM_ON[1]-model.N_STEADY)/model.N_WINDOW)], bins='auto')
ax[1].set_xlabel('Prev. Errors (°)')

ax[2].hist(errors[:, -1], bins=64)
ax[2].set_xlabel('Curr. Errors (°)')
# ax[1].set_xlim([-45, 45])
plt.show()
#+end_src

#+RESULTS:
[[./figures/odr/figure_24.png]]

#+begin_src ipython
mask = np.abs(errors) <= 35
print(mask.shape)

errors = np.where(mask, errors, np.nan)[:, -1]
print(errors.shape)
rel_loc = rel_loc[~np.isnan(errors)]
ref_loc = ref_loc[~np.isnan(errors)]
target_loc = target_loc[:, -1][~np.isnan(errors), np.newaxis]
errors = errors[~np.isnan(errors), np.newaxis]
# errors = errors[mask]
print(errors.shape, target_loc.shape, rel_loc.shape, ref_loc.shape)
#+end_src

#+RESULTS:
: (1280, 81)
: (1280,)
: (269, 1) (269, 1) (269, 1) (269, 1)

#+begin_src ipython
fig, ax = plt.subplots(1, 3, figsize=[2.75*width, height])
ax[0].hist(rel_loc[:, 0], bins='auto')
ax[0].set_xlabel('Rel. Location (°)')

ax[1].hist(errors2[:, int((model.N_STIM_ON[1]-model.N_STEADY)/model.N_WINDOW)], bins='auto')
ax[1].set_xlabel('Prev. Errors (°)')

ax[2].hist(errors[:, -1], bins='auto')
ax[2].set_xlabel('Curr. Errors (°)')
# ax[1].set_xlim([-45, 45])
plt.show()
#+end_src

#+RESULTS:
[[./figures/odr/figure_26.png]]

** biases

#+begin_src ipython
fig, ax = plt.subplots(1, 3, figsize=[2.5*width, height])

ax[0].plot(target_loc[:, 0], errors[:,-1], 'o')
ax[0].set_xlabel('Target Loc. (°)')
ax[0].set_ylabel('Error (°)')

from scipy.stats import binned_statistic
stt = binned_statistic(target_loc[:,0], errors[:,-1], statistic='mean', bins=3, range=[0, 360])
dstt = np.mean(np.diff(stt.bin_edges))
ax[0].plot(stt.bin_edges[:-1]+dstt/2,stt.statistic,'r')

ax[0].axhline(color='k', linestyle=":")

ax[1].plot(rel_loc[:, 0], errors[:,-1], 'bo')
# ax[1].plot(rel_loc2[:, 0], errors2[:,-1], 'ro')
ax[1].set_xlabel('Rel. Loc. (°)')
ax[1].set_ylabel('Error (°)')

stt = binned_statistic(rel_loc[:, 0], errors[:, -1], statistic='mean', bins=6, range=[-180, 180])
dstt = np.mean(np.diff(stt.bin_edges))
ax[1].plot(stt.bin_edges[:-1]+dstt/2, stt.statistic, 'b')

ax[2].plot(ref_loc[:, 0], errors[:,-1], 'bo')
# ax[2].plot(ref_loc2[:, 0], errors2[:,-1], 'ro')
ax[2].set_xlabel('Ref. Loc. (°)')
ax[2].set_ylabel('Error (°)')

stt = binned_statistic(ref_loc[:, 0], errors[:, -1], statistic='mean', bins=6, range=[-180, 180])
dstt = np.mean(np.diff(stt.bin_edges))
ax[2].plot(stt.bin_edges[:-1]+dstt/2, stt.statistic, 'b')

# stt = binned_statistic(rel_loc2[:, 0], errors2[:, -1], statistic='mean', bins=6, range=[-180, 180])
# dstt = np.mean(np.diff(stt.bin_edges))
# ax[1].plot(stt.bin_edges[:-1]+dstt/2,stt.statistic, 'r')
# ax[1].set_ylim([-120, 120])
# ax[1].axhline(color='k', linestyle=":")

plt.show()
#+end_src

#+RESULTS:
[[./figures/odr/figure_27.png]]

#+begin_src ipython
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Assuming rel_loc and errors are defined earlier
data = pd.DataFrame({'rel_loc': rel_loc[:, 0], 'errors': errors[:, -1], 'ref_loc': ref_loc[:, -1]})

# Bin data using pandas cut
data['bin'] = pd.cut(data['rel_loc'], bins=8)
# Calculate mean and standard error for each bin
binned_data = data.groupby('bin')['errors'].agg(['mean', 'sem'])
# Get bin centers
bin_edges = binned_data.index.get_level_values(0)
bin_centers = (bin_edges.categories.left + bin_edges.categories.right) / 2

# Plot
fig, ax = plt.subplots(1, 2, figsize=[2*width, height])
ax[0].plot(bin_centers, binned_data['mean'], 'b')
ax[0].fill_between(bin_centers,
                binned_data['mean'] - binned_data['sem'],
                binned_data['mean'] + binned_data['sem'],
                color='b', alpha=0.2)
# ax[0].set_ylim([-30, 30])
ax[0].axhline(color='k', linestyle=":")
ax[0].set_xlabel('Rel. Loc. (°)')
ax[0].set_ylabel('Curr. Error (°)')

data['bin'] = pd.cut(data['ref_loc'], bins=8)
# Calculate mean and standard error for each bin
binned_data = data.groupby('bin')['errors'].agg(['mean', 'sem'])
# Get bin centers
bin_edges = binned_data.index.get_level_values(0)
bin_centers = (bin_edges.categories.left + bin_edges.categories.right) / 2

ax[1].plot(bin_centers, binned_data['mean'], 'b')
ax[1].fill_between(bin_centers,
                binned_data['mean'] - binned_data['sem'],
                binned_data['mean'] + binned_data['sem'],
                color='b', alpha=0.2)
# ax[1].set_ylim([-30, 30])
ax[1].axhline(color='k', linestyle=":")
ax[1].set_xlabel('Ref. Loc. (°)')
ax[1].set_ylabel('Curr. Error (°)')

plt.show()
#+end_src

#+RESULTS:
[[./figures/odr/figure_28.png]]

#+begin_src ipython

#+end_src

#+RESULTS:
