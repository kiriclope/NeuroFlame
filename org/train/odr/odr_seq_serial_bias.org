#+STARTUP: fold
#+TITLE: ODR serial bias
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session odr_sb :kernel torch :exports results :output-dir ./figures/odr_sb :file (lc/org-babel-tangle-figure-filename)

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run ../../../notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'

  REPO_ROOT = "/home/leon/models/NeuroFlame"
  pal = sns.color_palette("tab10")
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.optim as optim
  import torch.nn.functional as F
  from torch.utils.data import Dataset, TensorDataset, DataLoader
  from scipy.stats import binned_statistic
#+end_src

#+RESULTS:

#+begin_src ipython
  import sys
  sys.path.insert(0, '../../../')

  import pandas as pd
  import torch.nn as nn
  from time import perf_counter
  from scipy.stats import circmean

  from src.network import Network
  from src.plot_utils import plot_con
  from src.decode import decode_bump, circcvl, decode_bump_torch
  from src.lr_utils import masked_normalize, clamp_tensor, normalize_tensor
  from src.utils import clear_cache
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  import pickle as pkl
  import os

  def pkl_save(obj, name, path="."):
      os.makedirs(path, exist_ok=True)
      destination = path + "/" + name + ".pkl"
      print("saving to", destination)
      pkl.dump(obj, open(destination, "wb"))


  def pkl_load(name, path="."):
      source = path + "/" + name + '.pkl'
      print('loading from', source)
      return pkl.load(open( source, "rb"))

#+end_src

#+RESULTS:

* Helpers

#+begin_src ipython
def map2center(angles):
    """Map angles from [0, 2π] to [-π, π] using PyTorch tensors."""
    return np.where(angles > np.pi, angles - 2 * np.pi, angles)

def map2pos(angles):
    """Map angles from [-π, π] to [0, 2π] using PyTorch tensors."""
    return np.where(angles < 0, angles + 2 * np.pi, angles)
#+end_src

#+RESULTS:

#+begin_src ipython
def maptens2center(angles):
    """Map angles from [0, 2π] to [-π, π] using PyTorch tensors."""
    return torch.where(angles > torch.pi, angles - 2 * torch.pi, angles)

def maptens2pos(angles):
    """Map angles from [-π, π] to [0, 2π] using PyTorch tensors."""
    return torch.where(angles < 0, angles + 2 * torch.pi, angles)
#+end_src

#+RESULTS:

#+begin_src ipython
def add_vlines(model, ax=None):

    if ax is None:
        for i in range(len(model.T_STIM_ON)):
            plt.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)
    else:
        for i in range(len(model.T_STIM_ON)):
            ax.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)

#+end_src

#+RESULTS:


#+begin_src ipython
import torch
import numpy as np

def generate_weighted_phase_samples(N_BATCH, angles, preferred_angle, sigma):
    # Convert angles list to a tensor
    angles_tensor = torch.tensor(angles)

    # Calculate Gaussian probability distribution centered at preferred_angle
    probs = np.exp(-0.5 * ((angles - preferred_angle) / sigma) ** 2)
    probs /= probs.sum()  # Normalize to get probabilities

    # Create a categorical distribution from the computed probabilities
    distribution = torch.distributions.Categorical(torch.tensor(probs))

    # Sample from the distribution
    indices = distribution.sample((N_BATCH,))

    # Map indices to angles and reshape to (N_BATCH, 1)
    phase_samples = angles_tensor[indices].reshape(N_BATCH, 1)

    return phase_samples
#+end_src

#+RESULTS:

#+begin_src ipython
import torch
import numpy as np
import matplotlib.pyplot as plt

def continuous_biased_phases(N_BATCH, preferred_angle, sigma):
    # Generate samples from a normal distribution using PyTorch
    phase_samples = torch.normal(mean=preferred_angle, std=sigma, size=(N_BATCH, 1))

    # Normalize angles to the range [0, 360)
    phase_samples = phase_samples % 360

    return phase_samples
    #+end_src

    #+RESULTS:

#+begin_src ipython
import torch
import numpy as np
import matplotlib.pyplot as plt

def continuous_bimodal_phases(N_BATCH, preferred_angle, sigma):
    # Sample half from preferred_angle and half from preferred_angle + 180
    half_batch = N_BATCH // 2

    # Sample from preferred_angle
    samples_1 = torch.normal(mean=preferred_angle, std=sigma, size=(half_batch, 1))

    # Sample from preferred_angle + 180
    samples_2 = torch.normal(mean=(preferred_angle + 180) % 360, std=sigma, size=(N_BATCH - half_batch, 1))

    # Combine samples and wrap around 360
    phase_samples = torch.cat((samples_1, samples_2), dim=0) % 360

    return phase_samples

# Example usage
# N_BATCH = 500
# preferred_angle = 45
# sigma = 45

# samples = continuous_bimodal_phases(N_BATCH, preferred_angle, sigma)

# plt.hist(samples.numpy(), bins='auto', density=True)
# plt.xlabel('Phase (degrees)')
# plt.ylabel('Probability Density')
# plt.title('Bimodal Distribution of Phases')
# plt.show()
#+end_src

#+RESULTS:

* Model

#+begin_src ipython
kwargs = {
    'GAIN': 1.0,
    'DURATION': 12.0,
    'T_STEADY': 2,
    'T_STIM_ON': [1.0, 5.0, 7.0, 11.0],
    'T_STIM_OFF': [2.0, 6.0, 8.0, 12.0],
    'I0': [1.0, -5.0, 1.0, -5.0],
    'PHI0': [180.0, 180, 180, 180],
    'SIGMA0': [1.0, 0.0, 1.0, 0.0],
    'RANDOM_DELAY': 0,
    'MIN_DELAY': 2,
    'MAX_DELAY': 5,
    'IF_ADAPT': 1,
    'A_ADAPT': 0.15,
    'TAU_ADAPT': [25.0, 25.0],
}
#+end_src

#+RESULTS:

#+begin_src ipython
REPO_ROOT = "/home/leon/models/NeuroFlame"
conf_name = "train_odr_EI.yml"
DEVICE = 'cuda:1'
seed = np.random.randint(0, 1e6)

seed = 1
print('seed', seed)
#+end_src

#+RESULTS:
: seed 1

#+begin_src ipython
N_BATCH = 128
model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE=DEVICE, SEED=seed, N_BATCH=N_BATCH, **kwargs)
#+end_src

#+RESULTS:

#+begin_src ipython
model_state_dict = torch.load('../models/odr/odr_%d.pth' % seed)
model.load_state_dict(model_state_dict);
model.eval();
#+end_src

#+RESULTS:

* Batching Inputs

#+begin_src ipython
model.N_BATCH = N_BATCH

model.PHI0 = torch.randint(low=0, high=360, size=(N_BATCH, len(model.I0), 1), device=DEVICE, dtype=torch.float)
ff_input = model.init_ff_input()
rates_tensor = model.forward(ff_input=ff_input)
del ff_input
clear_cache()
#+end_src

#+RESULTS:
: Initializing network

#+begin_src ipython
num_epochs = 50

rates_list = [rates_tensor.cpu().detach()]
phi0_list = [model.PHI0.cpu().detach()]
thresh_list = [model.thresh.cpu().detach()]

for epoch in range(num_epochs):
    print('epoch', epoch)
    with torch.no_grad():
        model.PHI0 = torch.randint(low=0, high=360, size=(N_BATCH, len(model.I0), 1), device=DEVICE, dtype=torch.float)

        ff_input = model.init_ff_input()
        rates = model.forward(ff_input=ff_input, IF_INIT=0)

        phi0_list.append(model.PHI0.cpu().detach())
        rates_list.append(rates.cpu().detach())
        thresh_list.append(model.thresh.cpu().detach())

        del ff_input, model.PHI0
        clear_cache()


rates_list = np.stack(rates_list)
thresh_list = np.stack(thresh_list)
phi0_list = np.stack(phi0_list)

print(rates_list.shape, thresh_list.shape, phi0_list.shape)
#+end_src

#+RESULTS:
#+begin_example
epoch 0
epoch 1
epoch 2
epoch 3
epoch 4
epoch 5
epoch 6
epoch 7
epoch 8
epoch 9
epoch 10
epoch 11
epoch 12
epoch 13
epoch 14
epoch 15
epoch 16
epoch 17
epoch 18
epoch 19
epoch 20
epoch 21
epoch 22
epoch 23
epoch 24
epoch 25
epoch 26
epoch 27
epoch 28
epoch 29
epoch 30
epoch 31
epoch 32
epoch 33
epoch 34
epoch 35
epoch 36
epoch 37
epoch 38
epoch 39
epoch 40
epoch 41
epoch 42
epoch 43
epoch 44
epoch 45
epoch 46
epoch 47
epoch 48
epoch 49
(51, 128, 121, 750) (51, 128, 1000) (51, 128, 4, 1)
#+end_example

#+begin_src ipython
plt.imshow(thresh_list[:,0].T, aspect='auto', cmap='jet', vmin=0, vmax=2)
plt.xlabel('Trial Pair')
plt.ylabel('Neuron #')
plt.show()
#+end_src

#+RESULTS:
[[./figures/odr_sb/figure_16.png]]

#+begin_src ipython
fig, ax = plt.subplots(1, 2, figsize=[2*width, height])
ax[0].scatter(rates_list[0, 0, -1], rates_list[-1, 0, -1])
ax[1].scatter(thresh_list[0, 0], thresh_list[-1, 0])

plt.show()
#+end_src

#+RESULTS:
[[./figures/odr_sb/figure_17.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Errors

#+begin_src ipython
idx = 1
phi0_ini =  phi0_list[idx] * 180.0 / np.pi
m0, m1, phi_ini = decode_bump(rates_list[idx], axis=-1)

idx = -1
phi0_last =  phi0_list[idx] * 180.0 / np.pi
m0, m1, phi_last = decode_bump(rates_list[idx], axis=-1)
#+end_src

#+RESULTS:

#+begin_src ipython
def get_rel_tar_error(phi, PHI0):
    target_loc = PHI0[:, 2]

    rel_loc = (PHI0[:, 0] - PHI0[:, 2]) * np.pi / 180.0
    rel_loc = (rel_loc + np.pi) % (2 * np.pi) - np.pi
    rel_loc *= 180 / np.pi

    error_curr = (phi - PHI0[:, 2] * np.pi / 180.0)
    error_curr = (error_curr + np.pi) % (2 * np.pi) - np.pi
    error_curr *= 180 / np.pi

    error_prev = ((phi - PHI0[:, 0] * np.pi / 180.0))
    error_prev = (error_prev + np.pi) % (2 * np.pi) - np.pi
    error_prev *= 180 / np.pi

    errors = np.stack((error_prev, error_curr))

    return target_loc, rel_loc, errors
#+end_src

#+RESULTS:

#+begin_src ipython
def get_end_point(model, errors):

    stim_start_idx = ((model.start_indices - model.N_STEADY) / model.N_WINDOW - 1).to(int).cpu().numpy()

    end_point = []
    for i, j in enumerate([1, 3]):
        end_ = []
        for k in range(errors.shape[1]):
            idx = stim_start_idx[j][k]
            end_.append(errors[i][k][idx])

        end_point.append(end_)

    end_point = np.array(end_point)
    return end_point
#+end_src

#+RESULTS:

#+begin_src ipython
targ_ini, rel_ini, errors_ini = get_rel_tar_error(phi_ini, phi0_ini)
targ_last, rel_last, errors_last = get_rel_tar_error(phi_last, phi0_last)
#+end_src

#+RESULTS:

#+begin_src ipython
end_point_ini = get_end_point(model, errors_ini)
end_point_last = get_end_point(model, errors_last)
#+end_src

#+RESULTS:

* Biases

#+begin_src ipython
n_bins = 8
data_ini = pd.DataFrame({'target_loc': targ_ini[:, -1], 'rel_loc': rel_ini[:, -1], 'errors': end_point_ini[1]})
data_last = pd.DataFrame({'target_loc': targ_last[:, -1], 'rel_loc': rel_last[:, -1], 'errors': end_point_last[1]})
#+end_src

#+RESULTS:

#+begin_src ipython
def get_correct_error(nbins, data):
    # 1. Bias-correct both error and error_half
    bin_edges = np.linspace(0, 360, n_bins + 1)
    data['bin_target'] = pd.cut(data['target_loc'], bins=bin_edges, include_lowest=True)
    mean_errors_per_bin = data.groupby('bin_target')['errors'].mean()
    data['adjusted_errors'] = data['errors'] - data['bin_target'].map(mean_errors_per_bin).astype(float)


    # 2. Bin by relative location for both sessions (full version, [-180, 180])
    data['bin_rel'] = pd.cut(data['rel_loc'], bins=n_bins)
    bin_rel = data.groupby('bin_rel')['adjusted_errors'].agg(['mean', 'sem']).reset_index()
    edges = bin_rel['bin_rel'].cat.categories
    centers = (edges.left + edges.right) / 2

    # 3. FLIP SIGN for abs(rel_loc): defects on the left (-) are flipped so all bins reflect the same "direction"
    data['rel_loc_abs'] = np.abs(data['rel_loc'])
    data['bin_rel_abs'] = pd.cut(data['rel_loc_abs'], bins=n_bins, include_lowest=True)

    # Flip errors for abs plot:
    data['adjusted_errors_abs'] = data['adjusted_errors'] * np.sign(data['rel_loc'])

    bin_rel_abs = data.groupby('bin_rel_abs')['adjusted_errors_abs'].agg(['mean', 'sem']).reset_index()
    edges_abs = bin_rel_abs['bin_rel_abs'].cat.categories
    centers_abs = (edges_abs.left + edges_abs.right) / 2

    # 4. Bin by target location for target-centered analysis (optional)
    bin_target = data.groupby('bin_target')['adjusted_errors'].agg(['mean', 'sem']).reset_index()
    edges_target = bin_target['bin_target'].cat.categories
    target_centers = (edges_target.left + edges_target.right) / 2

    return centers, bin_rel, centers_abs, bin_rel_abs
#+end_src

#+RESULTS:

#+begin_src ipython
centers_ini, bin_rel_ini, centers_abs_ini, bin_rel_abs_ini = get_correct_error(n_bins, data_ini)
centers_last, bin_rel_last, centers_abs_last, bin_rel_abs_last = get_correct_error(n_bins, data_last)
#+end_src

#+RESULTS:

#+begin_src ipython
fig, ax = plt.subplots(1, 2, figsize=[2*width, height])

# Panel 2: By Relative Location (Full vs Half session, -180..180)
ax[0].plot(centers_ini, bin_rel_ini['mean'], 'r', label='ini')
ax[0].fill_between(centers_ini, bin_rel_ini['mean'] - bin_rel_ini['sem'], bin_rel_ini['mean'] + bin_rel_ini['sem'], color='r', alpha=0.2)

ax[0].plot(centers_last, bin_rel_last['mean'], 'b', label='last')
ax[0].fill_between(centers_last, bin_rel_last['mean'] - bin_rel_last['sem'], bin_rel_last['mean'] + bin_rel_last['sem'], color='b', alpha=0.2)

ax[0].axhline(0, color='k', linestyle=":")
ax[0].set_xlabel('Rel. Loc. (°)')
ax[0].set_ylabel('Corrected Error (°)')


# Panel 3: By |Relative Location| (Full and Half)
ax[1].plot(centers_abs_ini, bin_rel_abs_ini['mean'], 'r', label='ini')
ax[1].fill_between(centers_abs_ini, bin_rel_abs_ini['mean'] - bin_rel_abs_ini['sem'], bin_rel_abs_ini['mean'] + bin_rel_abs_ini['sem'], color='r', alpha=0.2)

ax[1].plot(centers_abs_last, bin_rel_abs_last['mean'], 'b', label='last')
ax[1].fill_between(centers_abs_last, bin_rel_abs_last['mean'] - bin_rel_abs_last['sem'], bin_rel_abs_last['mean'] + bin_rel_abs_last['sem'], color='b', alpha=0.2)

ax[1].axhline(0, color='k', linestyle=":")
ax[1].set_xlabel('|Rel. Loc.| (°)')
ax[1].set_ylabel('Corrected Error (°)')
ax[1].legend(fontsize=12)

plt.tight_layout()
plt.show()
#+end_src

#+RESULTS:
[[./figures/odr_sb/figure_27.png]]

#+begin_src ipython

#+end_src

#+RESULTS:
