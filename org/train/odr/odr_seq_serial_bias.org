#+STARTUP: fold
#+TITLE: ODR Sequential Serial Bias
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session odr_seq_sb :kernel torch :exports results :output-dir ./figures/odr_seq_sb :file (lc/org-babel-tangle-figure-filename)

* Imports

#+begin_src ipython
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, TensorDataset, DataLoader
from scipy.stats import binned_statistic
#+end_src

#+RESULTS:

#+begin_src ipython
  import sys
  sys.path.insert(0, '../../../')

  import pandas as pd
  import torch.nn as nn
  from time import perf_counter
  from scipy.stats import circmean

  from src.network import Network
  from src.plot_utils import plot_con
  from src.decode import *
  from src.lr_utils import masked_normalize, clamp_tensor, normalize_tensor
  from src.utils import clear_cache
#+end_src

#+RESULTS:

#+begin_src ipython
  import pickle as pkl
  import os

  def pkl_save(obj, name, path="."):
      os.makedirs(path, exist_ok=True)
      destination = path + "/" + name + ".pkl"
      print("saving to", destination)
      pkl.dump(obj, open(destination, "wb"))


  def pkl_load(name, path="."):
      source = path + "/" + name + '.pkl'
      print('loading from', source)
      return pkl.load(open( source, "rb"))

#+end_src

#+RESULTS:

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run ../../../notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'

  REPO_ROOT = "/home/leon/models/NeuroFlame"
  pal = sns.color_palette("tab10")
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Helpers

#+begin_src ipython
def map2center(angles):
    """Map angles from [0, 2π] to [-π, π] using PyTorch tensors."""
    return np.where(angles > np.pi, angles - 2 * np.pi, angles)

def map2pos(angles):
    """Map angles from [-π, π] to [0, 2π] using PyTorch tensors."""
    return np.where(angles < 0, angles + 2 * np.pi, angles)
#+end_src

#+RESULTS:

#+begin_src ipython
def maptens2center(angles):
    """Map angles from [0, 2π] to [-π, π] using PyTorch tensors."""
    return torch.where(angles > torch.pi, angles - 2 * torch.pi, angles)

def maptens2pos(angles):
    """Map angles from [-π, π] to [0, 2π] using PyTorch tensors."""
    return torch.where(angles < 0, angles + 2 * torch.pi, angles)
#+end_src

#+RESULTS:

#+begin_src ipython
def add_vlines(model, ax=None):

    if ax is None:
        for i in range(len(model.T_STIM_ON)):
            plt.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)
    else:
        for i in range(len(model.T_STIM_ON)):
            ax.axvspan(model.T_STIM_ON[i], model.T_STIM_OFF[i], alpha=0.25)

#+end_src

#+RESULTS:

* Model

#+begin_src ipython
name = 'patient'
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ./figures/NIH/patients.py
kwargs = {
    'GAIN': 1.,
    'DURATION': 6.0,
    'T_STEADY': 4,

    'T_STIM_ON': [1.0, 5.0],
    'T_STIM_OFF': [2.0, 6.0],

    'I0': [0.25, -2.0],
    'PHI0': [180.0, 180],
    'SIGMA0': [2.0, 0.0],
    'M0': 1.0,

    'RANDOM_DELAY': 0,
    'MIN_DELAY': 0,
    'MAX_DELAY': 5,

    'RANDOM_ITI': 0,
    'MAX_ITI': 6,
    'MIN_ITI': 2,

    'IF_FF_STP': 0,
    'FF_USE': 0.5,
    'TAU_FF_FAC': 0.0,
    'TAU_FF_REC': 0.5,

    'Jab': [1.0, -1.4, 1, -1],

    'IS_STP': [1, 0, 0, 0],
    'USE': [0.1, 0.03, 0.03, 0.1],
    'TAU_FAC': [2.0, 2.0, 2.0, 0.0],
    'TAU_REC': [0.2, 0.2, 0.2, 0.1],
    'W_STP': [1.0, 3.0, 4.0, 1.0],

    'IF_FF_ADAPT': 0,
    'A_FF_ADAPT': 1.0,
    'TAU_FF_ADAPT': 100.0,

    'IF_ADAPT': 1,
    'A_ADAPT': 1.0,
    'TAU_ADAPT': 100.0,

    'REP_BIAS': 0.0,
    'REP_VAR': 5.0,
}
#+end_src

#+RESULTS:

#+begin_src ipython
REPO_ROOT = "/home/leon/models/NeuroFlame"
conf_name = "train_odr_EI.yml"
DEVICE = 'cuda'
seed = np.random.randint(0, 1e6)

seed = 0
print('seed', seed)
#+end_src

#+RESULTS:
: seed 0

#+begin_src ipython
N_BATCH = 768
model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE=DEVICE, SEED=seed, N_BATCH=N_BATCH, **kwargs)
#+end_src

#+RESULTS:
: EE post 0 pre 0 Jab tensor(0.0632, device='cuda:0') torch.Size([750, 750])

#+begin_src ipython
model_state_dict = torch.load('../models/odr/odr_%d.pth' % seed)
model.load_state_dict(model_state_dict);
model.eval();
#+end_src

#+RESULTS:

#+begin_src ipython
GAIN = 1.0

model.J_STP =torch.nn.Parameter(GAIN * model.J_STP.detach())
model.Wab_T[model.slices[0], model.slices[1]] *= GAIN
print(model.J_STP)
#+end_src

#+RESULTS:
: Parameter containing:
: tensor(2.1821, device='cuda:0', requires_grad=True)

* Simulating Consecutive Trials

#+begin_src ipython
model.N_BATCH = N_BATCH

# continuous odr
model.PHI0 = torch.randint(low=0, high=360, size=(N_BATCH, len(model.I0), 1), device=DEVICE, dtype=torch.float)

# n target odr
# angles = torch.linspace(0, 360, steps=8+1, device=DEVICE)[:-1]  # exclude 360
# idx = torch.randint(0, 8, size=(N_BATCH, len(model.I0), 1), device=DEVICE)
# model.PHI0 = angles[idx]

with torch.no_grad():
    ff_input = model.init_ff_input()
    rates_tensor = model.forward(ff_input=ff_input)
    # del ff_input
    clear_cache()
print(ff_input.shape, rates_tensor.shape)
#+end_src

#+RESULTS:
: torch.Size([768, 505, 1000]) torch.Size([768, 61, 750])

#+begin_src ipython
def shifted_phase(phase1, phase2, bias_strength, bias_var, direction=-1):
    """
    shift phase2_original away from phase1 by bias_strength (in radians)
    direction='repulsive' for away, 'attractive' for toward
    All phases in radians
    """
    delta = (phase1 - phase2) * torch.pi / 180.0
    # - for repulsion, + for attraction
    phase2_biased = phase2 + direction * bias_strength * torch.sin(delta) + bias_var * torch.randn_like(phase2)
    return torch.remainder(phase2_biased, 360.0)
#+end_src

#+RESULTS:

#+begin_src ipython
from src.configuration import init_time_const

N_TRIALS = 100

rates_list = []
prev_list = [model.PHI0[:, 0].cpu().detach()]
curr_list = []

iti_list = [model.N_STEADY * model.DT]

model.RANDOM_ITI = 0

for epoch in tqdm(range(N_TRIALS)):
    with torch.no_grad():

        if model.RANDOM_ITI:
            init_time_const(model)

        iti_list.append(model.N_STEADY * model.DT)

        model.PHI0 = torch.randint(low=0, high=360, size=(N_BATCH, len(model.I0), 1), device=DEVICE, dtype=torch.float)
        model.PHI0_UNBIASED = torch.deg2rad(model.PHI0.clone())

        if model.REP_BIAS>0:
            model.PHI0[:, 0] = shifted_phase(prev_list[-1].to(DEVICE)*180.0 / torch.pi, model.PHI0[:, 0], model.REP_BIAS, model.REP_VAR)


        ff_input = model.init_ff_input()

        rates = model.forward(ff_input=ff_input, IF_INIT=0)

        curr_list.append(model.PHI0_UNBIASED[:, 0].cpu().detach())
        prev_list.append(model.PHI0_UNBIASED[:, 0].cpu().detach())

        rates_list.append(rates.cpu().detach())

        del ff_input, model.PHI0, rates
        clear_cache()

rates_list = torch.stack(rates_list).cpu().numpy()
prev_list = torch.stack(prev_list).cpu().numpy()[:-1]
curr_list = torch.stack(curr_list).cpu().numpy()
iti_list = np.stack(iti_list)

print('rates', rates_list.shape)
print('curr', curr_list.shape, 'prev', prev_list.shape, 'iti', iti_list.shape)
#+end_src

#+RESULTS:
: 100% 100/100 [01:14<00:00,  1.34it/s]
: rates (100, 768, 61, 750)
: curr (100, 768, 1) prev (100, 768, 1) iti (101,)

#+begin_src ipython
m0_list, m1_list, phi_list = decode_bump_torch(rates_list, axis=-1, RET_TENSOR=0)
print(phi_list.shape)
#+end_src

#+RESULTS:
: (100, 768, 61)

#+begin_src ipython
DURATION = rates_list.shape[2] / 10
N_NEURONS = rates_list.shape[-1]
N_SESSION = rates_list.shape[1]
#+end_src

#+RESULTS:

#+begin_src ipython
fig, ax = plt.subplots(1, 1, figsize=[3*width, height])

n_trials = 5
idx = np.random.randint(0, 100)
rates = np.vstack(rates_list[:n_trials, idx]).T
vmin, vmax = np.percentile(rates.reshape(-1), [5, 95])

plt.imshow(rates, aspect='auto', cmap='jet', vmin=vmin, vmax=vmax, origin='lower', extent=[0, n_trials * DURATION, 0, N_NEURONS])
plt.ylabel('Pref. Location (°)')
plt.yticks(np.linspace(0, N_NEURONS, 5), np.linspace(0, 360, 5).astype(int))
plt.xlabel('Time (s)')

plt.show()
#+end_src

#+RESULTS:
[[file:./figures/odr_seq_sb/figure_18.png]]

#+begin_src ipython
fig, ax = plt.subplots(1, 3, figsize=[3*width, height])


idx = np.random.randint(0, 100)

m0 = np.hstack(m0_list[:n_trials, idx]).T
m1 = np.hstack(m1_list[:n_trials, idx]).T
phi = np.hstack(phi_list[:n_trials, idx]).T

xtime = np.linspace(0, n_trials*DURATION, phi.shape[-1])
idx = np.random.randint(0, model.N_BATCH, 8)

ax[0].plot(xtime, m0)
ax[0].set_ylabel('$\mathcal{F}_0$ (Hz)')
ax[0].set_xlabel('Time (s)')

ax[1].plot(xtime, m1)
ax[1].set_ylabel('$\mathcal{F}_1$ (Hz)')
ax[1].set_xlabel('Time (s)')

ax[2].plot(xtime, phi * 180 / np.pi)
ax[2].set_yticks(np.linspace(0, 360, 5).astype(int), np.linspace(0, 360, 5).astype(int))
ax[2].set_ylabel('Bump Center (°)')
ax[2].set_xlabel('Time (s)')

# for i in range(n_trials):
#     ax[2].axhline(model.PHI0[i, 0, 0].cpu().detach(), xmin=0, xmax=int(1.0/n_trials), ls='--', color=colors[i])

plt.show()
#+end_src

#+RESULTS:
[[file:./figures/odr_seq_sb/figure_19.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Errors

#+begin_src ipython
n_half = N_TRIALS // 2
#+end_src

#+RESULTS:

#+begin_src ipython
curr_ini =  curr_list[:n_half]
curr_last = curr_list[-n_half:]

prev_ini =  prev_list[:n_half]
prev_last = prev_list[-n_half:]
print(curr_ini.shape, prev_ini.shape)
#+end_src

#+RESULTS:
: (50, 768, 1) (50, 768, 1)

#+begin_src ipython
phi_ini = phi_list[:n_half]
phi_last = phi_list[n_half:]
#+end_src

#+RESULTS:

#+begin_src ipython
def get_error_curr_prev(phi, curr, prev):
    target_loc = curr  * 180.0 / np.pi

    rel_loc = prev - curr
    rel_loc = (rel_loc + np.pi) % (2 * np.pi) - np.pi
    rel_loc *= 180 / np.pi

    error_curr = phi - curr
    error_curr = (error_curr + np.pi) % (2 * np.pi) - np.pi
    error_curr *= 180 / np.pi

    return np.vstack(target_loc), np.vstack(rel_loc), np.array(error_curr)
#+end_src

#+RESULTS:

#+begin_src ipython
def get_end_point(model, errors):

    stim_start_idx = ((model.start_indices - model.N_STEADY) / model.N_WINDOW).to(int).cpu().numpy()

    end_point = []
    for k in range(errors.shape[1]):
            idx = stim_start_idx[1][k]-1
            end_point.append(errors[:, k, idx])

    return np.array(end_point).T.reshape(-1, 1)
#+end_src

#+RESULTS:

#+begin_src ipython
targ_ini, rel_ini, errors_ini = get_error_curr_prev(phi_ini, curr_ini, prev_ini)
targ_last, rel_last, errors_last = get_error_curr_prev(phi_last, curr_last, prev_last)
print(targ_ini.shape, rel_ini.shape, errors_ini.shape)
#+end_src

#+RESULTS:
: (38400, 1) (38400, 1) (50, 768, 61)

#+begin_src ipython
end_point_ini = get_end_point(model, errors_ini)
end_point_last = get_end_point(model, errors_last)
print(end_point_ini.shape, end_point_last.shape)
#+end_src

#+RESULTS:
: (38400, 1) (38400, 1)

#+begin_src ipython
fig, ax = plt.subplots(1, 3, figsize=[3*width, height])

# ax[0].hist(reference[:, 0])
ax[1].hist(end_point_ini[:, 0], bins=50)
ax[2].hist(end_point_last[:, 0], bins=50)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/odr_seq_sb/figure_28.png]]


#+begin_src ipython
time_points = np.linspace(0, model.DURATION, errors_ini.shape[-1])
idx = np.random.randint(errors_ini.shape[1], size=100)

fig, ax = plt.subplots(1, 2, figsize=[2*width, height])
ax[0].plot(time_points, errors_ini[0][idx].T, alpha=.4)
add_vlines(model, ax[0])

ax[0].set_xlabel('t')
ax[0].set_ylabel('error first half(°)')

ax[1].plot(time_points, errors_last[0][idx].T, alpha=.4)
add_vlines(model, ax[1])

ax[1].set_xlabel('t')
ax[1].set_ylabel('error last half (°)')
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/odr_seq_sb/figure_29.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Biases

#+begin_src ipython
print(targ_ini.shape, rel_ini.shape, end_point_ini.shape)
#+end_src

#+RESULTS:
: (38400, 1) (38400, 1) (38400, 1)

#+begin_src ipython
n_bins = 16
data_ini = pd.DataFrame({'target_loc': targ_ini[:, -1], 'rel_loc': rel_ini[:, -1], 'errors': end_point_ini[:, 0]})
data_last = pd.DataFrame({'target_loc': targ_last[:, -1], 'rel_loc': rel_last[:, -1], 'errors': end_point_last[:, 0]})
#+end_src

#+RESULTS:

#+begin_src ipython
def get_correct_error(nbins, df, thresh=None):
    if thresh is not None:
        data = df[(df['errors'] >= -thresh) & (df['errors'] <= thresh)].copy()
    else:
        data = df.copy()

    # 1. Bias-correct both error and error_half
    bin_edges = np.linspace(0, 360, n_bins + 1)
    data['bin_target'] = pd.cut(data['target_loc'], bins=bin_edges, include_lowest=True)
    mean_errors_per_bin = data.groupby('bin_target')['errors'].mean()
    data['adjusted_errors'] = data['errors'] - data['bin_target'].map(mean_errors_per_bin).astype(float)


    # 2. Bin by relative location for both sessions (full version, [-180, 180])
    data['bin_rel'] = pd.cut(data['rel_loc'], bins=n_bins)
    bin_rel = data.groupby('bin_rel')['adjusted_errors'].agg(['mean', 'sem']).reset_index()
    edges = bin_rel['bin_rel'].cat.categories
    centers = (edges.left + edges.right) / 2

    # 3. FLIP SIGN for abs(rel_loc): defects on the left (-) are flipped so all bins reflect the same "direction"
    data['rel_loc_abs'] = np.abs(data['rel_loc'])
    data['bin_rel_abs'] = pd.cut(data['rel_loc_abs'], bins=n_bins, include_lowest=True)

    # Flip errors for abs plot:
    data['adjusted_errors_abs'] = data['adjusted_errors'] * np.sign(data['rel_loc'])

    bin_rel_abs = data.groupby('bin_rel_abs')['adjusted_errors_abs'].agg(['mean', 'sem']).reset_index()
    edges_abs = bin_rel_abs['bin_rel_abs'].cat.categories
    centers_abs = (edges_abs.left + edges_abs.right) / 2

    # 4. Bin by target location for target-centered analysis (optional)
    bin_target = data.groupby('bin_target')['adjusted_errors'].agg(['mean', 'sem']).reset_index()
    edges_target = bin_target['bin_target'].cat.categories
    target_centers = (edges_target.left + edges_target.right) / 2

    return centers, bin_rel, centers_abs, bin_rel_abs
#+end_src

#+RESULTS:

#+begin_src ipython
centers_ini, bin_rel_ini, centers_abs_ini, bin_rel_abs_ini = get_correct_error(n_bins, data_ini)
centers_last, bin_rel_last, centers_abs_last, bin_rel_abs_last = get_correct_error(n_bins, data_last)
#+end_src

#+RESULTS:

#+begin_src ipython
fig, ax = plt.subplots(1, 2, figsize=[2*width, height])

# Panel 2: By Relative Location (Full vs Half session, -180..180)
ax[0].plot(centers_ini, bin_rel_ini['mean'], 'r', label='First half')
ax[0].fill_between(centers_ini, bin_rel_ini['mean'] - bin_rel_ini['sem'], bin_rel_ini['mean'] + bin_rel_ini['sem'], color='r', alpha=0.2)

ax[0].plot(centers_last, bin_rel_last['mean'], 'b', label='Second half')
ax[0].fill_between(centers_last, bin_rel_last['mean'] - bin_rel_last['sem'], bin_rel_last['mean'] + bin_rel_last['sem'], color='b', alpha=0.2)

ax[0].axhline(0, color='k', linestyle=":")
ax[0].set_xlabel('Rel. Loc. (°)')
ax[0].set_ylabel('Corrected Error (°)')


# Panel 3: By |Relative Location| (Full and Half)
ax[1].plot(centers_abs_ini, bin_rel_abs_ini['mean'], 'r', label='First half')
ax[1].fill_between(centers_abs_ini, bin_rel_abs_ini['mean'] - bin_rel_abs_ini['sem'], bin_rel_abs_ini['mean'] + bin_rel_abs_ini['sem'], color='r', alpha=0.2)

ax[1].plot(centers_abs_last, bin_rel_abs_last['mean'], 'b', label='Second half')
ax[1].fill_between(centers_abs_last, bin_rel_abs_last['mean'] - bin_rel_abs_last['sem'], bin_rel_abs_last['mean'] + bin_rel_abs_last['sem'], color='b', alpha=0.2)

ax[1].axhline(0, color='k', linestyle=":")
ax[1].set_xlabel('|Rel. Loc.| (°)')
ax[1].set_ylabel('Corrected Error (°)')
ax[1].legend(fontsize=12)

plt.tight_layout()

plt.savefig('./figures/NIH/10_25/sb_half_%s.svg' % name)

plt.show()
#+end_src

#+RESULTS:
[[file:./figures/odr_seq_sb/figure_35.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Trial Dependency

#+begin_src ipython
import numpy as np
from scipy.optimize import curve_fit

def fit_deriv_gaussian_circular(df, n_bins, target_col='target_loc', error_col='errors', rel_col='rel_loc', n_tries=20, thresh=25):
    if thresh is not None:
        data = df[(df['errors'] >= -thresh) & (df['errors'] <= thresh)].copy()
    else:
        data = df.copy()

    # 1. Compute "adjusted_errors"
    bin_edges = np.linspace(0, 360, n_bins + 1)
    data = data.copy()
    data['bin_target'] = pd.cut(
        data[target_col], bins=bin_edges, include_lowest=True, right=False)
    mean_errors_per_bin = data.groupby('bin_target', observed=False)[error_col].mean()

    data['adjusted_errors'] = (
        data[error_col] - data['bin_target'].map(mean_errors_per_bin).astype(float)
    )

    # 2. Circular binning for kernel fitting
    x = data[rel_col].values
    y = data['adjusted_errors'].values
    bins = np.linspace(-180, 180, n_bins + 1)
    bin_indices = np.digitize(x, bins, right=False) - 1
    bin_indices[bin_indices == n_bins] = 0

    bin_centers = (bins[:-1] + bins[1:]) / 2
    bin_means = np.array([
        y[bin_indices == i].mean() if np.any(bin_indices == i) else np.nan
        for i in range(n_bins)
    ])

    # Guess parameters from the data
    ampl_guess = (np.nanmax(bin_means) - np.nanmin(bin_means)) / 2
    sigma_guess = (np.nanmax(bin_centers) - np.nanmin(bin_centers)) / 4

    # Model
    def deriv_gaussian(x, A, sigma, mu=0):
        return -A * (x - mu) * np.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / (sigma ** 2)

    mask = np.isfinite(bin_means)
    fit_centers = bin_centers[mask]
    fit_means = bin_means[mask]

    best_loss = np.inf
    best_popt = None

    for _ in range(n_tries):
        # Vary around data-driven guess
        p0 = [
            ampl_guess * np.random.uniform(0.0, 10.0),
            sigma_guess * np.random.uniform(1.0, 10.0),
        ]
        try:
            popt, _ = curve_fit(
                deriv_gaussian, fit_centers, fit_means, p0=p0, maxfev=5000)
            residuals = fit_means - deriv_gaussian(fit_centers, *popt)
            loss = np.sum(residuals**2)
            if loss < best_loss:
                best_loss = loss
                best_popt = popt
        except RuntimeError:
            continue

    if best_popt is None:
        raise RuntimeError("Fit did not converge in any of the tries.")

    result = {
        'amplitude_at_90': -best_popt[0] * (90 - 0) * np.exp(-((90 - 0) ** 2) / (2 * best_popt[1] ** 2)) / (best_popt[1] ** 2),
        'bin_centers': bin_centers,
        'bin_means': bin_means,
        'fit': lambda x: deriv_gaussian(x, *best_popt),
        'data': data
    }

    return result

#+end_src

#+RESULTS:

#+begin_src ipython
from joblib import Parallel, delayed
import numpy as np

def bootstrap_amplitude_at_90(
    data, n_bins, n_boot=100, n_jobs=-1, random_state=None, fit_kwargs=None
):
    # fit_kwargs: dict for extra arguments to fit_deriv_gaussian_circular
    if fit_kwargs is None:
        fit_kwargs = {}
    rng = np.random.RandomState(random_state)

    def _single_boot(random_seed):
        import warnings
        from scipy.optimize import OptimizeWarning
        warnings.simplefilter("ignore", OptimizeWarning)
        np.random.seed(random_seed)
        d_samp = data.sample(frac=1, replace=True, random_state=np.random.randint(0, 2**32))
        try:
            res = fit_deriv_gaussian_circular(d_samp, n_bins, **fit_kwargs)
            return res['amplitude_at_90']
        except Exception:
            return np.nan

    seeds = rng.randint(0, 2**32, size=n_boot)
    results = Parallel(n_jobs=n_jobs)(
        delayed(_single_boot)(s) for s in seeds
    )
    results = np.array([r for r in results if np.isfinite(r)])
    ci = np.percentile(results, [2.5, 97.5])
    return ci
#+end_src

#+RESULTS:

#+begin_src ipython
cmap = plt.get_cmap('Blues')
colors = [cmap( (i+1) / phi_list.shape[0] ) for i in range(phi_list.shape[0])]

n_bins = 8

serial_list = []

fig, ax = plt.subplots(1, 2, figsize=[2*width, height])

for i in range(phi_list.shape[0]): # trial by trial
    targ_trial, rel_trial, errors_trial = get_error_curr_prev(phi_list[i, np.newaxis], curr_list[i, np.newaxis], prev_list[i, np.newaxis])
    end_point_trial = get_end_point(model, errors_trial)

    data = pd.DataFrame({'target_loc': targ_trial[:, -1], 'rel_loc': rel_trial[:, -1], 'errors': end_point_trial[:, 0]})

    centers, bin_rel, centers_abs, bin_rel_abs = get_correct_error(n_bins, data)

    ax[0].plot(centers, bin_rel['mean'], color=colors[i], alpha=1)

    ax[0].axhline(0, ls='--', color='k')
    ax[0].set_xlabel('Rel. Loc. (°)')
    ax[0].set_ylabel('Error (°)')
    ax[0].set_xticks(np.linspace(-180, 180, 5))

    idx_max = np.argmax(np.abs(bin_rel['mean'][centers>0]))
    dum = bin_rel['mean'][centers>0]
    serial_max = dum.iloc[idx_max]

    dum = bin_rel['sem'][centers>0]
    serial_std = dum.iloc[idx_max]

    serial_list.append([serial_max, serial_std])

serial_list = np.array(serial_list).T
print(serial_list.shape)


from scipy.ndimage import gaussian_filter1d, uniform_filter1d

n_trials = 100
xdelay = np.linspace(0, n_trials, serial_list.shape[1])

s0 = 5
ax[1].plot(xdelay, uniform_filter1d(serial_list[0], s0, mode='nearest'), '-')
ax[1].fill_between(xdelay, uniform_filter1d(serial_list[0] - serial_list[1], s0, mode='nearest') ,
                   uniform_filter1d(serial_list[0] + serial_list[1], s0, mode='nearest'),
                   color='b', alpha=0.2)

ax[1].axhline(0, ls='--', color='k')

ax[1].set_xlabel('Trial #')
ax[1].set_ylabel('Serial Bias (°)')

plt.savefig('./figures/NIH/10_25/sb_trial_%s.svg' % name)

plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: (2, 100)
[[file:./figures/odr_seq_sb/figure_39.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

* Delay Dependency

#+begin_src ipython
targ_list, rel_list, errors_list = get_error_curr_prev(phi_list, curr_list, prev_list)
print(targ_list.shape, rel_list.shape, rel_list[:5, 0], errors_list.shape)
#+end_src

#+RESULTS:
: (76800, 1) (76800, 1) [-48.        28.000006 -76.         0.       113.00003 ] (100, 768, 61)

#+begin_src ipython
stim_start = (model.DT * (model.start_indices - model.N_STEADY)).cpu().numpy()
stim_end = (model.DT * (model.end_indices - model.N_STEADY)).cpu().numpy()

stim_start_idx = ((model.start_indices - model.N_STEADY) / model.N_WINDOW - 1).to(int).cpu().numpy()
stim_end_idx = ((model.end_indices - model.N_STEADY) / model.N_WINDOW - 1).to(int).cpu().numpy()
#+end_src

#+RESULTS:

#+begin_src ipython
delay_point = []

for i in range(errors_list.shape[1]): # loop over sessions
        idx_start = stim_end_idx[0][i] # delay start
        idx_end = stim_start_idx[1][i] # delay stops

        end_ = []

        for idx in range(idx_start, idx_end): # loop over delay idx
                end__ = []
                for j in range(errors_list.shape[0]): # loop over trials
                        end__.append(errors_list[j, i, idx]) # append all delay errors

                end_.append(end__)
        delay_point.append(end_)

delay_point = np.vstack(np.array(delay_point).T.swapaxes(-1, 1))
print(delay_point.shape, errors_list.shape)
#+end_src

#+RESULTS:
: (76800, 30) (100, 768, 61)

#+begin_src ipython
import numpy as np
from scipy.optimize import curve_fit

def fit_deriv_gaussian_circular(df, n_bins, target_col='target_loc', error_col='errors', rel_col='rel_loc', n_tries=5, thresh=25):
    if thresh is not None:
        data = df[(df['errors'] >= -thresh) & (df['errors'] <= thresh)].copy()
    else:
        data = df.copy()

    # 1. Compute "adjusted_errors"
    bin_edges = np.linspace(0, 360, n_bins + 1)
    data = data.copy()
    data['bin_target'] = pd.cut(
        data[target_col], bins=bin_edges, include_lowest=True, right=False)
    mean_errors_per_bin = data.groupby('bin_target', observed=False)[error_col].mean()

    data['adjusted_errors'] = (
        data[error_col] - data['bin_target'].map(mean_errors_per_bin).astype(float)
    )

    # 2. Circular binning for kernel fitting
    x = data[rel_col].values
    y = data['adjusted_errors'].values
    bins = np.linspace(-180, 180, n_bins + 1)
    bin_indices = np.digitize(x, bins, right=False) - 1
    bin_indices[bin_indices == n_bins] = 0

    bin_centers = (bins[:-1] + bins[1:]) / 2
    bin_means = np.array([
        y[bin_indices == i].mean() if np.any(bin_indices == i) else np.nan
        for i in range(n_bins)
    ])

    # Guess parameters from the data
    ampl_guess = (np.nanmax(bin_means) - np.nanmin(bin_means)) / 2
    sigma_guess = (np.nanmax(bin_centers) - np.nanmin(bin_centers)) / 4

    # Model
    def deriv_gaussian(x, A, sigma, mu=0):
        return -A * (x - mu) * np.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / (sigma ** 2)

    mask = np.isfinite(bin_means)
    fit_centers = bin_centers[mask]
    fit_means = bin_means[mask]

    best_loss = np.inf
    best_popt = None

    for _ in range(n_tries):
        # Vary around data-driven guess
        p0 = [
            ampl_guess * np.random.uniform(0.0, 10.0),
            sigma_guess * np.random.uniform(1.0, 10.0),
        ]
        try:
            popt, _ = curve_fit(
                deriv_gaussian, fit_centers, fit_means, p0=p0, maxfev=5000)
            residuals = fit_means - deriv_gaussian(fit_centers, *popt)
            loss = np.sum(residuals**2)
            if loss < best_loss:
                best_loss = loss
                best_popt = popt
        except RuntimeError:
            continue

    if best_popt is None:
        raise RuntimeError("Fit did not converge in any of the tries.")

    result = {
        'amplitude_at_90': -best_popt[0] * (90 - 0) * np.exp(-((90 - 0) ** 2) / (2 * best_popt[1] ** 2)) / (best_popt[1] ** 2),
        'bin_centers': bin_centers,
        'bin_means': bin_means,
        'fit': lambda x: deriv_gaussian(x, *best_popt),
        'data': data
    }

    return result

#+end_src

#+RESULTS:

#+begin_src ipython
from joblib import Parallel, delayed
import numpy as np

def bootstrap_amplitude_at_90(data, n_bins, n_boot=100, n_jobs=-1, random_state=None, fit_kwargs=None):
    # fit_kwargs: dict for extra arguments to fit_deriv_gaussian_circular
    if fit_kwargs is None:
        fit_kwargs = {}
    rng = np.random.RandomState(random_state)

    def _single_boot(random_seed):
        import warnings
        from scipy.optimize import OptimizeWarning
        warnings.simplefilter("ignore", OptimizeWarning)
        np.random.seed(random_seed)
        d_samp = data.sample(frac=1, replace=True, random_state=np.random.randint(0, 2**32))
        try:
            res = fit_deriv_gaussian_circular(d_samp, n_bins, **fit_kwargs)
            return res['amplitude_at_90']
        except Exception:
            return np.nan

    seeds = rng.randint(0, 2**32, size=n_boot)
    results = Parallel(n_jobs=n_jobs)(
        delayed(_single_boot)(s) for s in seeds
    )
    results = np.array([r for r in results if np.isfinite(r)])
    ci = np.percentile(results, [2.5, 97.5])
    return ci
#+end_src

#+RESULTS:

#+begin_src ipython
n_bins = 16

cmap = plt.get_cmap('Blues')
colors = [cmap((i+1)/ delay_point.shape[1]) for i in range(delay_point.shape[1])]

fig, ax = plt.subplots(1, 2, figsize=[2*width, height])

serial_list = []
for i in range(delay_point.shape[1]):
    data = pd.DataFrame({'target_loc': targ_list[:, -1], 'rel_loc': rel_list[:, -1], 'errors': delay_point[:, i]})
    centers, bin_rel, centers_abs, bin_rel_abs = get_correct_error(n_bins, data)

    ax[0].plot(centers, bin_rel['mean'], color=colors[i])

    idx_max = np.argmax(np.abs(bin_rel['mean'][centers>0]))
    dum = bin_rel['mean'][centers>0]
    serial_max = dum.iloc[idx_max]

    dum = bin_rel['sem'][centers>0]
    serial_std = dum.iloc[idx_max] * 1.96

    serial_list.append([serial_max, serial_std])

serial_list = np.array(serial_list).T
print(serial_list.shape)
ax[0].set_xlabel('Rel. Loc. (°)')
ax[0].set_ylabel('Error (°)')

delay_duration = stim_start[1, 0] - stim_end[0, 0]
xdelay = np.linspace(0, delay_duration, serial_list.shape[1])

ax[1].plot(xdelay, serial_list[0], '-', label=dum)
ax[1].fill_between(xdelay, serial_list[0] - serial_list[1], serial_list[0] + serial_list[1], color='b', alpha=0.2)
ax[1].set_xlabel('Delay Length (s)')
ax[1].set_ylabel('Serial Bias (°)')

ax[1].axhline(0, ls='--', color='k')
ax[0].axhline(0, ls='--', color = 'k')
# ax[1].legend(fontsize=12)
plt.savefig('./figures/NIH/10_25/sb_delay_%s.svg' % name)
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: (2, 30)
[[file:./figures/odr_seq_sb/figure_46.png]]
:END:

#+begin_src ipython
import warnings
from scipy.ndimage import gaussian_filter1d
from scipy.optimize import OptimizeWarning

n_bins = 16

cmap = plt.get_cmap('Blues')
colors = [cmap((i+1)/ delay_point.shape[1]) for i in range(delay_point.shape[1])]
fig, ax = plt.subplots(1, 2, figsize=[2*width, height])

serial_list = []
serial_ci = []
# for i in range(delay_point.shape[1]):
    # data = pd.DataFrame({'target_loc': targ_list[:, -1], 'rel_loc': rel_list[:, -1], 'errors': delay_point[:, i]})

    # result = fit_deriv_gaussian_circular(data, n_bins=n_bins)
    # ci = bootstrap_amplitude_at_90(data.copy(), n_bins=n_bins, n_boot=10)

    # result = fit_deriv_gaussian_circular(data, n_bins=n_bins)
    # ax[0].plot(result['bin_centers'], result['fit'](result['bin_centers']), alpha=1, color=colors[i])

    # serial_list.append(result['amplitude_at_90'])
    # serial_ci.append(ci)

serial_list = np.array(serial_list)
serial_ci = np.array(serial_ci)

ax[0].set_xlabel('Rel. Loc. (°)')
ax[0].set_ylabel('Error (°)')

delay_duration = stim_start[1, 0] - stim_end[0, 0]
xdelay = np.linspace(0, delay_duration, serial_list.shape[0])

# ax[1].plot(xdelay, serial_list, '-')
# ax[1].fill_between(xdelay, serial_ci[:,0], serial_ci[:,1], color='gray', alpha=0.3, label='95% CI')

ax[1].set_xlabel('Delay Length (s)')
ax[1].set_ylabel('Serial Bias (°)')


plt.show()
#+end_src

#+RESULTS:
[[file:./figures/odr_seq_sb/figure_47.png]]

* ITI Dependency

#+begin_src ipython
unique_iti = np.unique(iti_list[1:])
print(unique_iti)
n_unique = len(unique_iti)
#+end_src

#+RESULTS:
: [4.]

#+begin_src ipython
n_bins = 8

cmap = plt.get_cmap('Blues')
colors = [cmap((i+1)/ n_unique) for i in range(n_unique+1)]

fig, ax = plt.subplots(1, 2, figsize=[2*width, height])

serial_list = []

# bins = np.histogram_bin_edges(iti_list, bins=n_bins)
# iti_bins = np.digitize(iti_list, bins) - 1  # bin indices for each trial
# for iti in range(n_bins):
#     idx = np.where(iti_bins == iti)[0]

for i, iti in enumerate(unique_iti):
    idx = np.where(iti_list[1:] == iti)[0]

    if len(idx) > 0:

        data = pd.DataFrame({'target_loc': targ_list[idx, -1], 'rel_loc': rel_list[idx, -1], 'errors': delay_point[idx, -1]})
        centers, bin_rel, centers_abs, bin_rel_abs = get_correct_error(n_bins, data)

        ax[0].plot(centers, bin_rel['mean'], color=colors[i])

        idx_max = np.argmax(np.abs(bin_rel['mean'][centers>0]))
        dum = bin_rel['mean'][centers>0]
        serial_max = dum.iloc[idx_max]

        dum = bin_rel['sem'][centers>0]
        serial_std = dum.iloc[idx_max]

        idx_max = np.argmax(np.abs(bin_rel_abs['mean']))
        dum = bin_rel_abs['mean']
        serial_max = dum.iloc[idx_max]

        dum = bin_rel_abs['sem']
        serial_std = dum.iloc[idx_max]

        serial_list.append([serial_max, serial_std])

serial_list = np.array(serial_list).T
print(serial_list.shape)
ax[0].set_xlabel('Rel. Loc. (°)')
ax[0].set_ylabel('Error (°)')
ax[0].axhline(0, ls="--", color='k')

delay_duration = model.MAX_ITI
xdelay = np.linspace(model.MIN_ITI, delay_duration, serial_list.shape[1])

ax[1].plot(xdelay, serial_list[0], '-')
ax[1].fill_between(xdelay, serial_list[0] - serial_list[1], serial_list[0] + serial_list[1], color='b', alpha=0.2)
ax[1].set_xlabel('ITI (s)')
ax[1].set_ylabel('Serial Bias (°)')
ax[1].axhline(0, ls='--', color='k')

plt.savefig('./figures/NIH/10_25/sb_iti_%s.svg' % name)
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: (2, 1)
[[file:./figures/odr_seq_sb/figure_49.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:
