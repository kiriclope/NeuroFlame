#+Startup: fold
#+TITLE: Training Low Rank RNNs
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session train_dual_multi :kernel torch :exports results :output-dir ./figures/multi :file (lc/org-babel-tangle-figure-filename)

* Notebook Settings

#+begin_src ipython :tangle no
%load_ext autoreload
%autoreload 2
%reload_ext autoreload
%run ../../../notebooks/setup.py
%matplotlib inline
%config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython :tangle ./run_dual_multi.py
import torch
import torch.nn as nn
import torch.optim as optim
import torchmetrics
from torch.utils.data import Dataset, TensorDataset, DataLoader

REPO_ROOT = "/home/leon/models/NeuroFlame"

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

pal = sns.color_palette("tab10")
DEVICE = 'cuda:0'
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ./run_dual_multi.py
import sys
sys.path.insert(0, '../../../')

from notebooks.setup import *

import pandas as pd
import torch.nn as nn
from time import perf_counter
from scipy.stats import circmean

from src.network import Network
from src.plot_utils import plot_con
from src.decode import decode_bump, circcvl
from src.lr_utils import masked_normalize, clamp_tensor, normalize_tensor
#+end_src

#+RESULTS:
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Train

#+begin_src ipython :tangle ./run_dual_multi.py
sys.path.insert(0, '../../../src')
# import src.train
from src.train.dual.train_dpa import train_dpa
from src.train.dual.train_gng import train_gng
from src.train.dual.train_dual import train_dual
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ./run_dual_multi.py
REPO_ROOT = "/home/leon/models/NeuroFlame"
conf_name = "train_dual.yml"
DEVICE = 'cuda:0'

# seed = np.random.randint(0, 1e6)
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ./run_dual_multi.py

#+end_src

#+RESULTS:

#+begin_src ipython :tangle ./run_dual_multi.py
for ite in range(0, 1):
    seed = ite
    print('model', seed)
    # train_dpa(REPO_ROOT, conf_name, seed, DEVICE)
    # train_gng(REPO_ROOT, conf_name, seed, DEVICE)
    train_dual(REPO_ROOT, conf_name, seed, DEVICE)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
model 0
training Dual
 10% 1/10 [00:45<06:45, 45.04s/it]Epoch 1/10, Training Loss: 0.8179, Validation Loss: 0.2883

 20% 2/10 [01:29<05:56, 44.58s/it]Epoch 2/10, Training Loss: 0.2280, Validation Loss: 0.2009

 30% 3/10 [02:13<05:11, 44.45s/it]Epoch 3/10, Training Loss: 0.1916, Validation Loss: 0.2011

 40% 4/10 [02:57<04:25, 44.31s/it]Epoch 4/10, Training Loss: 0.1816, Validation Loss: 0.1733
 50% 5/10 [03:41<03:41, 44.25s/it]Epoch 5/10, Training Loss: 0.1740, Validation Loss: 0.1704
 60% 6/10 [04:25<02:56, 44.20s/it]Epoch 6/10, Training Loss: 0.1633, Validation Loss: 0.1588
 70% 7/10 [05:10<02:12, 44.17s/it]Epoch 7/10, Training Loss: 0.1572, Validation Loss: 0.1525
 80% 8/10 [05:54<01:28, 44.22s/it]Epoch 8/10, Training Loss: 0.1539, Validation Loss: 0.1547
 90% 9/10 [06:38<00:44, 44.20s/it]Epoch 9/10, Training Loss: 0.1524, Validation Loss: 0.1458
 90% 9/10 [07:22<00:49, 49.18s/it]Epoch 10/10, Training Loss: 0.1485, Validation Loss: 0.1471
Stopping training as loss has fallen below the threshold: 0.14848024684649247, 0.14709122188679583
Elapsed (with compilation) = 0h 7m 22s

#+end_example
:END:

#+begin_src ipython :tangle ./run_dual_multi.py

#+end_src

#+RESULTS:
