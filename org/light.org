#+STARTUP: fold
#+TITLE: Training Low Rank RNNs
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session light :kernel torch

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run ../notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'

  REPO_ROOT = "/home/leon/models/NeuroFlame"
  pal = sns.color_palette("tab10")
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython
  import lightning as L
  import torch
  import torch.nn as nn
  import torch.nn.functional as F
  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader

  DEVICE = 'cuda:1'
#+end_src

#+RESULTS:

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  import pandas as pd
  import torch.nn as nn
  from time import perf_counter
  from scipy.stats import circmean

  from src.network import Network
  from src.lrnet import LRNet

  from src.plot_utils import plot_con
  from src.decode import decode_bump, circcvl
  from src.lr_utils import masked_normalize, clamp_tensor, normalize_tensor
#+end_src

#+RESULTS:

* Helpers
** Data Split

#+begin_src ipython
  from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

  def split_data(X, Y, train_perc=0.8, batch_size=32):

    if Y.ndim==3:
      X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                          train_size=train_perc,
                                                          stratify=Y[:, 0, 0].cpu().numpy(),
                                                          shuffle=True)
    else:
      X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                          train_size=train_perc,
                                                          stratify=Y[:, 0].cpu().numpy(),
                                                          shuffle=True)
    print(X_train.shape, X_test.shape)
    print(Y_train.shape, Y_test.shape)

    train_dataset = TensorDataset(X_train, Y_train)
    val_dataset = TensorDataset(X_test, Y_test)

    # Create data loaders
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader
#+end_src

#+RESULTS:

* Configuration
#+begin_src ipython
  REPO_ROOT = "/home/leon/models/NeuroFlame"
  conf_name = "config_light.yml"
  DEVICE = 'cuda:1'
  seed = np.random.randint(0, 1e6)
  print(seed)
  #seed = 760946
#+end_src

#+RESULTS:
: 840852

* Model

#+begin_src ipython
  model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE=DEVICE, SEED=seed, N_BATCH=16)
#+end_src

#+RESULTS:

* Dataset

#+begin_src ipython
  for name, param in model.named_parameters():
      if param.requires_grad:
          print(name, param.shape)
#+end_src

#+RESULTS:
: low_rank.U torch.Size([1000, 1])
: low_rank.V torch.Size([1000, 1])
: low_rank.lr_kappa torch.Size([1])
: low_rank.linear.weight torch.Size([1, 500])
: low_rank.linear.bias torch.Size([1])

#+begin_src ipython
  model.LR_TRAIN = 1
  model.LR_READOUT=1
#+end_src

#+RESULTS:

Testing the network on steps from sample odor offset to test odor onset

#+begin_src ipython
  steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)

  mask = (steps >= (model.N_STIM_OFF[0] - model.N_STEADY)) & (steps <= (model.N_STEPS - model.N_STEADY))
  rwd_idx = np.where(mask)[0]
  print('rwd', rwd_idx)

  model.lr_eval_win = rwd_idx.shape[0]

  stim_mask = (steps >= (model.N_STIM_ON[0] - model.N_STEADY)) & (steps < (model.N_STIM_OFF[0] - model.N_STEADY))

  zero_idx = np.where(~mask & ~stim_mask )[0]
  print('zero', zero_idx)
#+end_src

#+RESULTS:
: rwd [20 21 22 23 24 25 26 27 28 29 30]
: zero [0 1 2 3 4 5 6 7 8 9]


*** Inputs and Labels

#+begin_src ipython
  model.N_BATCH = 64

  model.I0[0] = 2.0
  model.I0[1] = 0
  model.I0[2] = 0

  A = model.init_ff_input()

  model.I0[0] = -2.0
  model.I0[1] = 0
  model.I0[2] = 0

  B = model.init_ff_input()

  ff_input = torch.cat((A, B))
  print(ff_input.shape)
#+end_src

#+RESULTS:
: torch.Size([128, 410, 1000])

#+begin_src ipython
  labels_A = torch.ones((model.N_BATCH, 1))
  labels_B = torch.zeros((model.N_BATCH, 1))
  labels = torch.cat((labels_A, labels_B))

  print('labels', labels.shape)
#+end_src

#+RESULTS:
: labels torch.Size([128, 1])

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(ff_input.to(DEVICE), labels.to(DEVICE), train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: torch.Size([102, 410, 1000]) torch.Size([26, 410, 1000])
: torch.Size([102, 1]) torch.Size([26, 1])

* Run

#+begin_src ipython
  autoencoder = LRNet(model)
#+end_src

#+RESULTS:

#+begin_src ipython
  y_pred = autoencoder()
#+end_src

#+RESULTS:

#+begin_src ipython
  print(y_pred.shape)
#+end_src

#+RESULTS:
: torch.Size([64, 1])

#+begin_src ipython
  from lightning.pytorch.callbacks import ModelCheckpoint

  # Init ModelCheckpoint callback, monitoring 'val_loss'
  checkpoint_callback = ModelCheckpoint(monitor="val_loss")

  # Add your callback to the callbacks list
#+end_src

#+RESULTS:

#+begin_src ipython
  trainer = L.Trainer(devices=[1], max_epochs=30, num_sanity_val_steps=0, callbacks=[checkpoint_callback], enable_progress_bar=0)
#+end_src

#+RESULTS:
: GPU available: True (cuda), used: True
: TPU available: False, using: 0 TPU cores
: IPU available: False, using: 0 IPUs
: HPU available: False, using: 0 HPUs
#+RESULTS:


#+begin_src ipython
  trainer.fit(model=autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader);
#+end_src

#+RESULTS:
#+begin_example
  Epoch 1 - Training loss: 0.06704595685005188 - Validation loss: 0.0834856778383255
  Epoch 2 - Training loss: 0.060021109879016876 - Validation loss: 0.05904477462172508
  Epoch 3 - Training loss: 0.03932273015379906 - Validation loss: 0.04328054189682007
  Epoch 4 - Training loss: 0.04199071228504181 - Validation loss: 0.03238746151328087
  Epoch 5 - Training loss: 0.03726363182067871 - Validation loss: 0.024486519396305084
  Epoch 6 - Training loss: 0.0178943183273077 - Validation loss: 0.01860305666923523
  Epoch 7 - Training loss: 0.009691016748547554 - Validation loss: 0.014304155483841896
  Epoch 8 - Training loss: 0.01536739245057106 - Validation loss: 0.011173679493367672
  Epoch 9 - Training loss: 0.013639538548886776 - Validation loss: 0.00878053717315197
  Epoch 10 - Training loss: 0.007884226739406586 - Validation loss: 0.0068742032162845135
  Epoch 11 - Training loss: 0.003983794711530209 - Validation loss: 0.005478280130773783
  Epoch 12 - Training loss: 0.005142988637089729 - Validation loss: 0.00443229591473937
  Epoch 13 - Training loss: 0.004213503561913967 - Validation loss: 0.003635395085439086
  Epoch 14 - Training loss: 0.0013969286810606718 - Validation loss: 0.003002690616995096
  Epoch 15 - Training loss: 0.002237213309854269 - Validation loss: 0.0025427157524973154
  Epoch 16 - Training loss: 0.001377263804897666 - Validation loss: 0.002163103548809886
  Epoch 17 - Training loss: 0.0013074110029265285 - Validation loss: 0.001859017414972186
  Epoch 18 - Training loss: 0.0017423949902877212 - Validation loss: 0.0016099984059110284
  Epoch 19 - Training loss: 0.001097857835702598 - Validation loss: 0.0013967609265819192
  Epoch 20 - Training loss: 0.001008376362733543 - Validation loss: 0.0012279892107471824
  Epoch 21 - Training loss: 0.0011198909487575293 - Validation loss: 0.0010823977645486593
  Epoch 22 - Training loss: 0.0006780208786949515 - Validation loss: 0.0009628292173147202
  Epoch 23 - Training loss: 0.0007404856733046472 - Validation loss: 0.0008571629296056926
  Epoch 24 - Training loss: 0.000729889958165586 - Validation loss: 0.0007685404270887375
  Epoch 25 - Training loss: 0.0005835747579112649 - Validation loss: 0.0006908354698680341
  Epoch 26 - Training loss: 0.0005645986529998481 - Validation loss: 0.0006235477048903704
  Epoch 27 - Training loss: 0.0006280404049903154 - Validation loss: 0.0005665569333359599
  Epoch 28 - Training loss: 0.0005484010325744748 - Validation loss: 0.0005155388498678803
  `Trainer.fit` stopped: `max_epochs=30` reached.
  Epoch 29 - Training loss: 0.0003459947183728218 - Validation loss: 0.0004710112407337874
#+end_example
#+begin_example
  LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

    | Name      | Type              | Params
  ------------------------------------------------
  0 | model     | Network           | 2.5 K
  1 | linear    | Linear            | 501
  2 | criterion | BCEWithLogitsLoss | 0
  ------------------------------------------------
  3.0 K     Trainable params
  0         Non-trainable params
  3.0 K     Total params
  0.012     Total estimated model params size (MB)
  Epoch 0 - Training loss: 0.20096835494041443 - Validation loss: 0.12238327413797379
#+end_example

#+begin_src ipython
  train_loss = trainer.logged_metrics.get('train_loss')
  val_loss = trainer.logged_metrics.get('val_loss')

  print(f'Training Loss: {train_loss}, Validation Loss: {val_loss}')
#+end_src

#+RESULTS:
: Training Loss: 0.19053184986114502, Validation Loss: 0.18418894708156586
