#+STARTUP: fold
#+TITLE: Training Low Rank RNNs
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session dual :kernel torch

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run ../notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'

  REPO_ROOT = "/home/leon/models/NeuroFlame"
  pal = sns.color_palette("tab10")
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader
#+end_src

#+RESULTS:

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  import pandas as pd
  import torch.nn as nn
  from time import perf_counter
  from scipy.stats import circmean

  from src.network import Network
  from src.plot_utils import plot_con
  from src.decode import decode_bump, circcvl
  from src.lr_utils import masked_normalize, clamp_tensor, normalize_tensor
#+end_src

#+RESULTS:

* Helpers
** Data Split

#+begin_src ipython
  from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

  def split_data(X, Y, train_perc=0.8, batch_size=32):

    if Y.ndim==3:
      X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                          train_size=train_perc,
                                                          stratify=Y[:, 0, 0].cpu().numpy(),
                                                          shuffle=True)
    else:
      X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                          train_size=train_perc,
                                                          stratify=Y[:, 0].cpu().numpy(),
                                                          shuffle=True)
    print(X_train.shape, X_test.shape)
    print(Y_train.shape, Y_test.shape)

    train_dataset = TensorDataset(X_train, Y_train)
    val_dataset = TensorDataset(X_test, Y_test)

    # Create data loaders
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader
#+end_src

#+RESULTS:

** Optimization

#+begin_src ipython
  def train(dataloader, model, loss_fn, optimizer, penalty=None, lbd=1):
      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      model.train()
      for batch, (X, y) in enumerate(dataloader):
          X, y = X.to(device), y.to(device)
          # Compute prediction error
          y_pred = model(X)

          # if y.ndim==y_pred.ndim:
          loss = loss_fn(y_pred, y)

          if penalty is not None:
              reg_loss = 0
              for param in model.parameters():
                  if penalty=='l1':
                      reg_loss += torch.sum(torch.abs(param))
                  else:
                      reg_loss += torch.sum(torch.square(param))

                  loss = loss + lbd * reg_loss

          # Backpropagation
          loss.backward()
          # Clip gradients (norm)
          # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

          # Or clip gradients (value)
          #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)

          optimizer.step()
          optimizer.zero_grad()

      return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def test(dataloader, model, loss_fn):
      size = len(dataloader.dataset)
      num_batches = len(dataloader)

      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      # Validation loop.
      model.eval()
      val_loss = 0.0
      with torch.no_grad():
          for data, targets in dataloader:
              data, targets = data.to(device), targets.to(device)

              outputs = model(data)
              loss = loss_fn(outputs, targets)
              val_loss += loss.item() * data.size(0)

          val_loss /= size

      return val_loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def run_optim(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100, penalty=None, lbd=1, thresh=.005):
      scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
      # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)
      # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      model.to(device)

      loss_list = []
      val_loss_list = []

      # Training loop.
      for epoch in range(num_epochs):
          loss = train(train_loader, model, loss_fn, optimizer, penalty, lbd)
          val_loss = test(val_loader, model, loss_fn)
          scheduler.step(val_loss)

          loss_list.append(loss.item())
          val_loss_list.append(val_loss)

          # if epoch % int(num_epochs  / 10) == 0:
          print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

          if val_loss < thresh:
              print(f'Stopping training as loss has fallen below the threshold: {val_loss}')
              break

          if val_loss > 300:
              print(f'Stopping training as loss is too high: {val_loss}')
              break

          if torch.isnan(loss):
              print(f'Stopping training as loss is NaN.')
              break

      return loss_list, val_loss_list
#+end_src

#+RESULTS:

** Loss

#+begin_src ipython
  def accuracy(y_pred, labels):
    # Assuming 'outputs' are logits from your model (raw scores before sigmoid)
    predicted = (y_pred > 0).float()  # Convert to 0 or 1 based on comparison with 0
    # 'labels' should be your ground truth labels for the binary classification, also in 0 or 1
    correct = (predicted == labels).sum().item()
    accuracy = correct / labels.size(0)
    return accuracy
#+end_src

#+RESULTS:

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class SignBCELoss(nn.Module):
      def __init__(self, alpha=0.1, thresh=1.0, N=1000):
          super(SignBCELoss, self).__init__()
          self.alpha = alpha
          self.thresh = thresh
          self.N = N

          self.bce_with_logits = nn.BCEWithLogitsLoss()

      def forward(self, readout, targets):
          bce_loss = self.bce_with_logits(readout, targets)

          # sign_overlap = torch.sign(2 * targets - 1) * readout / (1.0 * self.N)

          mean_activation = readout.mean(dim=1).unsqueeze(-1)
          sign_overlap = torch.sign(2 * targets - 1) * mean_activation / (1.0 * self.N)

          sign_loss = F.relu(self.thresh - sign_overlap).mean()
          # sign_loss = torch.sigmoid(self.thresh -sign_overlap).mean()

          combined_loss = (1-self.alpha) * bce_loss + self.alpha * sign_loss
          return combined_loss
#+end_src

#+RESULTS:

#+begin_src ipython
  class CosineLoss(nn.Module):
      def __init__(self, readout):
          super(CosineLoss, self).__init__()
          self.cosine_similarity = nn.CosineSimilarity(dim=-1)
          self.readout = readout

      def forward(self, rates, target):
          # Calculate cosine similarity
          cosine_sim = self.cosine_similarity(torch.sign(target) * rates, readout)
          # Calculate the loss as 1 - cosine_similarity
          loss = 1 - cosine_sim
          # Return the mean loss over the batch
          return loss.mean()
#+end_src

#+RESULTS:

#+begin_src ipython
  class DualLoss(nn.Module):
      def __init__(self, alpha=0.1, thresh=1.0, N=1000, cue_idx=[], rwd_idx=-1):
          super(DualLoss, self).__init__()
          self.alpha = alpha
          self.thresh = thresh
          self.N = N

          self.cue_idx = torch.tensor(cue_idx, dtype=torch.int, device='cuda')
          self.rwd_idx = torch.tensor(rwd_idx, dtype=torch.int, device='cuda')

          self.loss = SignBCELoss(self.alpha, self.thresh, self.N)

      def forward(self, readout, targets):

          is_empty = self.cue_idx.numel() == 0
          if is_empty:
              self.DPA_loss = self.loss(readout[:, self.rwd_idx], targets)
              return self.DPA_loss
          else:
              self.DPA_loss = self.loss(readout[:, self.rwd_idx], targets[:, 0, :self.rwd_idx.shape[0]])
              self.DRT_loss = self.loss(readout[:, self.cue_idx], targets[:, 1, :self.cue_idx.shape[0]])
              return (self.DPA_loss + self.DRT_loss) / 2.0
#+end_src

#+RESULTS:

** Other

#+begin_src ipython
  def get_theta(a, b, GM=0, IF_NORM=0):

      u, v = a, b

      if GM:
          v = b - np.dot(b, a) / np.dot(a, a) * a

      if IF_NORM:
          u = a / np.linalg.norm(a)
          v = b / np.linalg.norm(b)

      return np.arctan2(v, u)
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_idx(model, rank=2):
      ksi = torch.hstack((model.U, model.V)).T
      ksi = ksi[:, :model.Na[0]]

      readout = model.linear.weight.data
      ksi = torch.vstack((ksi, readout))

      print('ksi', ksi.shape)

      ksi = ksi.cpu().detach().numpy()
      theta = get_theta(ksi[0], ksi[rank])

      return theta.argsort()
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_overlap(model, rates):
      ksi = model.odors.cpu().detach().numpy()
      return rates @ ksi.T / rates.shape[-1]

#+end_src

#+RESULTS:

#+begin_src ipython
  import scipy.stats as stats

  def plot_smooth(data, ax, color):
      mean = data.mean(axis=0)
      ci = smooth.std(axis=0, ddof=1) * 1.96

      # Plot
      ax.plot(mean, color=color)
      ax.fill_between(range(data.shape[1]), mean - ci, mean + ci, alpha=0.25, color=color)

#+end_src

#+RESULTS:

#+begin_src ipython
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:
** plots

#+begin_src ipython
  def plot_rates_selec(rates, idx):
        ordered = rates[..., idx]
        fig, ax = plt.subplots(1, 2, figsize=[2*width, height])
        r_max = 0.2 * np.max(rates[0])

        ax[0].imshow(rates[0].T, aspect='auto', cmap='jet', vmin=0, vmax=r_max)
        # ax[0].axvline((np.array(model.N_STIM_ON) - model.N_STEADY) / model.N_WINDOW, 0, 360, color='w', ls='--')
        # ax[0].axvline((np.array(model.N_STIM_OFF) - model.N_STEADY) / model.N_WINDOW, 0, 360, color='w', ls='--')
        ax[0].set_ylabel('Neuron #')
        ax[0].set_xlabel('Step')

        ax[1].imshow(ordered[0].T, aspect='auto', cmap='jet', vmin=0, vmax=r_max)
        ax[1].set_yticks(np.linspace(0, model.Na[0].cpu().detach(), 5), np.linspace(0, 360, 5).astype(int))
        # ax[1].axvline((np.array(model.N_STIM_ON) - model.N_STEADY) / model.N_WINDOW, 0, 360, 'w', '--')
        # ax[1].axvline((np.array(model.N_STIM_OFF) - model.N_STEADY) / model.N_WINDOW, 0, 360, 'w', '--')
        ax[1].set_ylabel('Pref. Location (°)')
        ax[1].set_xlabel('Step')

        plt.show()
#+end_src

#+RESULTS:

#+begin_src ipython
  def plot_overlap(rates, readout, labels=['A', 'B']):
      overlap =(rates @ readout) / rates.shape[-1]
      print(overlap.shape)

      plt.plot(overlap.T[..., :2], label=labels[0])
      plt.plot(overlap.T[..., 2:], '--', label=labels[1])

      plt.legend(fontsize=10)
      plt.xlabel('Step')
      plt.ylabel('Overlap')

      plt.show()
#+end_src

#+RESULTS:

#+begin_src ipython
  def plot_m0_m1_phi(rates, idx):

      m0, m1, phi = decode_bump(rates[..., idx], axis=-1)
      fig, ax = plt.subplots(1, 3, figsize=[2*width, height])

      ax[0].plot(m0[:2].T)
      ax[0].plot(m0[2:].T, '--')
      #ax[0].set_ylim([0, 360])
      #ax[0].set_yticks([0, 90, 180, 270, 360])
      ax[0].set_ylabel('$\mathcal{F}_0$ (Hz)')
      ax[0].set_xlabel('Step')

      ax[1].plot(m1[:2].T)
      ax[1].plot(m1[2:].T, '--')
      # ax[1].set_ylim([0, 360])
      # ax[1].set_yticks([0, 90, 180, 270, 360])
      ax[1].set_ylabel('$\mathcal{F}_1$ (Hz)')
      ax[1].set_xlabel('Step')

      ax[2].plot(phi[:2].T * 180 / np.pi)
      ax[2].plot(phi[2:].T * 180 / np.pi, '--')
      ax[2].set_ylim([0, 360])
      ax[2].set_yticks([0, 90, 180, 270, 360])
      ax[2].set_ylabel('Phase (°)')
      ax[2].set_xlabel('Step')

      plt.show()
    #+end_src

#+RESULTS:

* Model

#+begin_src ipython
  REPO_ROOT = "/home/leon/models/NeuroFlame"
  conf_name = "config_train.yml"
#+end_src

#+RESULTS:

#+begin_src ipython
  start = perf_counter()
  model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE='cuda', SEED=0, N_BATCH=16)
#+end_src

#+RESULTS:

* Sample Classification
** Training
*** Parameters

#+begin_src ipython
  for name, param in model.named_parameters():
      if param.requires_grad:
          print(name, param.shape)
#+end_src

#+RESULTS:
: U torch.Size([2000, 2])
: V torch.Size([2000, 2])
: linear.weight torch.Size([1, 1600])
: linear.bias torch.Size([1])

#+begin_src ipython
  model.LR_TRAIN = 1
  model.LR_EVAL_WIN = model.T_STIM_OFF[2] - model.T_STIM_ON[0]
  model.lr_eval_win = int(model.LR_EVAL_WIN / model.DT / model.N_WINDOW)
  print(model.lr_eval_win)

  model.DURATION = 6.0
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:
: 50

Testing the network on steps from sample odor offset to test odor onset

#+begin_src ipython
  steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)
  mask = (steps >= (model.N_STIM_OFF[0] - model.N_STEADY)) & (steps <= (model.N_STIM_ON[2] - model.N_STEADY))
  rwd_idx = np.where(mask)[0]
  print(rwd_idx.shape)
#+end_src

#+RESULTS:
: (31,)

*** Inputs and Labels

#+begin_src ipython
  model.N_BATCH = 64

  model.I0[0] = 1.0
  model.I0[1] = 0
  model.I0[2] = 0

  A = model.init_ff_input()

  model.I0[0] = -1.0
  model.I0[1] = 0
  model.I0[2] = 0

  B = model.init_ff_input()

  ff_input = torch.cat((A, B))
  print(ff_input.shape)
#+end_src

#+RESULTS:
: torch.Size([128, 710, 2000])

#+begin_src ipython
  labels_A = torch.ones((model.N_BATCH, rwd_idx.shape[0]))
  labels_B = torch.zeros((model.N_BATCH, rwd_idx.shape[0]))
  labels = torch.cat((labels_A, labels_B))

  print('labels', labels.shape)
#+end_src

#+RESULTS:
: labels torch.Size([128, 31])

*** Run

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: torch.Size([102, 710, 2000]) torch.Size([26, 710, 2000])
: torch.Size([102, 31]) torch.Size([26, 31])

#+begin_src ipython
  # criterion = nn.BCEWithLogitsLoss()
  criterion = DualLoss(alpha=0.5, thresh=1.0, N=model.Na[0], rwd_idx=rwd_idx)

  # SGD, Adam, AdamW
  learning_rate = 0.1
  optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

  num_epochs = 30
  loss, val_loss = 0, 0
#+end_src

#+RESULTS:

#+begin_src ipython
  loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+end_src

#+RESULTS:
#+begin_example
  Epoch 1/30, Training Loss: 42.8731, Validation Loss: 31.0039
  Epoch 2/30, Training Loss: 10.6266, Validation Loss: 12.7745
  Epoch 3/30, Training Loss: 31.8756, Validation Loss: 15.8503
  Epoch 4/30, Training Loss: 5.4168, Validation Loss: 6.7865
  Epoch 5/30, Training Loss: 0.4235, Validation Loss: 0.4204
  Epoch 6/30, Training Loss: 0.4023, Validation Loss: 0.3940
  Epoch 7/30, Training Loss: 0.3957, Validation Loss: 0.5878
  Epoch 8/30, Training Loss: 0.3506, Validation Loss: 0.3451
  Epoch 9/30, Training Loss: 0.2857, Validation Loss: 0.2900
  Epoch 10/30, Training Loss: 0.1424, Validation Loss: 0.1665
  Epoch 11/30, Training Loss: 0.1009, Validation Loss: 0.1469
  Epoch 12/30, Training Loss: 0.1314, Validation Loss: 0.0858
  Epoch 13/30, Training Loss: 0.0000, Validation Loss: 0.0000
  Stopping training as loss has fallen below the threshold: 0.0
#+end_example

** Testing

#+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:

#+begin_src ipython
  plot_con(Wij[model.slices[0], model.slices[0]].cpu().numpy())
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/984a50d539ecd9c251fec28a20136cbcecd37176.png]]

#+begin_src ipython
  model.eval()

  if model.LR_NORM:
      lr = model.lr_kappa * model.lr_mask * (masked_normalize(model.U) @ masked_normalize(model.V).T)
  else:
      lr = model.lr_kappa * model.lr_mask * (model.U @ model.V.T)
  # lr = lr / model.Na[0]
  # lr = lr.clamp(min=-model.Wab_T[0, 0])

  lr = normalize_tensor(lr, 0, model.slices, model.Na)
  lr = normalize_tensor(lr, 1, model.slices, model.Na)

  model.Wab_T = (Wij +lr.T)
  model.Wab_T = clamp_tensor(model.Wab_T, 0, model.slices)
  model.Wab_T = clamp_tensor(model.Wab_T, 1, model.slices)

  if model.IF_STP:
    W_stp_T = model.W_stp_T + lr[model.slices[0], model.slices[0]].T
    W_stp_T = clamp_tensor(W_stp_T, 0, model.slices)

  model.LR_TRAIN=0
#+end_src

#+RESULTS:

#+begin_src ipython
  Mij = model.Wab_T.clone().cpu().detach().numpy()
  plot_con(Mij[model.slices[0], model.slices[0]].T)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/91392c44f0875b8a67aaecf663c905dd775d1260.png]]

#+begin_src ipython
  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 0

  A = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 0

  B = model.init_ff_input()

  ff_input = torch.cat((A, B))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ff_input torch.Size([2, 710, 2000])

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input, RET_FF=1).cpu().detach().numpy()
  print('rates', rates.shape)
  model.Wab_T = Wij.clone()
#+end_src

#+RESULTS:
: rates (2, 61, 1600)


#+begin_src ipython
  idx = get_idx(model, 2)
#+end_src

#+RESULTS:
: ksi torch.Size([5, 1600])

#+begin_src ipython
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/0ae3a7a32c354ad8499a8a1400cd765eb1547b17.png]]

#+begin_src ipython
    readout = model.linear.weight.data.cpu().detach().numpy()[0]
    plot_overlap(rates, readout, labels=['A', 'B'])
#+end_src

#+RESULTS:
:RESULTS:
: (2, 61)
[[file:./.ob-jupyter/3c466450b56bc72cfc249ca11ef87fe8b3631941.png]]
:END:

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a9656a5346066343135a5ec5a0eb16884b487be8.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* DPA
** Training
*** Parameters

#+begin_src ipython
  model.LR_TRAIN = 1
  model.LR_EVAL_WIN = model.T_STIM_OFF[2] - model.T_STIM_ON[2]
  model.lr_eval_win = int(model.LR_EVAL_WIN / model.DT / model.N_WINDOW)

  model.DURATION = 6.0
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

Here we only evaluate performance from test onset to test offset

#+begin_src ipython
  steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)
  mask = (steps >= (model.N_STIM_ON[2] - model.N_STEADY)) & (steps < (model.N_STIM_OFF[2] - model.N_STEADY))
  rwd_idx = np.where(mask)[0]
#+end_src

#+RESULTS:

*** Inputs and Labels

#+begin_src ipython
  model.N_BATCH = 64

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 1

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = -1

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = -1

  BD_pair = model.init_ff_input()

  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ff_input torch.Size([256, 710, 2000])

 #+begin_src ipython
  labels_pair = torch.ones((2 * model.N_BATCH, model.lr_eval_win))
  labels_unpair = torch.zeros((2 * model.N_BATCH, model.lr_eval_win))

  labels = torch.cat((labels_pair, labels_unpair))
  print('labels', labels.shape)
#+end_src

#+RESULTS:
: labels torch.Size([256, 10])

#+RESULTS:

*** Run

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: torch.Size([204, 710, 2000]) torch.Size([52, 710, 2000])
: torch.Size([204, 10]) torch.Size([52, 10])

#+begin_src ipython
  # Loss
  # criterion = nn.BCEWithLogitsLoss()
  criterion = DualLoss(alpha=0.5, thresh=1.0, N=model.Na[0], rwd_idx=rwd_idx)

  # Optimizer: SGD, Adam, AdamW
  learning_rate = 0.1
  optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
#+end_src

#+RESULTS:

#+begin_src ipython
  num_epochs = 30
  loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+End_src

#+RESULTS:
: Epoch 1/30, Training Loss: 0.0991, Validation Loss: 0.7371
: Epoch 2/30, Training Loss: 0.1303, Validation Loss: 0.0000
: Stopping training as loss has fallen below the threshold: 0.0

#+begin_src ipython
  plt.plot(loss)
  plt.plot(val_loss)
  plt.xlabel('epochs')
  plt.ylabel('Loss')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/236a9a24d3240cd89688e3e1f43f9a6a306f354e.png]]

** Testing

#+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:

#+begin_src ipython
  model.eval()
  if model.LR_NORM:
      lr = model.lr_kappa * model.lr_mask * (masked_normalize(model.U) @ masked_normalize(model.V).T)
  else:
      lr = model.lr_kappa * model.lr_mask * (model.U @ model.V.T)

  # lr = lr / model.Na[0]
  # lr = lr.clamp(min=-model.Wab_T[0, 0])

  lr = normalize_tensor(lr, 0, model.slices, model.Na)
  lr = normalize_tensor(lr, 1, model.slices, model.Na)

  model.Wab_T = (Wij +lr.T)
  model.Wab_T = clamp_tensor(model.Wab_T, 0, model.slices)
  model.Wab_T = clamp_tensor(model.Wab_T, 1, model.slices)

  if model.IF_STP:
    W_stp_T = model.W_stp_T + lr[model.slices[0], model.slices[0]].T
    W_stp_T = clamp_tensor(W_stp_T, 0, model.slices)

  model.N_BATCH = 1
  model.VERBOSE=1
  model.LR_TRAIN=0
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1
  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 1

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = -1

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = -1

  BD_pair = model.init_ff_input()

  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ff_input torch.Size([4, 710, 2000])

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input, RET_FF=1).cpu().detach().numpy()
  model.Wab_T = Wij
  print(rates.shape)
#+end_src

#+RESULTS:
: (4, 61, 1600)

#+begin_src ipython
  idx = get_idx(model, 2)
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
:RESULTS:
: ksi torch.Size([5, 1600])
[[file:./.ob-jupyter/08e6935a20ae836741a77adff31526e47236a6e2.png]]
:END:

#+begin_src ipython
    readout = model.linear.weight.data.cpu().detach().numpy()[0]
    plot_overlap(rates, readout, labels=['pair', 'unpair'])
#+end_src

#+RESULTS:
:RESULTS:
: (4, 61)
[[file:./.ob-jupyter/ee69197436849125b6932d877cf9f82c4c22f4eb.png]]
:END:

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c3b077c5e7f0b57d7878e509293a6e90ee3739ad.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Go/NoGo
** Training

#+begin_src ipython
  model.LR_TRAIN=1
  model.LR_EVAL_WIN = 1
  model.lr_eval_win = int(model.LR_EVAL_WIN / model.DT / model.N_WINDOW+1)
  print(model.lr_eval_win)

  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:
: 11

#+begin_src ipython
  steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)
  mask = (steps >= (model.N_STIM_OFF[1] - model.N_STEADY)) & (steps <= (model.N_STIM_ON[2] - model.N_STEADY))

  rwd_idx = np.where(mask)[0]
  model.lr_eval_win = rwd_idx.shape[0]
#+end_src

#+RESULTS:

#+begin_src ipython
  # for param in model.linear.parameters():
  #     param.requires_grad = False
#+end_src

#+RESULTS:

#+begin_src ipython
  for name, param in model.named_parameters():
      if param.requires_grad:
          print(name, param.shape)
#+end_src

#+RESULTS:
: U torch.Size([2000, 2])
: V torch.Size([2000, 2])
: linear.weight torch.Size([1, 1600])
: linear.bias torch.Size([1])

#+begin_src ipython
  # switching sample and distractor odors
  odors = model.odors.clone()
  model.odors[0] = odors[1]

  model.N_BATCH = 64

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 0

  Go = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 0

  NoGo = model.init_ff_input()

  ff_input = torch.cat((Go, NoGo))
  print(ff_input.shape)
  model.odors[0] = odors[0]
#+end_src

#+RESULTS:
: torch.Size([128, 710, 2000])

#+begin_src ipython
  labels_Go = torch.ones((model.N_BATCH, model.lr_eval_win))
  labels_NoGo = torch.zeros((model.N_BATCH, model.lr_eval_win))
  labels = torch.cat((labels_Go, labels_NoGo))

  print('labels', labels.shape)
#+end_src

#+RESULTS:
: labels torch.Size([128, 11])

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: torch.Size([102, 710, 2000]) torch.Size([26, 710, 2000])
: torch.Size([102, 11]) torch.Size([26, 11])

#+begin_src ipython
  # criterion = nn.BCEWithLogitsLoss()
  criterion = DualLoss(alpha=0.5, thresh=1.0, N=model.Na[0], rwd_idx=rwd_idx)

  # SGD, Adam, AdamW
  learning_rate = 0.1
  optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
#+end_src

#+RESULTS:

#+begin_src ipython
  num_epochs = 30
  loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
  # switching back sample and distractor odors
  model.odors[0] = odors[0]
#+end_src
#+RESULTS:
#+begin_example
  Epoch 1/30, Training Loss: 37.6953, Validation Loss: 23.1774
  Epoch 2/30, Training Loss: 2.1198, Validation Loss: 3.3160
  Epoch 3/30, Training Loss: 0.4589, Validation Loss: 5.9065
  Epoch 4/30, Training Loss: 0.4438, Validation Loss: 0.4417
  Epoch 5/30, Training Loss: 0.4176, Validation Loss: 0.4246
  Epoch 6/30, Training Loss: 0.4120, Validation Loss: 0.4011
  Epoch 7/30, Training Loss: 0.3752, Validation Loss: 0.3819
  Epoch 8/30, Training Loss: 0.3654, Validation Loss: 0.3638
  Epoch 9/30, Training Loss: 0.3501, Validation Loss: 0.3436
  Epoch 10/30, Training Loss: 0.3146, Validation Loss: 0.3193
  Epoch 11/30, Training Loss: 0.2583, Validation Loss: 0.2853
  Epoch 12/30, Training Loss: 0.2442, Validation Loss: 0.2360
  Epoch 13/30, Training Loss: 0.0421, Validation Loss: 0.1708
  Epoch 14/30, Training Loss: 0.2089, Validation Loss: 0.1540
  Epoch 15/30, Training Loss: 0.1301, Validation Loss: 0.1260
  Epoch 16/30, Training Loss: 0.0665, Validation Loss: 0.0951
  Epoch 17/30, Training Loss: 0.0239, Validation Loss: 0.0572
  Epoch 18/30, Training Loss: 0.0241, Validation Loss: 0.0095
  Epoch 19/30, Training Loss: 0.0000, Validation Loss: 0.0000
  Stopping training as loss has fallen below the threshold: 0.0
#+end_example

#+begin_src ipython
  plt.plot(loss)
  plt.plot(val_loss)
  plt.xlabel('epochs')
  plt.ylabel('Loss')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/0d762f493e5e1ea61577c24fa94e3e86d458ce00.png]]

** Testing

 #+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:

#+begin_src ipython
  model.eval()
  if model.LR_NORM:
      lr = model.lr_kappa * model.lr_mask * (masked_normalize(model.U) @ masked_normalize(model.V).T)
  else:
      lr = model.lr_kappa * model.lr_mask * (model.U @ model.V.T)

  # lr = lr / model.Na[0]
  # lr = lr.clamp(min=-model.Wab_T[0, 0])

  lr = normalize_tensor(lr, 0, model.slices, model.Na)
  lr = normalize_tensor(lr, 1, model.slices, model.Na)

  model.Wab_T = (Wij +lr.T)
  model.Wab_T = clamp_tensor(model.Wab_T, 0, model.slices)
  model.Wab_T = clamp_tensor(model.Wab_T, 1, model.slices)

  if model.IF_STP:
    W_stp_T = model.W_stp_T + lr[model.slices[0], model.slices[0]].T
    W_stp_T = clamp_tensor(W_stp_T, 0, model.slices)

  model.N_BATCH = 1
  model.VERBOSE=1
  model.LR_TRAIN=0
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1
  model.DURATION = 3
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

#+begin_src ipython
  odors = model.odors.clone()
  model.odors[0] = odors[1]
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 0

  A = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 0

  B = model.init_ff_input()

  ff_input = torch.cat((A, B))
  print('ff_input', ff_input.shape)
  model.odors[0] = odors[0]
#+end_src

#+RESULTS:
: ff_input torch.Size([2, 410, 2000])

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input, RET_FF=1).cpu().detach().numpy()
  model.Wab_T = Wij
  print(rates.shape)
#+end_src

#+RESULTS:
: (2, 31, 1600)

#+begin_src ipython
  idx = get_idx(model, 2)
#+end_src

#+RESULTS:
: ksi torch.Size([5, 1600])

#+begin_src ipython
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/8430001da74097faea8bb334c10f1b343dfb79ca.png]]

#+begin_src ipython
    readout = model.linear.weight.data.cpu().detach().numpy()[0]
    plot_overlap(rates, readout, labels=['Go', 'NoGo'])
#+end_src

#+RESULTS:
:RESULTS:
: (2, 31)
[[file:./.ob-jupyter/9b108afa054e147d787dcec0472c4698408bd36e.png]]
:END:

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/0bdb00219297b5566ecc4d2412b42f2048307ee1.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Dual
** Testing

 #+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:

#+begin_src ipython
  model.eval()

  if model.LR_NORM:
      lr = model.lr_kappa * model.lr_mask * (masked_normalize(model.U) @ masked_normalize(model.V).T)
  else:
      lr = model.lr_kappa * model.lr_mask * (model.U @ model.V.T)
  # lr = lr / model.Na[0]
  # lr = lr.clamp(min=-model.Wab_T[0, 0])

  lr = normalize_tensor(lr, 0, model.slices, model.Na)
  lr = normalize_tensor(lr, 1, model.slices, model.Na)

  model.Wab_T = (Wij +lr.T)
  model.Wab_T = clamp_tensor(model.Wab_T, 0, model.slices)
  model.Wab_T = clamp_tensor(model.Wab_T, 1, model.slices)

  if model.IF_STP:
    W_stp_T = model.W_stp_T + lr[model.slices[0], model.slices[0]].T
    W_stp_T = clamp_tensor(W_stp_T, 0, model.slices)

  model.LR_TRAIN=0
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1
  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 1

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = -1

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = -1

  BD_pair = model.init_ff_input()

  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ff_input torch.Size([4, 710, 2000])

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input).cpu().detach().numpy()
  model.Wab_T = Wij
  print(rates.shape)
#+end_src

#+RESULTS:
: (4, 61, 1600)

#+begin_src ipython
  idx = get_idx(model, 2)
  ordered = rates[..., idx]
  m0, m1, phi = decode_bump(ordered, axis=-1)
#+end_src

#+RESULTS:
: ksi torch.Size([5, 1600])

#+begin_src ipython
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2185340a765237ec95f8a6bb12877a7dc7011b31.png]]

#+begin_src ipython
    readout = model.linear.weight.data.cpu().detach().numpy()[0]
    plot_overlap(rates, readout, labels=['pair', 'unpair'])
#+end_src

#+RESULTS:
:RESULTS:
: (4, 61)
[[file:./.ob-jupyter/fded109dfe955e88174810e754a8df5e591d98dd.png]]
:END:

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/51ea9841ca021d9338e03115c331e57945aa8795.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

** Training

#+begin_src ipython
  model.LR_TRAIN = 1
  model.LR_EVAL_WIN = 1
  model.lr_eval_win = int(model.LR_EVAL_WIN / model.DT / model.N_WINDOW)
  print(model.lr_eval_win)

  model.DURATION = 6.0
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:
: 10

#+begin_src ipython
  steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)
  print(steps.shape)

  mask = (steps >= (model.N_STIM_ON[2] - model.N_STEADY)) & (steps <= (model.N_STIM_OFF[2] - model.N_STEADY))
  rwd_idx = np.where(mask)[0]
  print(rwd_idx)

  mask = (steps >= (model.N_STIM_OFF[1] - model.N_STEADY)) & (steps <= (model.N_STIM_ON[2] - model.N_STEADY))
  cue_idx = np.where(mask)[0]
  print(cue_idx)
#+end_src

#+RESULTS:
: (61,)
: [50 51 52 53 54 55 56 57 58 59 60]
: [40 41 42 43 44 45 46 47 48 49 50]

#+begin_src ipython
  for param in model.linear.parameters():
       param.requires_grad = True
#+end_src

#+RESULTS:

#+begin_src ipython
  for name, param in model.named_parameters():
      if param.requires_grad:
          print(name, param.shape)
#+end_src
#+RESULTS:
: U torch.Size([2000, 2])
: V torch.Size([2000, 2])
: linear.weight torch.Size([1, 1600])
: linear.bias torch.Size([1])

#+begin_src ipython
  model.N_BATCH = 64

  model.lr_eval_win = np.max( (rwd_idx.shape[0], cue_idx.shape[0]))

  ff_input = []
  labels = np.zeros((2, 12, model.N_BATCH, model.lr_eval_win))
  l=0
  for i in [-1, 1]:
      for j in [-1, 0, 1]:
          for k in [1, -1]:

              model.I0[0] = i
              model.I0[1] = j
              model.I0[2] = k

              if i==k: # Pair Trials
                  labels[0, l] = np.ones((model.N_BATCH, model.lr_eval_win))
              else: # Unpair Trials
                  labels[0, l] = np.ones((model.N_BATCH, model.lr_eval_win))

              if j==1: # Go
                  labels[1, l] = np.ones((model.N_BATCH, model.lr_eval_win))
              if j==-1: # NoGo
                  labels[1, l] = np.ones((model.N_BATCH, model.lr_eval_win))

              l+=1

              ff_input.append(model.init_ff_input())

  labels = torch.tensor(labels, dtype=torch.float, device='cuda').reshape(2, -1, model.lr_eval_win).transpose(0, 1)
  ff_input = torch.vstack(ff_input)
  print('ff_input', ff_input.shape, 'labels', labels.shape)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: OutOfMemoryError                          Traceback (most recent call last)
: Cell In[122], line 31
:      28             ff_input.append(model.init_ff_input())
:      30 labels = torch.tensor(labels, dtype=torch.float, device='cuda').reshape(2, -1, model.lr_eval_win).transpose(0, 1)
: ---> 31 ff_input = torch.vstack(ff_input)
:      32 print('ff_input', ff_input.shape, 'labels', labels.shape)
:
: OutOfMemoryError: CUDA out of memory. Tried to allocate 8.13 GiB. GPU 0 has a total capacity of 23.50 GiB of which 3.79 GiB is free. Process 136878 has 318.00 MiB memory in use. Including non-PyTorch memory, this process has 19.37 GiB memory in use. Of the allocated memory 17.88 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
:END:

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
  ---------------------------------------------------------------------------
  ValueError                                Traceback (most recent call last)
  Cell In[123], line 2
        1 batch_size = 16
  ----> 2 train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)

  Cell In[4], line 6, in split_data(X, Y, train_perc, batch_size)
        3 def split_data(X, Y, train_perc=0.8, batch_size=32):
        5   if Y.ndim==3:
  ----> 6     X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
        7                                                         train_size=train_perc,
        8                                                         stratify=Y[:, 0, 0].cpu().numpy(),
        9                                                         shuffle=True)
       10   else:
       11     X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
       12                                                         train_size=train_perc,
       13                                                         stratify=Y[:, 0].cpu().numpy(),
       14                                                         shuffle=True)

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
      208 try:
      209     with config_context(
      210         skip_parameter_validation=(
      211             prefer_skip_nested_validation or global_skip_validation
      212         )
      213     ):
  --> 214         return func(*args, **kwargs)
      215 except InvalidParameterError as e:
      216     # When the function is just a wrapper around an estimator, we allow
      217     # the function to delegate validation to the estimator, but we replace
      218     # the name of the estimator by the name of the function in the error
      219     # message to avoid confusion.
      220     msg = re.sub(
      221         r"parameter of \w+ must be",
      222         f"parameter of {func.__qualname__} must be",
      223         str(e),
      224     )

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2646, in train_test_split(test_size, train_size, random_state, shuffle, stratify, *arrays)
     2643 if n_arrays == 0:
     2644     raise ValueError("At least one array required as input")
  -> 2646 arrays = indexable(*arrays)
     2648 n_samples = _num_samples(arrays[0])
     2649 n_train, n_test = _validate_shuffle_split(
     2650     n_samples, test_size, train_size, default_test_size=0.25
     2651 )

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:453, in indexable(*iterables)
      434 """Make arrays indexable for cross-validation.
      435
      436 Checks consistent length, passes through None, and ensures that everything
     (...)
      449     sparse matrix, or dataframe) or `None`.
      450 """
      452 result = [_make_indexable(X) for X in iterables]
  --> 453 check_consistent_length(*result)
      454 return result

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:407, in check_consistent_length(*arrays)
      405 uniques = np.unique(lengths)
      406 if len(uniques) > 1:
  --> 407     raise ValueError(
      408         "Found input variables with inconsistent numbers of samples: %r"
      409         % [int(l) for l in lengths]
      410     )

  ValueError: Found input variables with inconsistent numbers of samples: [12, 768]
#+end_example
:END:

#+begin_src ipython
  # criterion = nn.BCEWithLogitsLoss()
  criterion = DualLoss(alpha=0.5, thresh=1.0, N=model.Na[0], cue_idx=cue_idx, rwd_idx=rwd_idx)

  # SGD, Adam, AdamW
  learning_rate = 0.1
  optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
#+end_src

#+RESULTS:

#+begin_src ipython
  num_epochs = 30
  loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+end_src
#+RESULTS:
: 495f0abf-a776-47bd-8e86-44bdc314cd40

** Re-Testing

#+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:

#+begin_src ipython
  model.eval()
  if model.LR_NORM:
      lr = model.lr_kappa * model.lr_mask * (masked_normalize(model.U) @ masked_normalize(model.V).T) / (1.0 * model.Na[0])
  else:
      lr = model.lr_kappa * model.lr_mask * (model.U @ model.V.T) / (1.0 * model.Na[0])

  lr = lr.clamp(min=-model.Wab_T[0, 0])
  model.Wab_T = Wij + lr.T

  model.N_BATCH = 1
  model.VERBOSE=1
  model.LR_TRAIN=0
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1
  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 1

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = -1

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = -1

  BD_pair = model.init_ff_input()

  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ff_input torch.Size([4, 710, 2000])

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input).cpu().detach().numpy()
  model.Wab_T = Wij
  print(rates.shape)
#+end_src

#+RESULTS:
: (4, 61, 1600)

#+begin_src ipython
  idx = get_idx(model, 2)
#+end_src

#+RESULTS:
: ksi torch.Size([5, 1600])

#+begin_src ipython
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/6581daa6193dba178737887eb28a27115b26ebb1.png]]

#+begin_src ipython
    readout = model.linear.weight.data[0].cpu().detach().numpy()
    plot_overlap(rates, readout)
#+end_src

#+RESULTS:
:RESULTS:
: (4, 61)
[[file:./.ob-jupyter/a3f13427065b74d083a0c5b6dd370315098956e3.png]]
:END:

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/e1dff1fa02ec53f57cc7ad64e6c144b2543b0267.png]]

#+begin_src ipython

#+end_src

#+RESULTS:
