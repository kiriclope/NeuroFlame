#+STARTUP: fold
#+TITLE: Training Low Rank RNNs
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session dual :kernel torch

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run ../notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'

  REPO_ROOT = "/home/leon/models/NeuroFlame"
  pal = sns.color_palette("tab10")
#+end_src

#+RESULTS:
:RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python
: <Figure size 600x370.82 with 0 Axes>
:END:

* Imports

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader
#+end_src

#+RESULTS:

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  import pandas as pd
  import torch.nn as nn
  from time import perf_counter
  from scipy.stats import circmean

  from src.network import Network
  from src.plot_utils import plot_con
  from src.decode import decode_bump, circcvl
  from src.lr_utils import masked_normalize
#+end_src

#+RESULTS:

* Helpers
** Data Split

#+begin_src ipython
  from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

  def split_data(X, Y, train_perc=0.8, batch_size=32):

    if Y.ndim==3:
      X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                          train_size=train_perc,
                                                          stratify=Y[:, 0, 0].cpu().numpy(),
                                                          shuffle=True)
    else:
      X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                          train_size=train_perc,
                                                          stratify=Y[:, 0].cpu().numpy(),
                                                          shuffle=True)
    print(X_train.shape, X_test.shape)
    print(Y_train.shape, Y_test.shape)

    train_dataset = TensorDataset(X_train, Y_train)
    val_dataset = TensorDataset(X_test, Y_test)

    # Create data loaders
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader
#+end_src

#+RESULTS:

** Optimization

#+begin_src ipython
  def train(dataloader, model, loss_fn, optimizer, penalty=None, lbd=1):
      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      model.train()
      for batch, (X, y) in enumerate(dataloader):
          X, y = X.to(device), y.to(device)
          # Compute prediction error
          y_pred = model(X)

          # if y.ndim==y_pred.ndim:
          loss = loss_fn(y_pred, y)

          if penalty is not None:
              reg_loss = 0
              for param in model.parameters():
                  if penalty=='l1':
                      reg_loss += torch.sum(torch.abs(param))
                  else:
                      reg_loss += torch.sum(torch.square(param))

                  loss = loss + lbd * reg_loss

          # Backpropagation
          loss.backward()
          # Clip gradients (norm)
          # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

          # Or clip gradients (value)
          #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)

          optimizer.step()
          optimizer.zero_grad()

      return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def test(dataloader, model, loss_fn):
      size = len(dataloader.dataset)
      num_batches = len(dataloader)

      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      # Validation loop.
      model.eval()
      val_loss = 0.0
      with torch.no_grad():
          for data, targets in dataloader:
              data, targets = data.to(device), targets.to(device)

              outputs = model(data)
              loss = loss_fn(outputs, targets)
              val_loss += loss.item() * data.size(0)

          val_loss /= size

      return val_loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def run_optim(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100, penalty=None, lbd=1, thresh=.005):
      scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
      # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)
      # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      model.to(device)

      loss_list = []
      val_loss_list = []

      # Training loop.
      for epoch in range(num_epochs):
          loss = train(train_loader, model, loss_fn, optimizer, penalty, lbd)
          val_loss = test(val_loader, model, loss_fn)
          scheduler.step(val_loss)

          loss_list.append(loss.item())
          val_loss_list.append(val_loss)

          # if epoch % int(num_epochs  / 10) == 0:
          print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

          if val_loss < thresh:
              print(f'Stopping training as loss has fallen below the threshold: {val_loss}')
              break

          if val_loss > 300:
              print(f'Stopping training as loss is too high: {val_loss}')
              break

          if torch.isnan(loss):
              print(f'Stopping training as loss is NaN.')
              break

      return loss_list, val_loss_list
#+end_src

#+RESULTS:

** Loss

#+begin_src ipython
  def accuracy(y_pred, labels):
    # Assuming 'outputs' are logits from your model (raw scores before sigmoid)
    predicted = (y_pred > 0).float()  # Convert to 0 or 1 based on comparison with 0
    # 'labels' should be your ground truth labels for the binary classification, also in 0 or 1
    correct = (predicted == labels).sum().item()
    accuracy = correct / labels.size(0)
    return accuracy
#+end_src

#+RESULTS:

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class SignBCELoss(nn.Module):
      def __init__(self, alpha=0.1, thresh=1.0, N=1000):
          super(SignBCELoss, self).__init__()
          self.alpha = alpha
          self.thresh = thresh
          self.N = N

          self.bce_with_logits = nn.BCEWithLogitsLoss()

      def forward(self, readout, targets):
          bce_loss = self.bce_with_logits(readout, targets)

          # sign_overlap = torch.sign(2 * targets - 1) * readout / (1.0 * self.N)

          mean_activation = readout.mean(dim=1).unsqueeze(-1)
          sign_overlap = torch.sign(2 * targets - 1) * mean_activation / (1.0 * self.N)

          # sign_loss = F.relu(self.thresh - sign_overlap).mean()
          sign_loss = torch.sigmoid(self.thresh -sign_overlap).mean()

          combined_loss = (1-self.alpha) * bce_loss + self.alpha * sign_loss
          return combined_loss
#+end_src

#+RESULTS:

#+begin_src ipython
  class CosineLoss(nn.Module):
      def __init__(self, readout):
          super(CosineLoss, self).__init__()
          self.cosine_similarity = nn.CosineSimilarity(dim=-1)
          self.readout = readout

      def forward(self, rates, target):
          # Calculate cosine similarity
          cosine_sim = self.cosine_similarity(torch.sign(target) * rates, readout)
          # Calculate the loss as 1 - cosine_similarity
          loss = 1 - cosine_sim
          # Return the mean loss over the batch
          return loss.mean()
#+end_src

#+RESULTS:

#+begin_src ipython
  class DualLoss(nn.Module):
      def __init__(self, alpha=0.1, thresh=1.0, N=1000, cue_idx=[], rwd_idx=-1):
          super(DualLoss, self).__init__()
          self.alpha = alpha
          self.thresh = thresh
          self.N = N

          self.cue_idx = torch.tensor(cue_idx, dtype=torch.int, device='cuda')
          self.rwd_idx = torch.tensor(rwd_idx, dtype=torch.int, device='cuda')

          self.loss = SignBCELoss(self.alpha, self.thresh, self.N)

      def forward(self, readout, targets):

          is_empty = self.cue_idx.numel() == 0
          if is_empty:
              self.DPA_loss = self.loss(readout[:, self.rwd_idx], targets)
              return self.DPA_loss
          else:
              self.DPA_loss = self.loss(readout[:, self.rwd_idx], targets[:, 0, :self.rwd_idx.shape[0]])
              self.DRT_loss = self.loss(readout[:, self.cue_idx], targets[:, 1, :self.cue_idx.shape[0]])
              return (self.DPA_loss + self.DRT_loss) / 2.0
#+end_src

#+RESULTS:

** Other

#+begin_src ipython
  def get_theta(a, b, GM=0, IF_NORM=0):

      u, v = a, b

      if GM:
          v = b - np.dot(b, a) / np.dot(a, a) * a

      if IF_NORM:
          u = a / np.linalg.norm(a)
          v = b / np.linalg.norm(b)

      return np.arctan2(v, u)
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_idx(model, rank=2):
      ksi = torch.hstack((model.U, model.V)).T
      ksi = ksi[:, :model.Na[0]]

      readout = model.linear.weight.data
      ksi = torch.vstack((ksi, readout))

      print('ksi', ksi.shape)

      ksi = ksi.cpu().detach().numpy()
      theta = get_theta(ksi[0], ksi[rank])

      return theta.argsort()
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_overlap(model, rates):
      ksi = model.odors.cpu().detach().numpy()
      return rates @ ksi.T / rates.shape[-1]

#+end_src

#+RESULTS:

#+begin_src ipython
  import scipy.stats as stats

  def plot_smooth(data, ax, color):
      mean = data.mean(axis=0)
      ci = smooth.std(axis=0, ddof=1) * 1.96

      # Plot
      ax.plot(mean, color=color)
      ax.fill_between(range(data.shape[1]), mean - ci, mean + ci, alpha=0.25, color=color)

#+end_src

#+RESULTS:

#+begin_src ipython
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:
** plots

#+begin_src ipython
  def plot_rates_selec(rates, idx):
        ordered = rates[..., idx]
        fig, ax = plt.subplots(1, 2, figsize=[2*width, height])
        r_max = 0.2 * np.max(rates[0])

        ax[0].imshow(rates[0].T, aspect='auto', cmap='jet', vmin=0, vmax=r_max)
        # ax[0].axvline((np.array(model.N_STIM_ON) - model.N_STEADY) / model.N_WINDOW, 0, 360, color='w', ls='--')
        # ax[0].axvline((np.array(model.N_STIM_OFF) - model.N_STEADY) / model.N_WINDOW, 0, 360, color='w', ls='--')
        ax[0].set_ylabel('Neuron #')
        ax[0].set_xlabel('Step')

        ax[1].imshow(ordered[0].T, aspect='auto', cmap='jet', vmin=0, vmax=r_max)
        ax[1].set_yticks(np.linspace(0, model.Na[0].cpu().detach(), 5), np.linspace(0, 360, 5).astype(int))
        # ax[1].axvline((np.array(model.N_STIM_ON) - model.N_STEADY) / model.N_WINDOW, 0, 360, 'w', '--')
        # ax[1].axvline((np.array(model.N_STIM_OFF) - model.N_STEADY) / model.N_WINDOW, 0, 360, 'w', '--')
        ax[1].set_ylabel('Pref. Location (°)')
        ax[1].set_xlabel('Step')

        plt.show()
#+end_src

#+RESULTS:

#+begin_src ipython
  def plot_overlap(rates, readout, labels=['A', 'B']):
      overlap =(rates @ readout) / rates.shape[-1]
      print(overlap.shape)

      plt.plot(overlap.T[..., :2], label=labels[0])
      plt.plot(overlap.T[..., 2:], '--', label=labels[1])

      plt.legend(fontsize=10)
      plt.xlabel('Step')
      plt.ylabel('Overlap')

      plt.show()
#+end_src

#+RESULTS:

#+begin_src ipython
  def plot_m0_m1_phi(rates, idx):

      m0, m1, phi = decode_bump(rates[..., idx], axis=-1)
      fig, ax = plt.subplots(1, 3, figsize=[2*width, height])

      ax[0].plot(m0[:2].T)
      ax[0].plot(m0[2:].T, '--')
      #ax[0].set_ylim([0, 360])
      #ax[0].set_yticks([0, 90, 180, 270, 360])
      ax[0].set_ylabel('$\mathcal{F}_0$ (Hz)')
      ax[0].set_xlabel('Step')

      ax[1].plot(m1[:2].T)
      ax[1].plot(m1[2:].T, '--')
      # ax[1].set_ylim([0, 360])
      # ax[1].set_yticks([0, 90, 180, 270, 360])
      ax[1].set_ylabel('$\mathcal{F}_1$ (Hz)')
      ax[1].set_xlabel('Step')

      ax[2].plot(phi[:2].T * 180 / np.pi)
      ax[2].plot(phi[2:].T * 180 / np.pi, '--')
      ax[2].set_ylim([0, 360])
      ax[2].set_yticks([0, 90, 180, 270, 360])
      ax[2].set_ylabel('Phase (°)')
      ax[2].set_xlabel('Step')

      plt.show()
    #+end_src

#+RESULTS:

* Model

#+begin_src ipython
  REPO_ROOT = "/home/leon/models/NeuroFlame"
  conf_name = "config_train.yml"
#+end_src

#+RESULTS:

#+begin_src ipython
  start = perf_counter()
  model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE='cuda', SEED=0, N_BATCH=16)
#+end_src

#+RESULTS:

#+begin_src ipython
  print()
  for name, param in model.named_parameters():
      if param.requires_grad:
          print(name, param.shape)
#+end_src

#+RESULTS:
:
: U torch.Size([2000, 2])
: V torch.Size([2000, 2])
: linear.weight torch.Size([1, 1600])

* Sample Classification
** Training
*** Parameters

#+begin_src ipython
  model.LR_TRAIN = 1
  model.LR_EVAL_WIN = model.T_STIM_OFF[2] - model.T_STIM_ON[0]
  model.lr_eval_win = int(model.LR_EVAL_WIN / model.DT / model.N_WINDOW)
  print(model.lr_eval_win)

  model.DURATION = 6.0
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:
: 50

Testing the network on steps from sample odor offset to test odor onset

#+begin_src ipython
  steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)
  mask = (steps >= (model.N_STIM_OFF[0] - model.N_STEADY)) & (steps <= (model.N_STIM_ON[2] - model.N_STEADY))
  rwd_idx = np.where(mask)[0]
  print(rwd_idx.shape)
#+end_src

#+RESULTS:
: (31,)

*** Inputs and Labels

#+begin_src ipython
  model.N_BATCH = 64

  model.I0[0] = 1.0
  model.I0[1] = 0
  model.I0[2] = 0

  A = model.init_ff_input()

  model.I0[0] = -1.0
  model.I0[1] = 0
  model.I0[2] = 0

  B = model.init_ff_input()

  ff_input = torch.cat((A, B))
  print(ff_input.shape)
#+end_src

#+RESULTS:
: torch.Size([128, 710, 2000])

#+begin_src ipython
  labels_A = torch.ones((model.N_BATCH, rwd_idx.shape[0]))
  labels_B = torch.zeros((model.N_BATCH, rwd_idx.shape[0]))
  labels = torch.cat((labels_A, labels_B))

  print('labels', labels.shape)
#+end_src

#+RESULTS:
: labels torch.Size([128, 31])

*** Run

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: torch.Size([102, 710, 2000]) torch.Size([26, 710, 2000])
: torch.Size([102, 31]) torch.Size([26, 31])

#+begin_src ipython
  # criterion = nn.BCEWithLogitsLoss()
  criterion = DualLoss(alpha=1.0, thresh=2.0, N=model.Na[0], rwd_idx=rwd_idx)

  # SGD, Adam, AdamW
  learning_rate = 0.1
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)

  num_epochs = 30
  loss, val_loss = 0, 0
#+end_src

#+RESULTS:

#+begin_src ipython
  loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+end_src

#+RESULTS:
#+begin_example
  Epoch 1/30, Training Loss: 0.9091, Validation Loss: 0.8177
  Epoch 2/30, Training Loss: 0.6715, Validation Loss: 0.5001
  Epoch 3/30, Training Loss: 0.1667, Validation Loss: 0.5000
  Epoch 4/30, Training Loss: 0.5000, Validation Loss: 0.5000
  Epoch 5/30, Training Loss: 0.3333, Validation Loss: 0.5000
  Epoch 6/30, Training Loss: 0.3333, Validation Loss: 0.5000
  Epoch 7/30, Training Loss: 0.6667, Validation Loss: 0.5000
  Epoch 8/30, Training Loss: 0.3333, Validation Loss: 0.5000
  Epoch 9/30, Training Loss: 0.6667, Validation Loss: 0.5000
  Epoch 10/30, Training Loss: 0.3333, Validation Loss: 0.5000
  Epoch 11/30, Training Loss: 0.6667, Validation Loss: 0.5000
  Epoch 12/30, Training Loss: 0.8333, Validation Loss: 0.5000
  Epoch 13/30, Training Loss: 0.8333, Validation Loss: 0.5000
  Epoch 14/30, Training Loss: 0.5000, Validation Loss: 0.5000
  Epoch 15/30, Training Loss: 0.3333, Validation Loss: 0.5000
  Epoch 16/30, Training Loss: 0.5000, Validation Loss: 0.5000
  Epoch 17/30, Training Loss: 0.1667, Validation Loss: 0.5000
  Epoch 18/30, Training Loss: 0.3333, Validation Loss: 0.5000
  Epoch 19/30, Training Loss: 0.5000, Validation Loss: 0.5000
  Epoch 20/30, Training Loss: 0.6667, Validation Loss: 0.5000
  Epoch 21/30, Training Loss: 0.5000, Validation Loss: 0.5000
  Epoch 22/30, Training Loss: 0.1667, Validation Loss: 0.5000
  Epoch 23/30, Training Loss: 0.5000, Validation Loss: 0.5000
  Epoch 24/30, Training Loss: 0.1667, Validation Loss: 0.5000
  Epoch 25/30, Training Loss: 0.8333, Validation Loss: 0.5000
  Epoch 26/30, Training Loss: 0.5000, Validation Loss: 0.5000
  Epoch 27/30, Training Loss: 0.3333, Validation Loss: 0.5000
  Epoch 28/30, Training Loss: 0.3333, Validation Loss: 0.5000
  Epoch 29/30, Training Loss: 0.5000, Validation Loss: 0.5000
  Epoch 30/30, Training Loss: 0.3333, Validation Loss: 0.5000
#+end_example

** Testing

#+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:

#+begin_src ipython
  model.eval()

  lr = model.lr_kappa * model.lr_mask * (model.U @ model.V.T) / (1.0 * model.Na[0])
  # lr = lr.clamp(min=-model.Wab_T[0, 0])

  model.Wab_T = (Wij +lr.T)
  model.LR_TRAIN=0
#+end_src

#+RESULTS:

#+begin_src ipython
  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 0

  A = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 0

  B = model.init_ff_input()

  ff_input = torch.cat((A, B))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ff_input torch.Size([2, 710, 2000])

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input, RET_FF=1).cpu().detach().numpy()
  model.Wab_T = Wij.clone()

  print('rates', rates.shape)
  idx = get_idx(model, 2)
#+end_src

#+RESULTS:
: rates (2, 61, 1600)
: ksi torch.Size([5, 1600])

#+begin_src ipython
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/0a2bd1719182a0750cc6f8ec25d178282a1cb829.png]]

#+begin_src ipython
    readout = model.linear.weight.data.cpu().detach().numpy()[0]
    plot_overlap(rates, readout, labels=['A', 'B'])
#+end_src

#+RESULTS:
:RESULTS:
: (2, 61)
[[file:./.ob-jupyter/cf0428c818e18418e412dd7e3f1816ddaec88221.png]]
:END:

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/69e3dc7141c58ba219923ed6a708e90579ab2157.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* DPA
** Training
*** Parameters

#+begin_src ipython
  model.LR_TRAIN = 1
  model.LR_EVAL_WIN = model.T_STIM_OFF[2] - model.T_STIM_ON[2]
  model.lr_eval_win = int(model.LR_EVAL_WIN / model.DT / model.N_WINDOW)

  model.DURATION = 6.0
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

Here we only evaluate performance from test onset to test offset

#+begin_src ipython
  steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)
  mask = (steps >= (model.N_STIM_ON[2] - model.N_STEADY)) & (steps < (model.N_STIM_OFF[2] - model.N_STEADY))
  rwd_idx = np.where(mask)[0]
#+end_src

#+RESULTS:

*** Inputs and Labels

#+begin_src ipython
  model.N_BATCH = 96

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 1

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = -1

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = -1

  BD_pair = model.init_ff_input()

  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ff_input torch.Size([384, 710, 2000])

 #+begin_src ipython
  labels_pair = torch.ones((2 * model.N_BATCH, model.lr_eval_win))
  labels_unpair = torch.zeros((2 * model.N_BATCH, model.lr_eval_win))

  labels = torch.cat((labels_pair, labels_unpair))
  print('labels', labels.shape)
#+end_src

#+RESULTS:
: labels torch.Size([384, 10])

#+RESULTS:

*** Run

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: torch.Size([307, 710, 2000]) torch.Size([77, 710, 2000])
: torch.Size([307, 10]) torch.Size([77, 10])

#+begin_src ipython
  # Loss
  # criterion = nn.BCEWithLogitsLoss()
  criterion = DualLoss(alpha=0.5, thresh=2.0, N=model.Na[0], rwd_idx=rwd_idx)

  # Optimizer: SGD, Adam, AdamW
  learning_rate = 0.1
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
#+end_src

#+RESULTS:

#+begin_src ipython
  num_epochs = 30
  loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+End_src

#+RESULTS:
#+begin_example
  Epoch 1/30, Training Loss: 0.4052, Validation Loss: 0.4038
  Epoch 2/30, Training Loss: 0.4014, Validation Loss: 0.3936
  Epoch 3/30, Training Loss: 0.3934, Validation Loss: 0.3914
  Epoch 4/30, Training Loss: 0.3788, Validation Loss: 0.3905
  Epoch 5/30, Training Loss: 0.3887, Validation Loss: 0.3891
  Epoch 6/30, Training Loss: 0.3834, Validation Loss: 0.3866
  Epoch 7/30, Training Loss: 0.3952, Validation Loss: 0.3824
  Epoch 8/30, Training Loss: 0.3345, Validation Loss: 0.3755
  Epoch 9/30, Training Loss: 0.3699, Validation Loss: 0.3707
  Epoch 10/30, Training Loss: 0.3677, Validation Loss: 0.3745
  Epoch 11/30, Training Loss: 3.0816, Validation Loss: 0.3803
  Epoch 12/30, Training Loss: 0.3759, Validation Loss: 1.3083
  Epoch 13/30, Training Loss: 0.4032, Validation Loss: 0.3905
  Epoch 14/30, Training Loss: 0.9012, Validation Loss: 0.3885
  Epoch 15/30, Training Loss: 0.3641, Validation Loss: 0.3842
  Epoch 16/30, Training Loss: 0.3760, Validation Loss: 0.3813
  Epoch 17/30, Training Loss: 0.4088, Validation Loss: 0.3790
  Epoch 18/30, Training Loss: 0.3768, Validation Loss: 0.3762
  Epoch 19/30, Training Loss: 0.3743, Validation Loss: 0.3738
  Epoch 20/30, Training Loss: 0.3633, Validation Loss: 0.3773
  Epoch 21/30, Training Loss: 0.4040, Validation Loss: 0.3751
  Epoch 22/30, Training Loss: 0.3665, Validation Loss: 0.3725
  Epoch 23/30, Training Loss: 0.3628, Validation Loss: 0.3694
#+end_example

#+begin_src ipython
  plt.plot(loss)
  plt.plot(val_loss)
  plt.xlabel('epochs')
  plt.ylabel('Loss')
  plt.show()
#+end_src

#+RESULTS:
: 003bf404-8f2b-4353-8f15-75d3bcfc1ddf

** Testing

#+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:
: f2193e58-c212-41ab-abad-7bb8d60735c3

#+begin_src ipython
  model.eval()
  if model.LR_NORM:
      lr = model.lr_kappa * model.lr_mask * (masked_normalize(model.U) @ masked_normalize(model.V).T) / (1.0 * model.Na[0])
  else:
      lr = model.lr_kappa * model.lr_mask * (model.U @ model.V.T) / (1.0 * model.Na[0])

  lr = lr.clamp(min=-model.Wab_T[0, 0])

  model.Wab_T = Wij + lr.T

  model.N_BATCH = 1
  model.VERBOSE=1
  model.LR_TRAIN=0
#+end_src

#+RESULTS:
: 26f635b2-6b15-4590-a293-fcc540d71624

#+begin_src ipython
  model.N_BATCH = 1
  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:
: a383c549-5b33-49f6-a0cc-c349524beff7

#+begin_src ipython
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 1

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = -1

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = -1

  BD_pair = model.init_ff_input()

  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ad55490d-14c8-42ff-8ec9-0d0a0e6e6ed3

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input, RET_FF=1).cpu().detach().numpy()
  model.Wab_T = Wij
  print(rates.shape)
#+end_src

#+RESULTS:
: 9fc7e50a-4956-415c-878d-6ff59f0ea063

#+begin_src ipython
  idx = get_idx(model, 2)
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
: e9775f6f-9260-41d9-9d5c-a23d94d95f46

#+begin_src ipython
    readout = model.linear.weight.data.cpu().detach().numpy()[0]
    plot_overlap(rates, readout, labels=['pair', 'unpair'])
#+end_src

#+RESULTS:
: 44e0a350-b7d3-4604-aa74-89b415d79844

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
: aa67df69-fdb8-4976-9e40-3860eb97a32b

#+begin_src ipython

#+end_src

#+RESULTS:
: 604b4627-f42b-4d9f-bd47-6f0cc4340424

* Go/NoGo
** Training

#+begin_src ipython
  model.LR_TRAIN=1
  model.LR_EVAL_WIN = 1
  model.lr_eval_win = int(model.LR_EVAL_WIN / model.DT / model.N_WINDOW+1)
  print(model.lr_eval_win)

  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:
: 11

#+begin_src ipython
  steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)
  mask = (steps >= (model.N_STIM_OFF[1] - model.N_STEADY)) & (steps <= (model.N_STIM_ON[2] - model.N_STEADY))

  rwd_idx = np.where(mask)[0]
  model.lr_eval_win = rwd_idx.shape[0]
#+end_src

#+RESULTS:

#+begin_src ipython
  for param in model.linear.parameters():
      param.requires_grad = False
#+end_src

#+RESULTS:

#+begin_src ipython
  for name, param in model.named_parameters():
      if param.requires_grad:
          print(name, param.shape)
#+end_src

#+RESULTS:
: U torch.Size([3000, 2])
: V torch.Size([3000, 2])

#+begin_src ipython
  # switching sample and distractor odors
  odors = model.odors.clone()
  model.odors[0] = odors[1]

  model.N_BATCH = 64

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 0

  Go = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 0

  NoGo = model.init_ff_input()

  ff_input = torch.cat((Go, NoGo))
  print(ff_input.shape)
  model.odors[0] = odors[0]
#+end_src

#+RESULTS:
: torch.Size([128, 710, 3000])

#+begin_src ipython
  labels_Go = torch.ones((model.N_BATCH, model.lr_eval_win))
  labels_NoGo = torch.zeros((model.N_BATCH, model.lr_eval_win))
  labels = torch.cat((labels_Go, labels_NoGo))

  print('labels', labels.shape)
#+end_src

#+RESULTS:
: labels torch.Size([128, 11])

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: torch.Size([102, 710, 3000]) torch.Size([26, 710, 3000])
: torch.Size([102, 11]) torch.Size([26, 11])

#+begin_src ipython
  # criterion = nn.BCEWithLogitsLoss()
  criterion = DualLoss(alpha=0.5, thresh=2.0, N=model.Na[0], rwd_idx=rwd_idx)

  # SGD, Adam, AdamW
  learning_rate = 0.1
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
#+end_src

#+RESULTS:

#+begin_src ipython
  num_epochs = 30
  loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
  # switching back sample and distractor odors
  model.odors[0] = odors[0]
#+end_src
#+RESULTS:
#+begin_example
  Epoch 1/30, Training Loss: 3.6489, Validation Loss: 5.1639
  Epoch 2/30, Training Loss: 0.9587, Validation Loss: 0.9550
  Epoch 3/30, Training Loss: 1.0726, Validation Loss: 0.9484
  Epoch 4/30, Training Loss: 0.9331, Validation Loss: 0.9423
  Epoch 5/30, Training Loss: 0.9318, Validation Loss: 0.9396
  Epoch 6/30, Training Loss: 0.9454, Validation Loss: 0.9380
  Epoch 7/30, Training Loss: 0.9441, Validation Loss: 0.9368
  Epoch 8/30, Training Loss: 0.9358, Validation Loss: 0.9358
  Epoch 9/30, Training Loss: 0.9351, Validation Loss: 0.9349
  Epoch 10/30, Training Loss: 0.9272, Validation Loss: 0.9340
  Epoch 11/30, Training Loss: 0.9332, Validation Loss: 0.9331
  Epoch 12/30, Training Loss: 0.9464, Validation Loss: 0.9321
  Epoch 13/30, Training Loss: 0.9168, Validation Loss: 0.9312
  Epoch 14/30, Training Loss: 0.9301, Validation Loss: 0.9301
  Epoch 15/30, Training Loss: 0.9292, Validation Loss: 0.9290
  Epoch 16/30, Training Loss: 0.9120, Validation Loss: 0.9279
  Epoch 17/30, Training Loss: 0.9268, Validation Loss: 0.9267
  Epoch 18/30, Training Loss: 0.9338, Validation Loss: 0.9253
  Epoch 19/30, Training Loss: 0.9063, Validation Loss: 0.9239
  Epoch 20/30, Training Loss: 0.9317, Validation Loss: 0.9223
  Epoch 21/30, Training Loss: 0.9111, Validation Loss: 0.9206
  Epoch 22/30, Training Loss: 0.9192, Validation Loss: 0.9190
  Epoch 23/30, Training Loss: 0.9285, Validation Loss: 0.9175
  Epoch 24/30, Training Loss: 0.9162, Validation Loss: 0.9162
  Epoch 25/30, Training Loss: 0.9035, Validation Loss: 0.9148
  Epoch 26/30, Training Loss: 0.9018, Validation Loss: 0.9133
  Epoch 27/30, Training Loss: 0.9242, Validation Loss: 0.9118
  Epoch 28/30, Training Loss: 0.8859, Validation Loss: 0.9103
  Epoch 29/30, Training Loss: 0.8965, Validation Loss: 0.9088
  Epoch 30/30, Training Loss: 0.9202, Validation Loss: 0.9073
#+end_example

#+begin_src ipython
  plt.plot(loss)
  plt.plot(val_loss)
  plt.xlabel('epochs')
  plt.ylabel('Loss')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/f3d645d15f5970178060f8dc42f2636aea7037ee.png]]

** Testing

 #+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:

#+begin_src ipython
  model.eval()
  if model.LR_NORM:
      lr = model.lr_kappa * model.lr_mask * (masked_normalize(model.U) @ masked_normalize(model.V).T) / (1.0 * model.Na[0])
  else:
      lr = model.lr_mask * (model.U @ model.V.T) / (1.0 * model.Na[0])

  lr = lr.clamp(min=-model.Wab_T[0, 0])
  model.Wab_T = Wij + lr.T

  model.N_BATCH = 1
  model.LR_TRAIN=0
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1
  model.DURATION = 3
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

#+begin_src ipython
  odors = model.odors.clone()
  model.odors[0] = odors[1]
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 0

  A = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 0

  B = model.init_ff_input()

  ff_input = torch.cat((A, B))
  print('ff_input', ff_input.shape)
  model.odors[0] = odors[0]
#+end_src

#+RESULTS:
: ff_input torch.Size([2, 410, 3000])

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input, RET_FF=1).cpu().detach().numpy()
  model.Wab_T = Wij
  print(rates.shape)
#+end_src

#+RESULTS:
: (2, 31, 2400)

#+begin_src ipython
  idx = get_idx(model, 2)
#+end_src

#+RESULTS:
: ksi torch.Size([5, 2400])

#+begin_src ipython
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2051dcef8f1600e5af5162ddf61d2942a71b5144.png]]

#+begin_src ipython
    readout = model.linear.weight.data.cpu().detach().numpy()[0]
    plot_overlap(rates, readout, labels=['Go', 'NoGo'])
#+end_src

#+RESULTS:
:RESULTS:
: (2, 31)
[[file:./.ob-jupyter/d62062988e58241e293564f3e219c2620d1876bf.png]]
:END:

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b86b25edb44b1198aed6aaa4eb9b5e63bee19e29.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Dual
** Testing

 #+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:

#+begin_src ipython
  model.eval()
  if model.LR_NORM:
      lr = model.lr_kappa * model.lr_mask * (masked_normalize(model.U) @ masked_normalize(model.V).T) / (1.0 * model.Na[0])
  else:
      lr = model.lr_kappa * model.lr_mask * (model.U @ model.V.T) / (1.0 * model.Na[0])

  lr = lr.clamp(min=-model.Wab_T[0, 0])
  model.Wab_T = Wij + lr.T

  model.N_BATCH = 1
  model.VERBOSE=1
  model.LR_TRAIN=0
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1
  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 1

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = -1

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = -1

  BD_pair = model.init_ff_input()

  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ff_input torch.Size([4, 710, 3000])

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input).cpu().detach().numpy()
  model.Wab_T = Wij
  print(rates.shape)
#+end_src

#+RESULTS:
: (4, 61, 2400)

#+begin_src ipython
  idx = get_idx(model, 2)
  ordered = rates[..., idx]
  m0, m1, phi = decode_bump(ordered, axis=-1)
#+end_src

#+RESULTS:
: ksi torch.Size([5, 2400])

#+begin_src ipython
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/182695158ea375fed9d413ab0ea0b2bc960197f3.png]]

#+begin_src ipython
    readout = model.linear.weight.data.cpu().detach().numpy()[0]
    plot_overlap(rates, readout, labels=['pair', 'unpair'])
#+end_src

#+RESULTS:
:RESULTS:
: (4, 61)
[[file:./.ob-jupyter/45477123096aa04a71fa0a6b948874f388bcbfb2.png]]
:END:

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/f352f0891676005a8cca214a55d509ac6ce2b303.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

** Training

#+begin_src ipython
  model.LR_TRAIN = 1
  model.LR_EVAL_WIN = 1
  model.lr_eval_win = int(model.LR_EVAL_WIN / model.DT / model.N_WINDOW)
  print(model.lr_eval_win)

  model.DURATION = 6.0
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:
: 10

#+begin_src ipython
  steps = np.arange(0, model.N_STEPS - model.N_STEADY, model.N_WINDOW)
  print(steps.shape)

  mask = (steps >= (model.N_STIM_ON[2] - model.N_STEADY)) & (steps <= (model.N_STIM_OFF[2] - model.N_STEADY))
  rwd_idx = np.where(mask)[0]
  print(rwd_idx)

  mask = (steps >= (model.N_STIM_OFF[1] - model.N_STEADY)) & (steps <= (model.N_STIM_ON[2] - model.N_STEADY))
  cue_idx = np.where(mask)[0]
  print(cue_idx)
#+end_src

#+RESULTS:
: (61,)
: [50 51 52 53 54 55 56 57 58 59 60]
: [40 41 42 43 44 45 46 47 48 49 50]

#+begin_src ipython
  for param in model.linear.parameters():
       param.requires_grad = True
#+end_src

#+RESULTS:

#+begin_src ipython
  for name, param in model.named_parameters():
      if param.requires_grad:
          print(name, param.shape)
#+end_src
#+RESULTS:
: U torch.Size([3000, 2])
: V torch.Size([3000, 2])
: linear.weight torch.Size([1, 2400])
: linear.bias torch.Size([1])

#+begin_src ipython
  model.N_BATCH = 64

  model.lr_eval_win = np.max( (rwd_idx.shape[0], cue_idx.shape[0]))

  ff_input = []
  labels = np.zeros((2, 12, model.N_BATCH, model.lr_eval_win))
  l=0
  for i in [-1, 1]:
      for j in [-1, 0, 1]:
          for k in [1, -1]:

              model.I0[0] = i
              model.I0[1] = j
              model.I0[2] = k

              if i==k: # Pair Trials
                  labels[0, l] = np.ones((model.N_BATCH, model.lr_eval_win))
              else: # Unpair Trials
                  labels[0, l] = np.ones((model.N_BATCH, model.lr_eval_win))

              if j==1: # Go
                  labels[1, l] = np.ones((model.N_BATCH, model.lr_eval_win))
              if j==-1: # NoGo
                  labels[1, l] = np.ones((model.N_BATCH, model.lr_eval_win))

              l+=1

              ff_input.append(model.init_ff_input())

  labels = torch.tensor(labels, dtype=torch.float, device='cuda').reshape(2, -1, model.lr_eval_win).transpose(0, 1)
  ff_input = torch.vstack(ff_input)
  print('ff_input', ff_input.shape, 'labels', labels.shape)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: OutOfMemoryError                          Traceback (most recent call last)
: Cell In[162], line 31
:      28             ff_input.append(model.init_ff_input())
:      30 labels = torch.tensor(labels, dtype=torch.float, device='cuda').reshape(2, -1, model.lr_eval_win).transpose(0, 1)
: ---> 31 ff_input = torch.vstack(ff_input)
:      32 print('ff_input', ff_input.shape, 'labels', labels.shape)
:
: OutOfMemoryError: CUDA out of memory. Tried to allocate 12.19 GiB. GPU 0 has a total capacity of 23.50 GiB of which 5.18 GiB is free. Process 3166108 has 498.00 MiB memory in use. Process 3487083 has 458.00 MiB memory in use. Including non-PyTorch memory, this process has 17.35 GiB memory in use. Of the allocated memory 16.77 GiB is allocated by PyTorch, and 279.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
:END:

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
  ---------------------------------------------------------------------------
  ValueError                                Traceback (most recent call last)
  Cell In[163], line 2
        1 batch_size = 16
  ----> 2 train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)

  Cell In[4], line 6, in split_data(X, Y, train_perc, batch_size)
        3 def split_data(X, Y, train_perc=0.8, batch_size=32):
        5   if Y.ndim==3:
  ----> 6     X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
        7                                                         train_size=train_perc,
        8                                                         stratify=Y[:, 0, 0].cpu().numpy(),
        9                                                         shuffle=True)
       10   else:
       11     X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
       12                                                         train_size=train_perc,
       13                                                         stratify=Y[:, 0].cpu().numpy(),
       14                                                         shuffle=True)

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
      208 try:
      209     with config_context(
      210         skip_parameter_validation=(
      211             prefer_skip_nested_validation or global_skip_validation
      212         )
      213     ):
  --> 214         return func(*args, **kwargs)
      215 except InvalidParameterError as e:
      216     # When the function is just a wrapper around an estimator, we allow
      217     # the function to delegate validation to the estimator, but we replace
      218     # the name of the estimator by the name of the function in the error
      219     # message to avoid confusion.
      220     msg = re.sub(
      221         r"parameter of \w+ must be",
      222         f"parameter of {func.__qualname__} must be",
      223         str(e),
      224     )

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2646, in train_test_split(test_size, train_size, random_state, shuffle, stratify, *arrays)
     2643 if n_arrays == 0:
     2644     raise ValueError("At least one array required as input")
  -> 2646 arrays = indexable(*arrays)
     2648 n_samples = _num_samples(arrays[0])
     2649 n_train, n_test = _validate_shuffle_split(
     2650     n_samples, test_size, train_size, default_test_size=0.25
     2651 )

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:453, in indexable(*iterables)
      434 """Make arrays indexable for cross-validation.
      435
      436 Checks consistent length, passes through None, and ensures that everything
     (...)
      449     sparse matrix, or dataframe) or `None`.
      450 """
      452 result = [_make_indexable(X) for X in iterables]
  --> 453 check_consistent_length(*result)
      454 return result

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:407, in check_consistent_length(*arrays)
      405 uniques = np.unique(lengths)
      406 if len(uniques) > 1:
  --> 407     raise ValueError(
      408         "Found input variables with inconsistent numbers of samples: %r"
      409         % [int(l) for l in lengths]
      410     )

  ValueError: Found input variables with inconsistent numbers of samples: [12, 768]
#+end_example
:END:

#+begin_src ipython
  # criterion = nn.BCEWithLogitsLoss()
  criterion = DualLoss(alpha=0.5, thresh=2.0, N=model.Na[0], cue_idx=cue_idx, rwd_idx=rwd_idx)

  # SGD, Adam, AdamW
  learning_rate = 0.1
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
#+end_src

#+RESULTS:

#+begin_src ipython
  num_epochs = 30
  loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+end_src
#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
  ---------------------------------------------------------------------------
  IndexError                                Traceback (most recent call last)
  Cell In[165], line 2
        1 num_epochs = 30
  ----> 2 loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)

  Cell In[7], line 14, in run_optim(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, penalty, lbd, thresh)
       12 # Training loop.
       13 for epoch in range(num_epochs):
  ---> 14     loss = train(train_loader, model, loss_fn, optimizer, penalty, lbd)
       15     val_loss = test(val_loader, model, loss_fn)
       16     scheduler.step(val_loss)

  Cell In[5], line 11, in train(dataloader, model, loss_fn, optimizer, penalty, lbd)
        8 y_pred = model(X)
       10 # if y.ndim==y_pred.ndim:
  ---> 11 loss = loss_fn(y_pred, y)
       13 if penalty is not None:
       14     reg_loss = 0

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)
     1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
     1510 else:
  -> 1511     return self._call_impl(*args, **kwargs)

  File ~/mambaforge/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)
     1515 # If we don't have any hooks, we want to skip the rest of the logic in
     1516 # this function, and just call forward.
     1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
     1518         or _global_backward_pre_hooks or _global_backward_hooks
     1519         or _global_forward_hooks or _global_forward_pre_hooks):
  -> 1520     return forward_call(*args, **kwargs)
     1522 try:
     1523     result = None

  Cell In[12], line 20, in DualLoss.forward(self, readout, targets)
       18     return self.DPA_loss
       19 else:
  ---> 20     self.DPA_loss = self.loss(readout[:, self.rwd_idx], targets[:, 0, :self.rwd_idx.shape[0]])
       21     self.DRT_loss = self.loss(readout[:, self.cue_idx], targets[:, 1, :self.cue_idx.shape[0]])
       22     return (self.DPA_loss + self.DRT_loss) / 2.0

  IndexError: too many indices for tensor of dimension 2
#+end_example
:END:

** Re-Testing

#+begin_src ipython
  Wij = model.Wab_T.clone()
#+end_src

#+RESULTS:

#+begin_src ipython
  model.eval()

  if model.LR_NORM:
      lr = model.lr_kappa * model.lr_mask * (masked_normalize(model.U) @ masked_normalize(model.V).T) / (1.0 * model.Na[0])
  else:
      lr = model.lr_mask * (model.U @ model.V.T) / (1.0 * model.Na[0])
  lr = lr.clamp(min=-model.Wab_T[0, 0])

  model.Wab_T = Wij + lr.T

  model.N_BATCH = 1
  model.VERBOSE=1
  model.LR_TRAIN=0
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1
  model.DURATION = 6
  model.N_STEPS = int(model.DURATION / model.DT) + model.N_STEADY + model.N_WINDOW
#+end_src

#+RESULTS:

#+begin_src ipython
  model.N_BATCH = 1

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = 1

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = 0
  model.I0[2] = -1

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 0
  model.I0[2] = -1

  BD_pair = model.init_ff_input()

  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print('ff_input', ff_input.shape)
#+end_src

#+RESULTS:
: ff_input torch.Size([4, 710, 3000])

#+begin_src ipython
  model.VERBOSE = 0
  rates = model.forward(ff_input=ff_input).cpu().detach().numpy()
  model.Wab_T = Wij
  print(rates.shape)
#+end_src

#+RESULTS:
: (4, 61, 2400)

#+begin_src ipython
  idx = get_idx(model, 2)
#+end_src

#+RESULTS:
: ksi torch.Size([5, 2400])

#+begin_src ipython
  plot_rates_selec(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ae314c21b949bcc0a83ae3ea80a32b6ad73e36b0.png]]

#+begin_src ipython
    readout = model.linear.weight.data[0].cpu().detach().numpy()
    plot_overlap(rates, readout)
#+end_src

#+RESULTS:
:RESULTS:
: (4, 61)
[[file:./.ob-jupyter/40802e1447fca814415ec62dfc49b66ad00964fb.png]]
:END:

#+begin_src ipython
  plot_m0_m1_phi(rates, idx)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/7d0fca32c3308062189afa334537bbddbf091938.png]]

#+begin_src ipython

#+end_src

#+RESULTS:
