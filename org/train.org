#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session dual :kernel torch

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run ../notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'

  REPO_ROOT = "/home/leon/models/NeuroTorch"
  pal = sns.color_palette("tab10")
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Helpers
** Training
*** split data

#+begin_src ipython
  def split_data(X, Y, train_perc=0.8, batch_size=32):

    # Split the dataset into training and validation sets
    train_size = int(train_perc * X.shape[0])

    X_train = X[:train_size]
    X_test = X[train_size:]

    # X_train, X_mean, X_std = standard_scaler(X_train, IF_RETURN=1)
    # X_test = (X_test - X_mean) / X_std

    Y_train = Y[:train_size]    
    Y_test = Y[train_size:]

    # Y_train, Y_mean, Y_std = standard_scaler(Y_train, IF_RETURN=1)
    # Y_test = (Y_test - Y_mean) / Y_std

    # Create data sets
    # train_dataset = TensorDataset(X_train_scaled, Y_train_scaled)
    # val_dataset = TensorDataset(X_test_scaled, Y_test_scaled)

    # print('X_train', X_train.shape, 'y_train', Y_train.shape)
    train_dataset = TensorDataset(X_train, Y_train)

    # print('X_test', X_test.shape, 'y_test', Y_test.shape)
    val_dataset = TensorDataset(X_test, Y_test)
    
    # Create data loaders
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

    # sequence_length = 14  # or any other sequence length you want
    # stride = 1  # or any other stride you want

    # sliding_window_dataset = SlidingWindowDataset(X, sequence_length, stride)
    # train_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)
    # val_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)

    return train_loader, val_loader
#+end_src

#+RESULTS:

** Optimization

#+begin_src ipython
  def train(dataloader, model, loss_fn, optimizer, penalty=None, lbd=1):
      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      model.train()
      for batch, (X, y) in enumerate(dataloader):
          optimizer.zero_grad()

          X, y = X.to(device), y.to(device)
          # Compute prediction error
          y_pred = model(X)
          loss = loss_fn(y_pred, y)

          if penalty is not None:
              reg_loss = 0
              for param in model.parameters():
                  if penalty=='l1':
                      reg_loss += torch.sum(torch.abs(param))
                  else:
                      reg_loss += torch.sum(torch.square(param))

                  loss = loss + lbd * reg_loss

          # Backpropagation
          loss.backward()
          optimizer.step()

      return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def test(dataloader, model, loss_fn):
      size = len(dataloader.dataset)
      num_batches = len(dataloader)

      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      # Validation loop.
      model.eval()
      val_loss = 0.0
      with torch.no_grad():
          for data, targets in dataloader:
              data, targets = data.to(device), targets.to(device)
              
              outputs = model(data)
              loss = loss_fn(outputs, targets)
              val_loss += loss.item() * data.size(0)
          val_loss /= size

      return val_loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def run_optim(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100, penalty=None, lbd=1):

    # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)
    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    loss_list = []
    val_loss_list = []

    # Training loop.
    for epoch in range(num_epochs):
        loss = train(train_loader, model, loss_fn, optimizer, penalty, lbd)
        val_loss = test(val_loader, model, loss_fn)
        scheduler.step(val_loss)

        loss_list.append(loss.item())
        val_loss_list.append(val_loss)

        # if epoch % int(num_epochs  / 10) == 0:
        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

    return loss_list, val_loss_list
#+end_src

#+RESULTS:

** Loss

#+begin_src ipython
  def correlation_loss(output, target):
      # Subtract the mean of each vector
      output_mean = output - torch.mean(output)
      target_mean = target - torch.mean(target)
    
      # Compute the covariance between output and target
      covariance = torch.mean(output_mean * target_mean)
      
      # Compute the standard deviations of the vectors
      output_std = torch.std(output)
      target_std = torch.std(target)
    
      # Calculate the Pearson correlation coefficient
      correlation = covariance / (output_std * target_std)
    
      # Since we want to increase the correlation, we minimize its negative
      loss = -correlation  # Maximizing correlation by minimizing its negative
    
      return loss
#+end_src

#+RESULTS:

#+begin_src ipython
    import torch
    import torch.nn as nn

    def sign_constrained_loss(output, xi, target_sign):
        dot_product = torch.dot(output.flatten(), xi.flatten())
        if target_sign > 0:
            loss = torch.relu(-dot_product)  # Encourages positive dot product
        else:
            loss = torch.relu(dot_product)   # Encourages negative dot product
        return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  class CosineLoss(nn.Module):
      def __init__(self):
          super(CosineLoss, self).__init__()
          self.cosine_similarity = nn.CosineSimilarity(dim=-1)
          
      def forward(self, input1, input2):
          # Calculate cosine similarity
          cosine_sim = self.cosine_similarity(input1, input2)
          # Calculate the loss as 1 - cosine_similarity
          loss = 1 - cosine_sim
          # Return the mean loss over the batch
          return loss.mean()
#+end_src

#+RESULTS:


#+RESULTS:

** Other

#+begin_src ipython
  def get_theta(a, b, GM=0, IF_NORM=0):

      u, v = a, b

      if GM:          
          v = b - np.dot(b, a) / np.dot(a, a) * a
          
      if IF_NORM:
          u = a / np.linalg.norm(a)
          v = b / np.linalg.norm(b)

      return np.arctan2(v, u)
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_idx(model):
      ksi = model.U.cpu().detach().numpy().T
      # ksi = model.PHI0.cpu().detach().numpy()

      print(ksi.shape)
      
      theta = get_theta(ksi[0], ksi[1], GM=0, IF_NORM=0)
      theta = get_theta(ksi[0][:model.Na[0]], ksi[1][:model.Na[0]], GM=0, IF_NORM=0)

      return theta.argsort()
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_overlap(model, rates):
      ksi = model.PHI0.cpu().detach().numpy()
      return rates @ ksi.T / rates.shape[-1]
  
#+end_src

#+RESULTS:

#+begin_src ipython
  import scipy.stats as stats

  def plot_smooth(data, ax, color):
      mean = data.mean(axis=0)  
      ci = smooth.std(axis=0, ddof=1) * 1.96
      
      # Plot
      ax.plot(mean, color=color)
      ax.fill_between(range(data.shape[1]), mean - ci, mean + ci, alpha=0.25, color=color)

#+end_src

#+RESULTS:

#+begin_src ipython
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

* Imports

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader
#+end_src

#+RESULTS:

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  import pandas as pd
  import torch.nn as nn
  from time import perf_counter  
  from scipy.stats import circmean

  from src.network import Network
  from src.plot_utils import plot_con
  from src.decode import decode_bump, circcvl
#+end_src

#+RESULTS:

* Train RNN
** Parameters

#+Begin_src ipython
  REPO_ROOT = "/home/leon/models/NeuroTorch"
  conf_name = "config_train.yml"
  name = "dual"
#+end_src

#+RESULTS:

** Model

#+begin_src ipython
  start = perf_counter()
  model = Network(conf_name, REPO_ROOT, VERBOSE=0, DEVICE='cuda', SEED=2)
#+end_src

#+RESULTS:

#+begin_src ipython
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name, param.data)
#+end_src

#+RESULTS:
#+begin_example
  U tensor([[ 2.2669,  1.3477],
          [-1.4438, -1.0484],
          [ 1.5167,  1.2639],
          ...,
          [-0.7353, -1.0578],
          [ 0.1241,  1.2592],
          [-2.0339,  2.7791]], device='cuda:0')
  lr_kappa tensor([3.0735])
  linear.weight tensor([[ 0.0030, -0.0416, -0.0053, -0.0130, -0.0162,  0.0406, -0.0353, -0.0410,
            0.0251, -0.0243, -0.0399,  0.0290, -0.0179, -0.0005,  0.0290,  0.0088,
           -0.0003, -0.0007, -0.0291, -0.0032, -0.0232,  0.0105, -0.0286, -0.0434,
            0.0005,  0.0375, -0.0069, -0.0146, -0.0392, -0.0027, -0.0219,  0.0171,
            0.0115,  0.0396, -0.0138,  0.0306,  0.0077,  0.0430, -0.0082, -0.0047,
           -0.0042,  0.0030, -0.0250,  0.0273, -0.0349, -0.0302, -0.0006, -0.0345,
            0.0064, -0.0243, -0.0118,  0.0121,  0.0407, -0.0418,  0.0229,  0.0331,
            0.0322,  0.0270, -0.0039, -0.0033,  0.0270,  0.0107, -0.0229, -0.0164,
           -0.0416, -0.0020,  0.0417,  0.0040, -0.0208,  0.0030, -0.0202, -0.0407,
           -0.0202, -0.0338, -0.0347,  0.0268,  0.0291,  0.0288,  0.0179, -0.0172,
            0.0033, -0.0370, -0.0323, -0.0263,  0.0285,  0.0121,  0.0267, -0.0352,
           -0.0205,  0.0268, -0.0323, -0.0185,  0.0418,  0.0227,  0.0025,  0.0132,
            0.0316,  0.0336, -0.0126,  0.0069, -0.0211,  0.0315,  0.0440,  0.0095,
           -0.0278, -0.0200, -0.0200, -0.0046,  0.0270, -0.0077, -0.0224, -0.0026,
            0.0441,  0.0349,  0.0216,  0.0301, -0.0089,  0.0015,  0.0160,  0.0242,
            0.0447,  0.0336,  0.0085, -0.0087,  0.0354,  0.0122,  0.0178, -0.0128,
           -0.0254,  0.0294, -0.0147, -0.0066, -0.0367, -0.0384,  0.0221,  0.0195,
           -0.0147, -0.0446,  0.0398, -0.0168,  0.0223,  0.0243, -0.0042,  0.0137,
            0.0442,  0.0034,  0.0023, -0.0328,  0.0315, -0.0222, -0.0166,  0.0040,
            0.0154,  0.0200, -0.0242, -0.0204, -0.0224,  0.0430, -0.0243,  0.0341,
           -0.0227,  0.0316, -0.0232, -0.0356,  0.0388, -0.0017, -0.0146, -0.0317,
           -0.0352, -0.0340,  0.0074,  0.0266,  0.0399, -0.0332,  0.0192, -0.0365,
           -0.0249, -0.0278,  0.0305, -0.0199,  0.0168,  0.0350, -0.0269, -0.0161,
            0.0211,  0.0069,  0.0323, -0.0188,  0.0245,  0.0428,  0.0284, -0.0047,
            0.0267, -0.0051, -0.0108,  0.0169, -0.0150, -0.0064,  0.0229, -0.0044,
           -0.0389, -0.0442, -0.0445,  0.0333,  0.0201,  0.0101, -0.0439,  0.0376,
           -0.0407,  0.0287,  0.0151,  0.0397,  0.0028, -0.0273,  0.0322,  0.0164,
           -0.0133, -0.0232, -0.0214,  0.0367,  0.0294,  0.0185, -0.0127, -0.0378,
           -0.0406, -0.0256, -0.0100, -0.0135,  0.0099, -0.0037,  0.0002, -0.0269,
           -0.0271, -0.0309,  0.0422, -0.0138,  0.0065,  0.0419, -0.0148, -0.0208,
            0.0343,  0.0016,  0.0377, -0.0305,  0.0031,  0.0004,  0.0201, -0.0312,
            0.0318,  0.0330,  0.0224,  0.0320,  0.0376,  0.0017, -0.0063, -0.0101,
            0.0079,  0.0014, -0.0250, -0.0124,  0.0334,  0.0284,  0.0018,  0.0037,
           -0.0349,  0.0255, -0.0351,  0.0175,  0.0377,  0.0197, -0.0047,  0.0200,
            0.0281,  0.0268, -0.0436, -0.0295,  0.0214, -0.0035, -0.0181, -0.0111,
           -0.0073,  0.0009, -0.0156,  0.0393, -0.0196,  0.0331,  0.0054,  0.0134,
            0.0418,  0.0158, -0.0135,  0.0356,  0.0083,  0.0212, -0.0198,  0.0020,
           -0.0423,  0.0143,  0.0247,  0.0296, -0.0396, -0.0370, -0.0313, -0.0015,
            0.0014, -0.0171,  0.0174,  0.0439, -0.0416,  0.0056,  0.0376, -0.0078,
           -0.0208, -0.0061, -0.0051,  0.0084,  0.0032, -0.0349, -0.0046, -0.0224,
            0.0249,  0.0269, -0.0273,  0.0162, -0.0237,  0.0223,  0.0165,  0.0006,
           -0.0094,  0.0242,  0.0053, -0.0171,  0.0368, -0.0362,  0.0249,  0.0427,
           -0.0212,  0.0204, -0.0233,  0.0245, -0.0094,  0.0269,  0.0077, -0.0097,
           -0.0363,  0.0274, -0.0337,  0.0109,  0.0277,  0.0237,  0.0146, -0.0105,
            0.0115,  0.0076, -0.0139, -0.0353, -0.0028, -0.0248,  0.0446,  0.0384,
            0.0140, -0.0328, -0.0236, -0.0272, -0.0301,  0.0347, -0.0040,  0.0017,
           -0.0090, -0.0253,  0.0016, -0.0075,  0.0128, -0.0219,  0.0326,  0.0206,
           -0.0351,  0.0081, -0.0013,  0.0356,  0.0074,  0.0147, -0.0296, -0.0335,
           -0.0094, -0.0168, -0.0048,  0.0070, -0.0319, -0.0310,  0.0320, -0.0156,
           -0.0381,  0.0408,  0.0425,  0.0279,  0.0212,  0.0283, -0.0130,  0.0349,
           -0.0346, -0.0248, -0.0331, -0.0213, -0.0105, -0.0435, -0.0077, -0.0430,
            0.0361, -0.0089,  0.0082,  0.0343, -0.0090, -0.0307, -0.0065, -0.0353,
           -0.0417, -0.0269,  0.0309,  0.0325,  0.0343, -0.0300,  0.0266, -0.0173,
            0.0066, -0.0301, -0.0427, -0.0041, -0.0148,  0.0205,  0.0422, -0.0198,
            0.0210, -0.0312, -0.0031, -0.0198,  0.0197,  0.0061, -0.0281, -0.0052,
            0.0276,  0.0113, -0.0400, -0.0256,  0.0088, -0.0318,  0.0391, -0.0143,
            0.0089, -0.0021, -0.0356,  0.0117,  0.0271,  0.0230, -0.0161,  0.0169,
            0.0100,  0.0321,  0.0264, -0.0122,  0.0255,  0.0028,  0.0371, -0.0099,
           -0.0403, -0.0244,  0.0313,  0.0191, -0.0204,  0.0330, -0.0197, -0.0281,
           -0.0167, -0.0330, -0.0352,  0.0021, -0.0190,  0.0084,  0.0189,  0.0393,
           -0.0189,  0.0058, -0.0333, -0.0067, -0.0410,  0.0174,  0.0352, -0.0393,
           -0.0364,  0.0036,  0.0035,  0.0099,  0.0150, -0.0411, -0.0308, -0.0315,
           -0.0324,  0.0315,  0.0182, -0.0043]], device='cuda:0')
#+end_example

** Inputs and labels

#+begin_src ipython
  model.N_BATCH = 16

  model.I0[0] = 1
  model.I0[1] = 1 

  AC_pair = model.init_ff_input()

  model.I0[0] = 1
  model.I0[1] = -1

  AD_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = 1

  BC_pair = model.init_ff_input()

  model.I0[0] = -1
  model.I0[1] = -1

  BD_pair = model.init_ff_input()
  
  ff_input = torch.cat((AC_pair, BD_pair, AD_pair, BC_pair))
  print(ff_input.shape)
#+end_src

#+RESULTS:
: torch.Size([64, 1220, 1000])

#+begin_src ipython
  labels_pair = torch.zeros((2 * model.N_BATCH, model.lr_eval_win))
  labels_unpair = torch.ones((2 * model.N_BATCH, model.lr_eval_win))
  
  labels = torch.cat((labels_pair, labels_unpair))
  print(ff_input.shape, labels.shape)
#+end_src

#+RESULTS:
: torch.Size([64, 1220, 1000]) torch.Size([64, 10])

#+begin_src ipython
  # plt.imshow(ff_input[0].T.cpu().detach().numpy(), cmap='jet', aspect='auto')
#+end_src

#+RESULTS:

** Train

#+begin_src ipython
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  batch_size = 32
  train_loader, val_loader = split_data(ff_input, labels, train_perc=0.8, batch_size=batch_size)

  learning_rate = 0.1

  # CosineLoss, BCELoss, BCEWithLogitLoss
  criterion = nn.BCEWithLogitsLoss()

  # SGD, Adam, AdamW
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
  
  num_epochs = 30
  loss, val_loss = run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+End_src

#+RESULTS:
: Epoch 1/30, Training Loss: nan, Validation Loss: nan
: Epoch 2/30, Training Loss: nan, Validation Loss: nan
: Epoch 3/30, Training Loss: nan, Validation Loss: nan
: Epoch 4/30, Training Loss: nan, Validation Loss: nan
: Epoch 5/30, Training Loss: nan, Validation Loss: nan
: Epoch 6/30, Training Loss: nan, Validation Loss: nan
: Epoch 7/30, Training Loss: nan, Validation Loss: nan

#+begin_src ipython
  model.eval()
#+end_src

#+RESULTS:
: f3da5b30-5419-457b-94c6-7bb860940c71

#+begin_src ipython
  plt.plot(loss[:10])
  plt.plot(val_loss[:10])
  plt.show()
#+end_src

#+RESULTS:
: 988a8a12-9402-4340-a6bb-50cc25c5460a

#+begin_src ipython

#+end_src

#+RESULTS:
: aaef9f22-fc6b-4e2a-a4e3-5cab0910f8b7

* Results

#+begin_src ipython
  ksi = model.U.T
  # ksi = torch.stack((model.U.T[0], model.V.T[0]))
  print(ksi.shape)
  
  print('kappa', model.lr_kappa.cpu().detach())

  angle = torch.arccos(nn.CosineSimilarity(dim=0)(ksi[0], ksi[1])) * 180 / torch.pi
  print('angle ksi1 vs ksi2', angle.cpu().detach())

  var = torch.var(ksi, axis=-1)
  print('variances', var.cpu().detach())
#+end_src

#+RESULTS:
: torch.Size([2, 1000])
: kappa tensor([3.0735])
: angle ksi1 vs ksi2 tensor(81.7612)
: variances tensor([0.6990, 0.6707])

#+begin_src ipython
  lr = (1.0 + model.U @ model.U.T / torch.sqrt(model.Ka[0]))  
  weights = model.Wab_T * lr
  weights = weights.cpu().detach().numpy()
#+end_src

#+RESULTS:

#+begin_src ipython  
  plot_con(weights)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a4f848a6b07b8236b2c3c6fbbc187bda19b0c884.png]]

#+begin_src ipython
  readout = model.linear.weight.data[0]
  print(readout.shape)
#+end_src

#+RESULTS:
: torch.Size([500])

#+begin_src ipython
  read0 = nn.CosineSimilarity(dim=0)(model.U[:model.Na[0],0], readout).cpu().detach().numpy()
  read1 = nn.CosineSimilarity(dim=0)(model.U[:model.Na[0],1], readout).cpu().detach().numpy()

  print('angle readout vs ksis', np.arccos(read0)*180/np.pi, np.arccos(read1)*180/np.pi)
#+end_src

#+RESULTS:
: angle readout vs ksis 69.85629249106067 113.9126792038004

** Eval

#+begin_src ipython
  model.eval()

  lr = model.mask * (model.U @ model.U.T) / model.Na[0]
  model.Wab_T = lr.T
  
  model.N_BATCH = 1
  model.VERBOSE=1
  model.LR_TRAIN=0
  # print(model.ff_input.shape)
  # print(ff_input.shape)
#+end_src

#+RESULTS:

#+begin_src ipython
  rates = model.forward(RET_FF=1).cpu().detach().numpy()
  print(rates.shape)
#+end_src

#+RESULTS:
#+begin_example
  Generating ff input
  times (s) 0.0 rates (Hz) [10.67, 5.65]
  times (s) 0.08 rates (Hz) [10.67, 5.65]
  times (s) 0.16 rates (Hz) [10.67, 5.65]
  times (s) 0.25 rates (Hz) [10.67, 5.65]
  times (s) 0.33 rates (Hz) [10.67, 5.65]
  times (s) 0.41 rates (Hz) [10.67, 5.65]
  times (s) 0.49 rates (Hz) [10.67, 5.65]
  times (s) 0.57 rates (Hz) [10.67, 5.65]
  times (s) 0.66 rates (Hz) [10.67, 5.65]
  times (s) 0.74 rates (Hz) [10.67, 5.65]
  times (s) 0.82 rates (Hz) [10.69, 5.65]
  times (s) 0.9 rates (Hz) [10.86, 5.65]
  times (s) 0.98 rates (Hz) [10.88, 5.65]
  times (s) 1.07 rates (Hz) [10.88, 5.65]
  times (s) 1.15 rates (Hz) [10.89, 5.65]
  times (s) 1.23 rates (Hz) [10.89, 5.65]
  times (s) 1.31 rates (Hz) [10.89, 5.65]
  times (s) 1.39 rates (Hz) [10.89, 5.65]
  times (s) 1.48 rates (Hz) [10.89, 5.65]
  times (s) 1.56 rates (Hz) [10.89, 5.65]
  times (s) 1.64 rates (Hz) [10.86, 5.65]
  times (s) 1.72 rates (Hz) [10.69, 5.65]
  times (s) 1.8 rates (Hz) [10.67, 5.65]
  times (s) 1.89 rates (Hz) [10.67, 5.65]
  times (s) 1.97 rates (Hz) [10.67, 5.65]
  times (s) 2.05 rates (Hz) [10.67, 5.65]
  times (s) 2.13 rates (Hz) [10.67, 5.65]
  times (s) 2.21 rates (Hz) [10.67, 5.65]
  times (s) 2.3 rates (Hz) [10.67, 5.65]
  times (s) 2.38 rates (Hz) [10.67, 5.65]
  times (s) 2.46 rates (Hz) [10.68, 5.65]
  times (s) 2.54 rates (Hz) [10.73, 5.65]
  times (s) 2.62 rates (Hz) [10.74, 5.65]
  times (s) 2.7 rates (Hz) [10.74, 5.65]
  times (s) 2.79 rates (Hz) [10.74, 5.65]
  times (s) 2.87 rates (Hz) [10.74, 5.65]
  times (s) 2.95 rates (Hz) [10.74, 5.65]
  times (s) 3.03 rates (Hz) [10.74, 5.65]
  times (s) 3.11 rates (Hz) [10.74, 5.65]
  times (s) 3.2 rates (Hz) [10.74, 5.65]
  times (s) 3.28 rates (Hz) [10.73, 5.65]
  times (s) 3.36 rates (Hz) [10.67, 5.65]
  times (s) 3.44 rates (Hz) [10.67, 5.65]
  times (s) 3.52 rates (Hz) [10.67, 5.65]
  times (s) 3.61 rates (Hz) [10.67, 5.65]
  times (s) 3.69 rates (Hz) [10.67, 5.65]
  times (s) 3.77 rates (Hz) [10.67, 5.65]
  times (s) 3.85 rates (Hz) [10.67, 5.65]
  times (s) 3.93 rates (Hz) [10.67, 5.65]
  times (s) 4.02 rates (Hz) [10.67, 5.65]
  times (s) 4.1 rates (Hz) [10.67, 5.65]
  Elapsed (with compilation) = 0.21793645061552525s
  (1, 51, 500)
#+end_example

#+begin_src ipython
  plt.plot(model.ff_input.cpu().detach().numpy()[0,:, :10])
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/78eae662836fff73267f8a5dd9412180ba326703.png]]

#+begin_src ipython
  plt.imshow(rates[0].T, aspect='auto', cmap='jet', vmin=0, vmax=30)
  plt.vlines((np.array(model.N_STIM_ON) - model.N_STEADY) / model.N_WINDOW, 0, 360, 'w', '--')
  plt.vlines((np.array(model.N_STIM_OFF) - model.N_STEADY) / model.N_WINDOW, 0, 360, 'w', '--')
  plt.ylabel('Neuron #')
  plt.xlabel('Step')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/e6262de68b554bf936976aef8c7d13a1dacfce96.png]]

#+begin_src ipython
  idx = get_idx(model)
  ordered = rates[..., idx]
  print(ordered.shape)
#+end_src

#+RESULTS:
: (2, 1000)
: (1, 51, 500)

#+begin_src ipython
  plt.imshow(ordered[0].T, aspect='auto', cmap='jet', vmin=0, vmax=30.0)
  plt.yticks(np.linspace(0, model.Na[0].cpu().detach(), 5), np.linspace(0, 360, 5).astype(int))
  plt.vlines((np.array(model.N_STIM_ON) - model.N_STEADY) / model.N_WINDOW, 0, 360, 'w', '--')
  plt.vlines((np.array(model.N_STIM_OFF) - model.N_STEADY) / model.N_WINDOW, 0, 360, 'w', '--')
  plt.ylabel('Pref. Location (°)')
  plt.xlabel('Step')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/0019c1b6cfd71003dbe256a23126568017e6c614.png]]

#+begin_src ipython
  print(model.N_STIM_ON)
#+end_src

#+RESULTS:
: [400 800]

#+begin_src ipython
  y_pred = model.linear.weight.data.cpu().detach().numpy()[0]
  print(y_pred.shape)

  overlap = (rates @ y_pred) / rates.shape[-1]
  print(overlap.shape)
  plt.plot(overlap.T)
  plt.xlabel('Step')
  plt.ylabel('Overlap')
  
  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: (500,)
: (1, 51)
[[file:./.ob-jupyter/42c5935794781e4b9e0a3a11d6cd022aa106f26c.png]]
:END:

#+begin_src ipython
  m0, m1, phi = decode_bump(ordered, axis=-1)
#+end_src

#+RESULTS:

#+begin_src ipython
  fig, ax = plt.subplots(1, 3, figsize=[2*width, height])
  
  ax[0].plot(m0.T)
  #ax[0].set_ylim([0, 360])
  #ax[0].set_yticks([0, 90, 180, 270, 360])
  ax[0].set_ylabel('$\mathcal{F}_0$ (Hz)')
  ax[0].set_xlabel('Step')

  ax[1].plot(m1.T)
  # ax[1].set_ylim([0, 360])
  # ax[1].set_yticks([0, 90, 180, 270, 360])
  ax[1].set_ylabel('$\mathcal{F}_1$ (Hz)')
  ax[1].set_xlabel('Step')

  ax[2].plot(phi.T * 180 / np.pi)
  ax[2].set_ylim([0, 360])
  ax[2].set_yticks([0, 90, 180, 270, 360])
  ax[2].set_ylabel('Phase (°)')
  ax[2].set_xlabel('Step')

  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2efd058e745d11ebe17009e000ccaabc351bd0b0.png]]

#+begin_src ipython

#+end_src

#+RESULTS:
