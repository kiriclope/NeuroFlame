#+STARTUP: fold
#+TITLE: RNN with pytorch
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session doc :kernel torch

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run ../notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  import torch
  import gc
  import pandas as pd
  from time import perf_counter

  from src.network import Network
  from src.plot_utils import plot_con
  from src.decode import decode_bump
  from src.utils import clear_cache

  REPO_ROOT = "/home/leon/models/NeuroTorch/"
#+end_src

#+RESULTS:
* Helpers

#+begin_src ipython
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

* RNN with torch
** Single Trial

Here we will run a single simulation with the parameters provided in config_2pop.yml

#+begin_src ipython

  start = perf_counter()

  # First we create a network with
  model = Network('config_2pop.yml', '', REPO_ROOT)

  # then we run the simulation with
  output = model()

  print('output', output.shape)
  # model outputs a tensor of rates of size (N_BATCH, N_STEPS, N_NEURON), so we need to convert it to numpy

  rates = output[0].cpu().numpy()
  print('rates', rates.shape)

  end = perf_counter()
  print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))
  
  Ne = model.Na[0].detach().cpu().numpy()
  N = model.N_NEURON
#+end_src

#+RESULTS:
: output torch.Size([1, 101, 8000])
: rates (101, 8000)
: Elapsed (with compilation) = 0h 0m 6s

#+RESULTS:

#+begin_src ipython
  print(torch.cuda.memory_allocated()/100000)
  del model
  clear_cache()
  print(torch.cuda.memory_allocated()/100000)
#+end_src

#+RESULTS:
: 4124.75904
: 117.51936

#+begin_src ipython
  fig, ax = plt.subplots(1, 3, figsize=(2*width, height))

  r_max = 10

  ax[0].imshow(rates.T, aspect='auto', cmap='jet', vmin=0, vmax=r_max, origin='lower')
  ax[0].set_ylabel('Neuron #')
  ax[0].set_xlabel('Step')

  ax[1].plot(rates.mean(-1))
  for i in range(10):
      ax[1].plot(rates[..., i], alpha=0.2)

  ax[1].set_ylabel('$<Rates>_i$')
  ax[1].set_xlabel('Step')
  ax[1].set_ylim([0, r_max])
  
  ax[2].hist(rates[-1], density=True, bins='auto')
  ax[2].set_xlabel('Density')
  ax[2].set_ylabel('Rates')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/de7f63661ec1db32ab020970f0f803ab6c213fce.png]]

#+begin_src ipython
  
#+end_src

#+RESULTS:

** Multiple Trials
*** Multiple Initial Conditions
We can run multiple initializations of the network changing N_BATCH to the number of initializations that we want.

#+begin_src ipython
  model = Network('config_2pop.yml', '', REPO_ROOT, GAIN=5)
  
  model.N_BATCH = 10
  rates = model().cpu().numpy()
  print('rates', rates.shape)
#+end_src

#+RESULTS:
: rates (10, 101, 8000)

#+begin_src ipython
  fig, ax = plt.subplots(1, 2, figsize=(2*width, height))

  for i in range(rates.shape[0]):
      ax[0].hist(rates.mean(1)[i], bins='auto', density=True)
  ax[0].set_ylabel('$<Rates>_i$')
  ax[0].set_xlabel('Initialization')

  ax[1].plot(rates.mean(-1).T)
  ax[1].set_ylabel('$<Rates>_i$')
  ax[1].set_xlabel('Step')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/09b05b5846dd6c9f5c1c7204bb10f4b4c00bc270.png]]

#+begin_src ipython
  print(torch.cuda.memory_allocated()/100000)
  del model
  clear_cache()
  print(torch.cuda.memory_allocated()/100000)
#+end_src

#+RESULTS:
: 4124.75904
: 117.51936

*** Batching Feedforward Inputs
To run some parameter searches, we can easily batch over a different set of ff inputs
Let's see an example where we change the ff inputs to the excitatory population

**** The easy way (but memory consuming)
We create a batch of inputs of size (N_BATCH, N_STEPS, N_NEURON)

#+begin_src ipython
  model = Network('config_2pop.yml', '', REPO_ROOT)
  
  ff_inputs = []
  for i in range(10):
      model.Ja0[:, 0] = i  # here we set the ff input to E to value i in 0 .. 10
      ff_inputs.append(model.init_ff_input())

  ff_inputs = torch.vstack(ff_inputs)
  print('ff_inputs', ff_inputs.shape)
#+end_src

#+RESULTS:
: ff_inputs torch.Size([10, 11100, 10000])

Then we path these inputs to the model

#+begin_src ipython
  rates = model(ff_inputs).cpu().numpy()
  print(rates.shape)
#+end_src

#+RESULTS:
: (10, 101, 8000)

#+begin_src ipython
  fig, ax = plt.subplots(1, 2, figsize=(2*width, height))

  ax[0].plot(rates.mean((1,-1)), '-o')
  ax[0].set_ylabel('$<Rates>_i$')
  ax[0].set_xlabel('FF inputs')

  ax[1].plot(rates.mean(-1).T)  
  ax[1].set_ylabel('$<Rates>_i$')
  ax[1].set_xlabel('Step')
  ax[1].set_ylim([0, 30])
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/98d286989691c458d315b6ee2f17862c4fa87e8e.png]]

#+begin_src ipython
  print(torch.cuda.memory_allocated()/100000)
  del model
  clear_cache()
  print(torch.cuda.memory_allocated()/100000)
#+end_src

#+RESULTS:
: 48524.75904
: 44517.51936

**** The hard way (slow but more memory friendly)
We create a batch of ff inputs that are updated at each time step

#+begin_src ipython
  model = Network('config_2pop.yml', '', REPO_ROOT, LIVE_FF_UPDATE=1)

  N_BATCH = 10
  print('original ff_input', model.Ja0.shape)

  new_Ja0 = model.Ja0.repeat((N_BATCH, 2, 1))  
  print('new ff_input', new_Ja0.shape)
  
  new_Ja0[:, 0] = torch.linspace(0, 10, N_BATCH, device='cuda').unsqueeze(-1) * model.M0 * torch.sqrt(model.Ka[0])
  print('batched ff_input', new_Ja0[:, 0].squeeze(-1))
#+end_src

#+RESULTS:
: original ff_input torch.Size([1, 2, 1])
: new ff_input torch.Size([10, 4, 1])
: batched ff_input tensor([  0.0000,  24.8452,  49.6904,  74.5356,  99.3808, 124.2260, 149.0712,
:         173.9164, 198.7616, 223.6068], device='cuda:0')

#+begin_src ipython
  model.N_BATCH = N_BATCH
  model.Ja0 = new_Ja0
  model.LIVE_FF_UPDATE = 1

  start = perf_counter()
  rates = model().cpu().numpy()
  end = perf_counter()
  print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

  print('rates', rates.shape)
#+end_src

#+RESULTS:
: Elapsed (with compilation) = 0h 0m 9s
: rates (10, 101, 8000)

#+begin_src ipython
  fig, ax = plt.subplots(1, 2, figsize=(2*width, height))

  ax[0].plot(rates.mean((1,-1)), '-o')
  ax[0].set_ylabel('$<Rates>_i$')
  ax[0].set_xlabel('FF inputs')

  ax[1].plot(rates.mean(-1).T)  
  ax[1].set_ylabel('$<Rates>_i$')
  ax[1].set_xlabel('Step')
  ax[1].set_ylim([0, 30])
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/03fbed52abd8e8861b0af19f521b27bfcc2a3213.png]]

#+begin_src ipython
  print(torch.cuda.memory_allocated()/100000)
  del model
  clear_cache()
  print(torch.cuda.memory_allocated()/100000)
#+end_src

#+RESULTS:
: 48531.15904
: 44517.52448

*** Batching Reccurent Weights Jab

#+begin_src ipython
  model = Network('config_2pop.yml', 'None', REPO_ROOT, IF_STP=0, DT=0.001, GAIN=0.5, VERBOSE=0, LIVE_FF_UPDATE=1)
#+end_src

#+RESULTS:

#+begin_src ipython
  model.IF_BATCH_J = 1

  Jee_list = torch.linspace(0.0, 1.5, 10, device='cuda')  
  model.Jab_batch = Jee_list.unsqueeze(-1) * model.Jab[0, 0]
  print(model.Jab_batch[:, 0])

  model.IF_STP = 1
  model.N_BATCH = model.Jab_batch.shape[0]
  model.VERBOSE = 0
#+end_src

#+RESULTS:
: tensor([0.0000, 0.0037, 0.0075, 0.0112, 0.0149, 0.0186, 0.0224, 0.0261, 0.0298,
:         0.0335], device='cuda:0')
    
#+begin_src ipython
  start = perf_counter()
  rates_Jee = model().cpu().detach().numpy()
  end = perf_counter()
  print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

  print('rates', rates.shape)
#+end_src

#+RESULTS:
: Elapsed (with compilation) = 0h 0m 24s
: rates (10, 101, 8000)

#+begin_src ipython
  fig, ax = plt.subplots(1, 2, figsize=[2*width, height])

  mean_rates = rates_Jee[:,-1].mean(-1)

  ax[0].plot(Jee_list.cpu().numpy(), mean_rates)
  ax[0].set_xlabel('$J_{EE}$')
  ax[0].set_ylabel('$<Rates>_i$')
  # ax[0].set_ylim([0, 60])

  ax[1].plot(rates_Jee.mean(-1).T)
  ax[1].set_xlabel('$J_{EE}$')
  ax[1].set_ylabel('Rates')
  # ax[1].set_ylim([0, 60])
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/74be1f0d8274c7ca81a8d431d7afca4e52b97cb7.png]]

#+begin_src ipython
  print(torch.cuda.memory_allocated()/100000)
  del model
  clear_cache()
  print(torch.cuda.memory_allocated()/100000)
#+end_src

#+RESULTS:
: 53651.19488
: 44517.5296

*** Batching Feedforward Inputs and Weights

#+begin_src ipython
  model = Network('config_2pop.yml', 'None', REPO_ROOT, IF_STP=0, DT=0.001, GAIN=0.5, LIVE_FF_UPDATE=1)
#+end_src

#+RESULTS:

#+begin_src ipython
  N_BATCH = 10
  
  JEE = torch.linspace(0.0, 5.0, N_BATCH, device='cuda')
  JE0 = torch.linspace(0.0, 5.0, N_BATCH, device='cuda')

  JEE = JEE.unsqueeze(1).expand(N_BATCH, N_BATCH) 
  JEE = JEE.reshape((-1, 1)) * model.Jab[0, 0]
  print('Jee', JEE.shape)

  JE0 = JE0.unsqueeze(0).expand(N_BATCH, N_BATCH)
  JE0 = JE0.reshape((-1, 1))
  print('Je0', JE0.shape)

  new_Ja0 = model.Ja0.repeat((N_BATCH*N_BATCH, 1, 1)) 

  print('Ja0', new_Ja0.shape)
  new_Ja0[:,0] = JE0 * torch.sqrt(model.Ka[0]) * model.M0
#+end_src

#+RESULTS:
: Jee torch.Size([100, 1])
: Je0 torch.Size([100, 1])
: Ja0 torch.Size([100, 2, 1])

#+begin_src ipython
  print(JEE[:, 0].reshape(N_BATCH, N_BATCH)[0])
  print(JEE[:, 0].reshape(N_BATCH, N_BATCH)[:, 0])
#+end_src

#+RESULTS:
: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
: tensor([0.0000, 0.0124, 0.0248, 0.0373, 0.0497, 0.0621, 0.0745, 0.0870, 0.0994,
:         0.1118], device='cuda:0')

#+begin_src ipython
  print(new_Ja0[..., 0, 0].reshape(N_BATCH, N_BATCH)[0])
  print(new_Ja0[..., 0, 0].reshape(N_BATCH, N_BATCH)[:, 0])
#+end_src

#+RESULTS:
: tensor([  0.0000,  12.4226,  24.8452,  37.2678,  49.6904,  62.1130,  74.5356,
:          86.9582,  99.3808, 111.8034], device='cuda:0')
: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

#+begin_src ipython
  model.IF_BATCH_J = 1
  model.Jab_batch = JEE * model.Jab[0, 0]

  model.Ja0 = new_Ja0

  model.N_BATCH = model.Jab_batch.shape[0]
  model.VERBOSE = 0

  start = perf_counter()
  rates = model().cpu().detach().numpy()
  end = perf_counter()
  print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

  print('rates', rates.shape)
#+end_src

#+RESULTS:
: Elapsed (with compilation) = 0h 1m 1s
: rates (100, 101, 8000)

#+begin_src ipython
  mean_rates = rates.mean(-1).reshape(N_BATCH, N_BATCH, -1)
  print(mean_rates[0, :, -1])
  print(mean_rates[:, 0, -1])
#+end_src

#+RESULTS:
: [1.4012985e-44 1.9618179e-44 1.6705768e-05 4.7168672e-01 1.0980070e+00
:  1.7318970e+00 2.3675365e+00 3.0038698e+00 3.6404078e+00 4.2769861e+00]
: [1.4e-44 1.4e-44 1.4e-44 1.4e-44 1.4e-44 1.4e-44 1.4e-44 1.4e-44 1.4e-44
:  1.4e-44]

#+begin_src ipython
  fig, ax = plt.subplots(1, 2, figsize=[2*width, height])

  ax[0].imshow(mean_rates[..., -1].T, cmap='jet', origin='lower', aspect='auto')
  ax[0].set_xlabel('$J_{EE}$')
  ax[0].set_ylabel('$J_{E0}$')

  ax[1].plot(mean_rates[-1, :, -1]) # over inputs
  ax[1].plot(mean_rates[:, -1, -1]) # over Js
  
  ax[1].set_xlabel('$J_{EE}$')
  ax[1].set_ylabel('$J_{E0}$')

  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/666865640538398619d0d4d02677f62bc707ec42.png]]

#+begin_src ipython
  print(torch.cuda.memory_allocated()/100000)
  del model
  clear_cache()
  print(torch.cuda.memory_allocated()/100000)
#+end_src

#+RESULTS:
: 51148.78464
: 44517.54496
